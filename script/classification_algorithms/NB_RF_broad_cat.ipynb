{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection, preprocessing, naive_bayes, metrics\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn import decomposition, ensemble\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from multiprocessing import Pool\n",
    "\n",
    "from imblearn.metrics import geometric_mean_score as gmean\n",
    "from imblearn.metrics import make_index_balanced_accuracy as iba\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le=LabelEncoder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in and prepare data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "154424"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_file_path='../../dataset/df_ntee_universal/train/'\n",
    "file_list=os.listdir(train_file_path)\n",
    "df_train=pd.DataFrame()\n",
    "for file in file_list:\n",
    "    df_train=pd.concat([df_train, pd.read_pickle(train_file_path+file, compression='gzip')])\n",
    "len(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(154424, 25, 9)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Code as 10 broad categories.\n",
    "broad_cat_dict={'I': ['A'],\n",
    "                'II': ['B'],\n",
    "                'III': ['C', 'D'],\n",
    "                'IV': ['E', 'F', 'G', 'H'],\n",
    "                'V': ['I', 'J', 'K', 'L', 'M', 'N', 'O', 'P'],\n",
    "                'VI': ['Q'],\n",
    "                'VII': ['R', 'S', 'T', 'U', 'V', 'W'],\n",
    "                'VIII': ['X'],\n",
    "                'IX': ['Y'],\n",
    "                'X': ['Z'],\n",
    "               }\n",
    "def ntee2cat(string):\n",
    "    global broad_cat_dict\n",
    "    return [s for s in broad_cat_dict.keys() if string in broad_cat_dict[s]][0]\n",
    "\n",
    "df_train['mission_prgrm']=df_train['TAXPAYER_NAME']+' '+df_train['mission']+' '+df_train['prgrm_dsc']\n",
    "df_train['mission_prgrm_spellchk']=df_train['TAXPAYER_NAME']+' '+df_train['mission_spellchk']+' '+df_train['prgrm_dsc_spellchk'] # Using spell-checked.\n",
    "df_train['broad_cat']=df_train['NTEE1'].apply(ntee2cat)\n",
    "len(df_train['mission_prgrm_spellchk']), len(df_train['NTEE1'].drop_duplicates()), len(df_train['broad_cat'].drop_duplicates())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DLN</th>\n",
       "      <th>EIN</th>\n",
       "      <th>FILING_TYPE</th>\n",
       "      <th>IRS990EZ_p3_DscrptnPrgrmSrvcAccmTxt</th>\n",
       "      <th>IRS990EZ_p3_PrmryExmptPrpsTxt</th>\n",
       "      <th>IRS990PF_p16b_RltnshpSttmntTxt</th>\n",
       "      <th>IRS990PF_p9a_DscrptnTxt</th>\n",
       "      <th>IRS990ScheduleO_ExplntnTxt</th>\n",
       "      <th>IRS990_p1_ActvtyOrMssnDsc</th>\n",
       "      <th>IRS990_p3_DscS</th>\n",
       "      <th>...</th>\n",
       "      <th>YEAR</th>\n",
       "      <th>95_and_before</th>\n",
       "      <th>NTEE1</th>\n",
       "      <th>mission</th>\n",
       "      <th>prgrm_dsc</th>\n",
       "      <th>mission_spellchk</th>\n",
       "      <th>prgrm_dsc_spellchk</th>\n",
       "      <th>mission_prgrm</th>\n",
       "      <th>mission_prgrm_spellchk</th>\n",
       "      <th>broad_cat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>135646</th>\n",
       "      <td>9.349303e+13</td>\n",
       "      <td>640438336</td>\n",
       "      <td>EFILE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>PROVIDE CHILD CARE SERVICES TO DISADVANTAGED C...</td>\n",
       "      <td>THE ORGANIZATION PROVIDES COMPLETE CHILD CARE ...</td>\n",
       "      <td>...</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>B</td>\n",
       "      <td>PROVIDE CHILD CARE SERVICES TO DISADVANTAGED C...</td>\n",
       "      <td>THE ORGANIZATION PROVIDES COMPLETE CHILD CARE ...</td>\n",
       "      <td>PROVIDE CHILD CARE SERVICES TO DISADVANTAGED C...</td>\n",
       "      <td>THE ORGANIZATION PROVIDES COMPLETE CHILD CARE ...</td>\n",
       "      <td>SINGING RIVER EDUCATION ASSOCIATION PROVIDE CH...</td>\n",
       "      <td>SINGING RIVER EDUCATION ASSOCIATION PROVIDE CH...</td>\n",
       "      <td>II</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1341150</th>\n",
       "      <td>9.349215e+13</td>\n",
       "      <td>591446521</td>\n",
       "      <td>EFILE</td>\n",
       "      <td>SUPPLY ASSISTANCE TO FLORIDA ELECTRICAL SUB-CO...</td>\n",
       "      <td>TO SUPPLY ASSISTANCE TO FLORIDA ELECTRICAL SUB...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TO SUPPLY ASSISTANCE TO FLORIDA ELECTRICAL SUB...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2014.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>W</td>\n",
       "      <td>TO SUPPLY ASSISTANCE TO FLORIDA ELECTRICAL SUB...</td>\n",
       "      <td>SUPPLY ASSISTANCE TO FLORIDA ELECTRICAL SUB-CO...</td>\n",
       "      <td>TO SUPPLY ASSISTANCE TO FLORIDA ELECTRICAL SUB...</td>\n",
       "      <td>SUPPLY ASSISTANCE TO FLORIDA ELECTRICAL SUB-CO...</td>\n",
       "      <td>FLORIDA ASSOC OF ELECTRICAL CONTRACTORS INC TO...</td>\n",
       "      <td>FLORIDA ASSOC OF ELECTRICAL CONTRACTORS INC TO...</td>\n",
       "      <td>VII</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1932821</th>\n",
       "      <td>9.349232e+13</td>\n",
       "      <td>222940095</td>\n",
       "      <td>EFILE</td>\n",
       "      <td>MAINTAINING A COMMUNITY RADIO STATION</td>\n",
       "      <td>THE MISSION OF SEACOAST ARTS AND CULTURAL ALLI...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>THE MISSION OF SEACOAST ARTS AND CULTURAL ALLI...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2014.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>A</td>\n",
       "      <td>THE MISSION OF SEACOAST ARTS AND CULTURAL ALLI...</td>\n",
       "      <td>THE MISSION OF SEACOAST ARTS AND CULTURAL ALLI...</td>\n",
       "      <td>THE MISSION OF SEACOAST ARTS AND CULTURAL ALLI...</td>\n",
       "      <td>THE MISSION OF SEACOAST ARTS AND CULTURAL ALLI...</td>\n",
       "      <td>SEACOAST ARTS AND CULTURAL ALLIANCE THE MISSIO...</td>\n",
       "      <td>SEACOAST ARTS AND CULTURAL ALLIANCE THE MISSIO...</td>\n",
       "      <td>I</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208342</th>\n",
       "      <td>9.349314e+13</td>\n",
       "      <td>231520310</td>\n",
       "      <td>EFILE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>TO PROVIDE MATERIALS AND SERVICES TO STIMULATE...</td>\n",
       "      <td>LIBRARY PROGRAMS - TO PROVIDE LIBRARY SERVICE,...</td>\n",
       "      <td>...</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>B</td>\n",
       "      <td>TO PROVIDE MATERIALS AND SERVICES TO STIMULATE...</td>\n",
       "      <td>LIBRARY PROGRAMS - TO PROVIDE LIBRARY SERVICE,...</td>\n",
       "      <td>TO PROVIDE MATERIALS AND SERVICES TO STIMULATE...</td>\n",
       "      <td>LIBRARY PROGRAMS - TO PROVIDE LIBRARY SERVICE ...</td>\n",
       "      <td>BUCKS COUNTY FREE LIBRARY TO PROVIDE MATERIALS...</td>\n",
       "      <td>BUCKS COUNTY FREE LIBRARY TO PROVIDE MATERIALS...</td>\n",
       "      <td>II</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>313275</th>\n",
       "      <td>9.349214e+13</td>\n",
       "      <td>611728536</td>\n",
       "      <td>EFILE</td>\n",
       "      <td>UZN MCA SET THE GROUND WORK IN 2016 WITH FRIEN...</td>\n",
       "      <td>FAMILY, EDUCATION, MUSIC &amp; COMMUNITY ENRICHMENT.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>A</td>\n",
       "      <td>FAMILY, EDUCATION, MUSIC &amp; COMMUNITY ENRICHMENT.</td>\n",
       "      <td>UZN MCA SET THE GROUND WORK IN 2016 WITH FRIEN...</td>\n",
       "      <td>FAMILY , EDUCATION , MUSIC &amp; COMMUNITY ENRICHM...</td>\n",
       "      <td>UN MCA SET THE GROUND WORK IN 2016 WITH FRIEND...</td>\n",
       "      <td>UZN MULTI CULTURAL ARTS CORPORATION FAMILY, ED...</td>\n",
       "      <td>UZN MULTI CULTURAL ARTS CORPORATION FAMILY , E...</td>\n",
       "      <td>I</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>681187</th>\n",
       "      <td>9.349332e+13</td>\n",
       "      <td>731328565</td>\n",
       "      <td>EFILE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>PROVIDING ORCHESTRA PERFORMANCES FOR THE PUBLIC</td>\n",
       "      <td>SYMPHONY ORCHESTRA PERFORMANCES FOR THE GENERA...</td>\n",
       "      <td>...</td>\n",
       "      <td>2014.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>A</td>\n",
       "      <td>PROVIDING ORCHESTRA PERFORMANCES FOR THE PUBLIC</td>\n",
       "      <td>SYMPHONY ORCHESTRA PERFORMANCES FOR THE GENERA...</td>\n",
       "      <td>PROVIDING ORCHESTRA PERFORMANCES FOR THE PUBLIC</td>\n",
       "      <td>SYMPHONY ORCHESTRA PERFORMANCES FOR THE GENERA...</td>\n",
       "      <td>OKLAHOMA PHILHARMONIC SOCIETY INC PROVIDING OR...</td>\n",
       "      <td>OKLAHOMA PHILHARMONIC SOCIETY INC PROVIDING OR...</td>\n",
       "      <td>I</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>676725</th>\n",
       "      <td>9.349336e+13</td>\n",
       "      <td>410905139</td>\n",
       "      <td>EFILE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>PROVIDES HEALTH AND WELFARE BENEFITS</td>\n",
       "      <td>THE PLAN PROVIDES COMPREHENSIVE MEDICAL BENEFI...</td>\n",
       "      <td>...</td>\n",
       "      <td>2014.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Y</td>\n",
       "      <td>PROVIDES HEALTH AND WELFARE BENEFITS</td>\n",
       "      <td>THE PLAN PROVIDES COMPREHENSIVE MEDICAL BENEFI...</td>\n",
       "      <td>PROVIDES HEALTH AND WELFARE BENEFITS</td>\n",
       "      <td>THE PLAN PROVIDES COMPREHENSIVE MEDICAL BENEFI...</td>\n",
       "      <td>MINNEAPOLIS RETAIL MEAT CUTTERS AND FOOD HANDL...</td>\n",
       "      <td>MINNEAPOLIS RETAIL MEAT CUTTERS AND FOOD HANDL...</td>\n",
       "      <td>IX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1840454</th>\n",
       "      <td>9.349316e+13</td>\n",
       "      <td>371088953</td>\n",
       "      <td>EFILE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>PROVIDE AREA HOME BUILDERS AND GENERAL PUBLIC ...</td>\n",
       "      <td>HOME SHOW IS AN ANNUAL EVENT DEDICATED TO PROM...</td>\n",
       "      <td>...</td>\n",
       "      <td>2014.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>L</td>\n",
       "      <td>PROVIDE AREA HOME BUILDERS AND GENERAL PUBLIC ...</td>\n",
       "      <td>MONTHLY DINNERS ARE HELD FOR MEMBERS IN WHICH ...</td>\n",
       "      <td>PROVIDE AREA HOME BUILDERS AND GENERAL PUBLIC ...</td>\n",
       "      <td>MONTHLY DINNERS ARE HELD FOR MEMBERS IN WHICH ...</td>\n",
       "      <td>HOME BUILDERS ASSOC OF GREATER SW IL PROVIDE A...</td>\n",
       "      <td>HOME BUILDERS ASSOC OF GREATER SW IL PROVIDE A...</td>\n",
       "      <td>V</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2179220</th>\n",
       "      <td>9.349313e+13</td>\n",
       "      <td>820223203</td>\n",
       "      <td>EFILE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>PROVIDE IRRIGATION WATER TO FARMS AND HOMES AL...</td>\n",
       "      <td>THE COMPANY PROVIDED IRRIGATION WATER AND MAIN...</td>\n",
       "      <td>...</td>\n",
       "      <td>2014.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>K</td>\n",
       "      <td>PROVIDE IRRIGATION WATER TO FARMS AND HOMES AL...</td>\n",
       "      <td>THE COMPANY PROVIDED IRRIGATION WATER AND MAIN...</td>\n",
       "      <td>PROVIDE IRRIGATION WATER TO FARMS AND HOMES AL...</td>\n",
       "      <td>THE COMPANY PROVIDED IRRIGATION WATER AND MAIN...</td>\n",
       "      <td>RUDY IRRIGATION CANAL COMPANY PROVIDE IRRIGATI...</td>\n",
       "      <td>RUDY IRRIGATION CANAL COMPANY PROVIDE IRRIGATI...</td>\n",
       "      <td>V</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1805992</th>\n",
       "      <td>9.349323e+13</td>\n",
       "      <td>133603559</td>\n",
       "      <td>EFILE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>TO OPERATE A COMMUNITY CENTER AND PROVIDE SUPP...</td>\n",
       "      <td>TO OPERATE A COMMUNITY CENTER AND PROVIDE SUPP...</td>\n",
       "      <td>...</td>\n",
       "      <td>2014.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>P</td>\n",
       "      <td>TO OPERATE A COMMUNITY CENTER AND PROVIDE SUPP...</td>\n",
       "      <td>TO OPERATE A COMMUNITY CENTER AND PROVIDE SUPP...</td>\n",
       "      <td>TO OPERATE A COMMUNITY CENTER AND PROVIDE SUPP...</td>\n",
       "      <td>TO OPERATE A COMMUNITY CENTER AND PROVIDE SUPP...</td>\n",
       "      <td>LOFT- THE LESBIAN AND GAY COMMUNITY SERVICES C...</td>\n",
       "      <td>LOFT- THE LESBIAN AND GAY COMMUNITY SERVICES C...</td>\n",
       "      <td>V</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1356658</th>\n",
       "      <td>9.349305e+13</td>\n",
       "      <td>460363353</td>\n",
       "      <td>EFILE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>OUR MISSION IS TO EQUIP CHILDREN OF CHRISTIAN ...</td>\n",
       "      <td>OPERATE A PRIVATE SCHOOL FOR CHILDREN IN GRADE...</td>\n",
       "      <td>...</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>B</td>\n",
       "      <td>OUR MISSION IS TO EQUIP CHILDREN OF CHRISTIAN ...</td>\n",
       "      <td>OPERATE A PRIVATE SCHOOL FOR CHILDREN IN GRADE...</td>\n",
       "      <td>OUR MISSION IS TO EQUIP CHILDREN OF CHRISTIAN ...</td>\n",
       "      <td>OPERATE A PRIVATE SCHOOL FOR CHILDREN IN GRADE...</td>\n",
       "      <td>RAPID CITY CHRISTIAN EDUCATION ASSOC OUR MISSI...</td>\n",
       "      <td>RAPID CITY CHRISTIAN EDUCATION ASSOC OUR MISSI...</td>\n",
       "      <td>II</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171819</th>\n",
       "      <td>9.349208e+13</td>\n",
       "      <td>752704955</td>\n",
       "      <td>EFILE</td>\n",
       "      <td>PROVIDED A RECREATIONAL FOOTBALL AND CHEERLEAD...</td>\n",
       "      <td>FOSTERING NATIONAL AMATEUR SPORTS COMPET</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "      <td>FOSTERING NATIONAL AMATEUR SPORTS COMPET</td>\n",
       "      <td>PROVIDED A RECREATIONAL FOOTBALL AND CHEERLEAD...</td>\n",
       "      <td>FOSTERING NATIONAL AMATEUR SPORTS COMPETE</td>\n",
       "      <td>PROVIDED A RECREATIONAL FOOTBALL AND CHEERLEAD...</td>\n",
       "      <td>LAKE CITIES FOOTBALL AND CHEERLEADING ASSOCIAT...</td>\n",
       "      <td>LAKE CITIES FOOTBALL AND CHEERLEADING ASSOCIAT...</td>\n",
       "      <td>V</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2221573</th>\n",
       "      <td>9.349313e+13</td>\n",
       "      <td>570984185</td>\n",
       "      <td>EFILE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>SUPPORTIVE SERVICES TO INDIVIDUALS WITH MENTAL...</td>\n",
       "      <td>CONTINUUM OF CARE (COC) HOUSING PROGRAMS UTILI...</td>\n",
       "      <td>...</td>\n",
       "      <td>2014.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>F</td>\n",
       "      <td>SUPPORTIVE SERVICES TO INDIVIDUALS WITH MENTAL...</td>\n",
       "      <td>MIRCI GROUP HOMES CONSISTS OF TWO SIX-BEDROOM ...</td>\n",
       "      <td>SUPPORTIVE SERVICES TO INDIVIDUALS WITH MENTAL...</td>\n",
       "      <td>MERCI GROUP HOMES CONSISTS OF TWO SIX-BEDROOM ...</td>\n",
       "      <td>MENTAL ILLNESS RECOVERY CENTER INC SUPPORTIVE ...</td>\n",
       "      <td>MENTAL ILLNESS RECOVERY CENTER INC SUPPORTIVE ...</td>\n",
       "      <td>IV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1781056</th>\n",
       "      <td>9.349222e+13</td>\n",
       "      <td>451541516</td>\n",
       "      <td>EFILE</td>\n",
       "      <td>CONTRIBUTIONS MAKE IT POSSIBLE TO RAISE FUNDS ...</td>\n",
       "      <td>SUPPORT INNER CITY JUNIOR TENNIS AND EDUCATION...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2014.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "      <td>SUPPORT INNER CITY JUNIOR TENNIS AND EDUCATION...</td>\n",
       "      <td>CONTRIBUTIONS MAKE IT POSSIBLE TO RAISE FUNDS ...</td>\n",
       "      <td>SUPPORT INNER CITY JUNIOR TENNIS AND EDUCATION...</td>\n",
       "      <td>CONTRIBUTIONS MAKE IT POSSIBLE TO RAISE FUNDS ...</td>\n",
       "      <td>SOUTHERN CONNECTICUT ALLIANCE FOR INNER- CITY ...</td>\n",
       "      <td>SOUTHERN CONNECTICUT ALLIANCE FOR INNER- CITY ...</td>\n",
       "      <td>V</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1442687</th>\n",
       "      <td>9.349323e+13</td>\n",
       "      <td>861078516</td>\n",
       "      <td>EFILE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>Set up and manage basketball Teams for the You...</td>\n",
       "      <td>Set basketball teams for the youth of Newtown,...</td>\n",
       "      <td>...</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "      <td>SET UP AND MANAGE BASKETBALL TEAMS FOR THE YOU...</td>\n",
       "      <td>SET BASKETBALL TEAMS FOR THE YOUTH OF NEWTOWN,...</td>\n",
       "      <td>SET UP AND MANAGE BASKETBALL TEAMS FOR THE YOU...</td>\n",
       "      <td>SET BASKETBALL TEAMS FOR THE YOUTH OF NEWTOWN ...</td>\n",
       "      <td>NEWTOWN YOUTH BASKETBALL ASSOCIATION CO SET UP...</td>\n",
       "      <td>NEWTOWN YOUTH BASKETBALL ASSOCIATION CO SET UP...</td>\n",
       "      <td>V</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1561973</th>\n",
       "      <td>9.349213e+13</td>\n",
       "      <td>203360448</td>\n",
       "      <td>EFILE</td>\n",
       "      <td>OPERATED MUSEUM AND EXHIBITS PROMOTING THE HIS...</td>\n",
       "      <td>OPERATE EXHIBITS AND MUSEUM FOR EDUCATIONAL PU...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2014.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>A</td>\n",
       "      <td>OPERATE EXHIBITS AND MUSEUM FOR EDUCATIONAL PU...</td>\n",
       "      <td>OPERATED MUSEUM AND EXHIBITS PROMOTING THE HIS...</td>\n",
       "      <td>OPERATE EXHIBITS AND MUSEUM FOR EDUCATIONAL PU...</td>\n",
       "      <td>OPERATED MUSEUM AND EXHIBITS PROMOTING THE HIS...</td>\n",
       "      <td>STEEL PLANT MUSEUM INC OPERATE EXHIBITS AND MU...</td>\n",
       "      <td>STEEL PLANT MUSEUM INC OPERATE EXHIBITS AND MU...</td>\n",
       "      <td>I</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>646964</th>\n",
       "      <td>9.349331e+13</td>\n",
       "      <td>910961051</td>\n",
       "      <td>EFILE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>TO LEAD COLLABORATION AMONG LAW ENFORCEMENT EX...</td>\n",
       "      <td>SEX OFFENDER ADDRESS VERIFICATION - PROVIDE GR...</td>\n",
       "      <td>...</td>\n",
       "      <td>2014.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>I</td>\n",
       "      <td>TO LEAD COLLABORATION AMONG LAW ENFORCEMENT EX...</td>\n",
       "      <td>SEX OFFENDER ADDRESS VERIFICATION - PROVIDE GR...</td>\n",
       "      <td>TO LEAD COLLABORATION AMONG LAW ENFORCEMENT EX...</td>\n",
       "      <td>SEX OFFENDER ADDRESS VERIFICATION - PROVIDE GR...</td>\n",
       "      <td>WASHINGTON ASSOCIATION OF SHERIFFS AND POLICE ...</td>\n",
       "      <td>WASHINGTON ASSOCIATION OF SHERIFFS AND POLICE ...</td>\n",
       "      <td>V</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188279</th>\n",
       "      <td>9.349317e+13</td>\n",
       "      <td>454737931</td>\n",
       "      <td>EFILE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>Osehra's mission is to build and support an op...</td>\n",
       "      <td>Annual ConferenceOSEHRA's annual open Source S...</td>\n",
       "      <td>...</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>E</td>\n",
       "      <td>OSEHRA'S MISSION IS TO BUILD AND SUPPORT AN OP...</td>\n",
       "      <td>OPEN SOURCE COMMUNITY SERVICESOSEHRA PROVIDES ...</td>\n",
       "      <td>OPERA IS MISSION IS TO BUILD AND SUPPORT AN OP...</td>\n",
       "      <td>OPEN SOURCE COMMUNITY SERVICESOSEHRA PROVIDES ...</td>\n",
       "      <td>OPEN SOURCE ELECTRONIC HEALTH RECORD ALLIANCE ...</td>\n",
       "      <td>OPEN SOURCE ELECTRONIC HEALTH RECORD ALLIANCE ...</td>\n",
       "      <td>IV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2125699</th>\n",
       "      <td>9.349232e+13</td>\n",
       "      <td>521713516</td>\n",
       "      <td>EFILE</td>\n",
       "      <td>Public Programs##History Museum and Conservati...</td>\n",
       "      <td>Education - local community history</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2014.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>A</td>\n",
       "      <td>EDUCATION - LOCAL COMMUNITY HISTORY</td>\n",
       "      <td>HISTORY MUSEUM AND CONSERVATION PROGRAMS; PUBL...</td>\n",
       "      <td>EDUCATION - LOCAL COMMUNITY HISTORY</td>\n",
       "      <td>HISTORY MUSEUM AND CONSERVATION PROGRAMS ; PUB...</td>\n",
       "      <td>LAUREL HISTORICAL SOCIETY INC EDUCATION - LOCA...</td>\n",
       "      <td>LAUREL HISTORICAL SOCIETY INC EDUCATION - LOCA...</td>\n",
       "      <td>I</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>465402</th>\n",
       "      <td>9.349212e+13</td>\n",
       "      <td>161557094</td>\n",
       "      <td>EFILE</td>\n",
       "      <td>Provided financial means to cover the cost of ...</td>\n",
       "      <td>To provide crucial emergency care to pets for ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>D</td>\n",
       "      <td>TO PROVIDE CRUCIAL EMERGENCY CARE TO PETS FOR ...</td>\n",
       "      <td>PROVIDED FINANCIAL MEANS TO COVER THE COST OF ...</td>\n",
       "      <td>TO PROVIDE CRUCIAL EMERGENCY CARE TO PETS FOR ...</td>\n",
       "      <td>PROVIDED FINANCIAL MEANS TO COVER THE COST OF ...</td>\n",
       "      <td>NIAGARA FRONTIER VETERINARY SOCIETY PET EMERGE...</td>\n",
       "      <td>NIAGARA FRONTIER VETERINARY SOCIETY PET EMERGE...</td>\n",
       "      <td>III</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2199188</th>\n",
       "      <td>9.349214e+13</td>\n",
       "      <td>346548260</td>\n",
       "      <td>EFILE</td>\n",
       "      <td>SUPPORT THE BOARDMAN TOWNSHIP FIREFIGHTERS</td>\n",
       "      <td>Support the Boardman Township Firefighters</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2014.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>M</td>\n",
       "      <td>SUPPORT THE BOARDMAN TOWNSHIP FIREFIGHTERS</td>\n",
       "      <td>SUPPORT THE BOARDMAN TOWNSHIP FIREFIGHTERS</td>\n",
       "      <td>SUPPORT THE BOARDMAN TOWNSHIP FIREFIGHTERS</td>\n",
       "      <td>SUPPORT THE BOARDMAN TOWNSHIP FIREFIGHTERS</td>\n",
       "      <td>BOARDMAN FIREFIGHTERS LOCAL 1176 IAFF SUPPORT ...</td>\n",
       "      <td>BOARDMAN FIREFIGHTERS LOCAL 1176 IAFF SUPPORT ...</td>\n",
       "      <td>V</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>661026</th>\n",
       "      <td>9.349329e+13</td>\n",
       "      <td>570699091</td>\n",
       "      <td>EFILE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>PROVIDES EMERGENCY SHELTER TO HOMELESS OR DISL...</td>\n",
       "      <td>PROVIDES EMERGENCY SHELTER AND FOOD TO HOMELES...</td>\n",
       "      <td>...</td>\n",
       "      <td>2014.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>L</td>\n",
       "      <td>PROVIDES EMERGENCY SHELTER TO HOMELESS OR DISL...</td>\n",
       "      <td>PROVIDES EMERGENCY SHELTER AND FOOD TO HOMELES...</td>\n",
       "      <td>PROVIDES EMERGENCY SHELTER TO HOMELESS OR DISL...</td>\n",
       "      <td>PROVIDES EMERGENCY SHELTER AND FOOD TO HOMELES...</td>\n",
       "      <td>FAMILY SHELTER INC PROVIDES EMERGENCY SHELTER ...</td>\n",
       "      <td>FAMILY SHELTER INC PROVIDES EMERGENCY SHELTER ...</td>\n",
       "      <td>V</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206397</th>\n",
       "      <td>9.349221e+13</td>\n",
       "      <td>462212022</td>\n",
       "      <td>EFILE</td>\n",
       "      <td>Purchased a parcel of land to be called Rotary...</td>\n",
       "      <td>The Mendocino Rotary Foundation mission is to ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>S</td>\n",
       "      <td>THE MENDOCINO ROTARY FOUNDATION MISSION IS TO ...</td>\n",
       "      <td>PURCHASED A PARCEL OF LAND TO BE CALLED ROTARY...</td>\n",
       "      <td>THE MENDOCINO ROTARY FOUNDATION MISSION IS TO ...</td>\n",
       "      <td>PURCHASED A PARCEL OF LAND TO BE CALLED ROTARY...</td>\n",
       "      <td>MENDOCINO ROTARY FOUNDATION THE MENDOCINO ROTA...</td>\n",
       "      <td>MENDOCINO ROTARY FOUNDATION THE MENDOCINO ROTA...</td>\n",
       "      <td>VII</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>368237</th>\n",
       "      <td>9.349225e+13</td>\n",
       "      <td>953389180</td>\n",
       "      <td>EFILE</td>\n",
       "      <td>1. The Chamber organized and sponsored functio...</td>\n",
       "      <td>A. PRESERVING THE COMPETITIVE ENTERPRISE SYSTE...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>S</td>\n",
       "      <td>A. PRESERVING THE COMPETITIVE ENTERPRISE SYSTE...</td>\n",
       "      <td>1. THE CHAMBER ORGANIZED AND SPONSORED FUNCTIO...</td>\n",
       "      <td>A . PRESERVING THE COMPETITIVE ENTERPRISE SYST...</td>\n",
       "      <td>1 . THE CHAMBER ORGANIZED AND SPONSORED FUNCTI...</td>\n",
       "      <td>LINCOLN HEIGHTS CHAMBER OF COMMERCE A. PRESERV...</td>\n",
       "      <td>LINCOLN HEIGHTS CHAMBER OF COMMERCE A . PRESER...</td>\n",
       "      <td>VII</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1841611</th>\n",
       "      <td>9.349322e+13</td>\n",
       "      <td>936031469</td>\n",
       "      <td>EFILE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>THE WILLAMETTANS IS A SOCIAL CLUB DESIGNED TO ...</td>\n",
       "      <td>SOCIAL CLUB SERVICES PROVIDED BY MEMBERS FOR T...</td>\n",
       "      <td>...</td>\n",
       "      <td>2014.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>THE WILLAMETTANS IS A SOCIAL CLUB DESIGNED TO ...</td>\n",
       "      <td>SOCIAL CLUB SERVICES PROVIDED BY MEMBERS FOR T...</td>\n",
       "      <td>THE WILLAMETTANS IS A SOCIAL CLUB DESIGNED TO ...</td>\n",
       "      <td>SOCIAL CLUB SERVICES PROVIDED BY MEMBERS FOR T...</td>\n",
       "      <td>WILLAMETTANS THE WILLAMETTANS IS A SOCIAL CLUB...</td>\n",
       "      <td>WILLAMETTANS THE WILLAMETTANS IS A SOCIAL CLUB...</td>\n",
       "      <td>V</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023553</th>\n",
       "      <td>9.349313e+13</td>\n",
       "      <td>476022615</td>\n",
       "      <td>EFILE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>To support and benefit the purposes of the Goo...</td>\n",
       "      <td>Grants to the Good News Broadcasting Associati...</td>\n",
       "      <td>...</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>X</td>\n",
       "      <td>TO SUPPORT AND BENEFIT THE PURPOSES OF THE GOO...</td>\n",
       "      <td>GRANTS TO THE GOOD NEWS BROADCASTING ASSOCIATI...</td>\n",
       "      <td>TO SUPPORT AND BENEFIT THE PURPOSES OF THE GOO...</td>\n",
       "      <td>GRANTS TO THE GOOD NEWS BROADCASTING ASSOCIATI...</td>\n",
       "      <td>BACK TO THE BIBLE FOUNDATION TO SUPPORT AND BE...</td>\n",
       "      <td>BACK TO THE BIBLE FOUNDATION TO SUPPORT AND BE...</td>\n",
       "      <td>VIII</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2083772</th>\n",
       "      <td>9.349223e+13</td>\n",
       "      <td>752440650</td>\n",
       "      <td>EFILE</td>\n",
       "      <td>PROVIDED FLOWERS FOR FUNERALS; FOOD, LODGING, ...</td>\n",
       "      <td>TO ASSIST WIDOWS AND MEMBERS OF THE FIRE DEPAR...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2014.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>M</td>\n",
       "      <td>TO ASSIST WIDOWS AND MEMBERS OF THE FIRE DEPAR...</td>\n",
       "      <td>PROVIDED FLOWERS FOR FUNERALS; FOOD, LODGING, ...</td>\n",
       "      <td>TO ASSIST WIDOWS AND MEMBERS OF THE FIRE DEPAR...</td>\n",
       "      <td>PROVIDED FLOWERS FOR FUNERALS ; FOOD , LODGING...</td>\n",
       "      <td>CHRISTIAN FIREFIGHTERS ASSOCIATION DALLAS DEPA...</td>\n",
       "      <td>CHRISTIAN FIREFIGHTERS ASSOCIATION DALLAS DEPA...</td>\n",
       "      <td>V</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1585174</th>\n",
       "      <td>9.349214e+13</td>\n",
       "      <td>860770510</td>\n",
       "      <td>EFILE</td>\n",
       "      <td>Provided fire protection and other emergency s...</td>\n",
       "      <td>Protect and preserve life and property and min...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2014.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>M</td>\n",
       "      <td>PROTECT AND PRESERVE LIFE AND PROPERTY AND MIN...</td>\n",
       "      <td>PROVIDED FIRE PROTECTION AND OTHER EMERGENCY S...</td>\n",
       "      <td>PROTECT AND PRESERVE LIFE AND PROPERTY AND MIN...</td>\n",
       "      <td>PROVIDED FIRE PROTECTION AND OTHER EMERGENCY S...</td>\n",
       "      <td>ELEPHANT HEAD VOLUNTEER FIRE DEPARTMENT INC PR...</td>\n",
       "      <td>ELEPHANT HEAD VOLUNTEER FIRE DEPARTMENT INC PR...</td>\n",
       "      <td>V</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1840818</th>\n",
       "      <td>9.349318e+13</td>\n",
       "      <td>391189598</td>\n",
       "      <td>EFILE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>OUR MISSION IS TO PREVENT CRUELTY, ABUSE, NEGL...</td>\n",
       "      <td>ALL RELATED EXPENSES WERE DEDICATED TO A SINGL...</td>\n",
       "      <td>...</td>\n",
       "      <td>2014.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>D</td>\n",
       "      <td>OUR MISSION IS TO PREVENT CRUELTY, ABUSE, NEGL...</td>\n",
       "      <td>ALL RELATED EXPENSES WERE DEDICATED TO A SINGL...</td>\n",
       "      <td>OUR MISSION IS TO PREVENT CRUELTY , ABUSE , NE...</td>\n",
       "      <td>ALL RELATED EXPENSES WERE DEDICATED TO A SINGL...</td>\n",
       "      <td>HUMANE SOCIETY OF PORTAGE COUNTY I OUR MISSION...</td>\n",
       "      <td>HUMANE SOCIETY OF PORTAGE COUNTY I OUR MISSION...</td>\n",
       "      <td>III</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>977706</th>\n",
       "      <td>9.349232e+13</td>\n",
       "      <td>60994032</td>\n",
       "      <td>EFILE</td>\n",
       "      <td>Provide after school and family activities##Cu...</td>\n",
       "      <td>The organization is a school based parent teac...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>B</td>\n",
       "      <td>THE ORGANIZATION IS A SCHOOL BASED PARENT TEAC...</td>\n",
       "      <td>PROVIDE AFTER SCHOOL AND FAMILY ACTIVITIES; PR...</td>\n",
       "      <td>THE ORGANIZATION IS A SCHOOL BASED PARENT TEAC...</td>\n",
       "      <td>PROVIDE AFTER SCHOOL AND FAMILY ACTIVITIES ; P...</td>\n",
       "      <td>PTA WEST WOODS CT CONGRESSPTA THE ORGANIZATION...</td>\n",
       "      <td>PTA WEST WOODS CT CONGRESSPTA THE ORGANIZATION...</td>\n",
       "      <td>II</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1579072</th>\n",
       "      <td>9.349322e+13</td>\n",
       "      <td>341627858</td>\n",
       "      <td>EFILE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>To promote the development of parks, recreatio...</td>\n",
       "      <td>Outdoor classroom at Penitentary Glen 30 bench...</td>\n",
       "      <td>...</td>\n",
       "      <td>2014.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>TO PROMOTE THE DEVELOPMENT OF PARKS, RECREATIO...</td>\n",
       "      <td>GORGE PANEL REPLACEMENT AT PG; OUTDOOR CLASSRO...</td>\n",
       "      <td>TO PROMOTE THE DEVELOPMENT OF PARKS , RECREATI...</td>\n",
       "      <td>GORGE PANEL REPLACEMENT AT PG ; OUTDOOR CLASSR...</td>\n",
       "      <td>LAKE PARKS FOUNDATION TO PROMOTE THE DEVELOPME...</td>\n",
       "      <td>LAKE PARKS FOUNDATION TO PROMOTE THE DEVELOPME...</td>\n",
       "      <td>V</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2067791</th>\n",
       "      <td>9.349227e+13</td>\n",
       "      <td>410941665</td>\n",
       "      <td>EFILE</td>\n",
       "      <td>The Organization provides its 370 members with...</td>\n",
       "      <td>Assist in locating employment for its members.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2014.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>J</td>\n",
       "      <td>ASSIST IN LOCATING EMPLOYMENT FOR ITS MEMBERS.</td>\n",
       "      <td>THE ORGANIZATION PROVIDES ITS 370 MEMBERS WITH...</td>\n",
       "      <td>ASSIST IN LOCATING EMPLOYMENT FOR ITS MEMBERS .</td>\n",
       "      <td>THE ORGANIZATION PROVIDES ITS 370 MEMBERS WITH...</td>\n",
       "      <td>UNITED BROTHERHOOD OF MILLWRIGHTS 1348 ASSIST ...</td>\n",
       "      <td>UNITED BROTHERHOOD OF MILLWRIGHTS 1348 ASSIST ...</td>\n",
       "      <td>V</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205187</th>\n",
       "      <td>9.349213e+13</td>\n",
       "      <td>463711546</td>\n",
       "      <td>EFILE</td>\n",
       "      <td>TO PROMOTE SELF CONFIDENCE GOOD SPORTMANSHIP T...</td>\n",
       "      <td>EDUCATION PHILANTHROPIC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "      <td>EDUCATION PHILANTHROPIC</td>\n",
       "      <td>TO PROMOTE SELF CONFIDENCE GOOD SPORTMANSHIP T...</td>\n",
       "      <td>EDUCATION PHILANTHROPIC</td>\n",
       "      <td>TO PROMOTE SELF CONFIDENCE GOOD SPORTSMANSHIP ...</td>\n",
       "      <td>MIDCREST PANTHERS FOOTBALL ORG LTD EDUCATION P...</td>\n",
       "      <td>MIDCREST PANTHERS FOOTBALL ORG LTD EDUCATION P...</td>\n",
       "      <td>V</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>394451</th>\n",
       "      <td>9.349314e+13</td>\n",
       "      <td>412086323</td>\n",
       "      <td>EFILE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>THE HOSPITAL IS COMMITTED TO PROVIDING EXCELLE...</td>\n",
       "      <td>BOONEVILLE COMMUNITY HOSPITAL (BCH) WAS A CRIT...</td>\n",
       "      <td>...</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>E</td>\n",
       "      <td>THE HOSPITAL IS COMMITTED TO PROVIDING EXCELLE...</td>\n",
       "      <td>BOONEVILLE COMMUNITY HOSPITAL (BCH) WAS A CRIT...</td>\n",
       "      <td>THE HOSPITAL IS COMMITTED TO PROVIDING EXCELLE...</td>\n",
       "      <td>BONNEVILLE COMMUNITY HOSPITAL ( BC ) WAS A CRI...</td>\n",
       "      <td>BOONEVILLE COMMUNITY HOSPITAL THE HOSPITAL IS ...</td>\n",
       "      <td>BOONEVILLE COMMUNITY HOSPITAL THE HOSPITAL IS ...</td>\n",
       "      <td>IV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>693763</th>\n",
       "      <td>9.349332e+13</td>\n",
       "      <td>752304155</td>\n",
       "      <td>EFILE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>PUBLIC EDUCATION ABOUT CRANIOFACIAL DEFORMITIE...</td>\n",
       "      <td>PROVIDED HOSPITAL, SURGICAL SERVICES, SUPPLIES...</td>\n",
       "      <td>...</td>\n",
       "      <td>2014.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Q</td>\n",
       "      <td>PUBLIC EDUCATION ABOUT CRANIOFACIAL DEFORMITIE...</td>\n",
       "      <td>PROVIDED SURGICAL FELLOWSHIPS AND OTHER FORMS ...</td>\n",
       "      <td>PUBLIC EDUCATION ABOUT CRANIOFACIAL DEFORMITIE...</td>\n",
       "      <td>PROVIDED SURGICAL FELLOWSHIPS AND OTHER FORMS ...</td>\n",
       "      <td>WORLD CRANIOFACIAL FOUNDATION PUBLIC EDUCATION...</td>\n",
       "      <td>WORLD CRANIOFACIAL FOUNDATION PUBLIC EDUCATION...</td>\n",
       "      <td>VI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146083</th>\n",
       "      <td>9.349314e+13</td>\n",
       "      <td>621192763</td>\n",
       "      <td>EFILE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>TO CREATE AND FOSTER A SPIRIT OF UNDERSTANDING...</td>\n",
       "      <td>TO HELP PROVIDE VISION CARE TO ALL THOSE WHO C...</td>\n",
       "      <td>...</td>\n",
       "      <td>2014.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>S</td>\n",
       "      <td>TO CREATE AND FOSTER A SPIRIT OF UNDERSTANDING...</td>\n",
       "      <td>TO HELP PROVIDE VISION CARE TO ALL THOSE WHO C...</td>\n",
       "      <td>TO CREATE AND FOSTER A SPIRIT OF UNDERSTANDING...</td>\n",
       "      <td>TO HELP PROVIDE VISION CARE TO ALL THOSE WHO C...</td>\n",
       "      <td>MONTEREY LIONS CLUB INC TO CREATE AND FOSTER A...</td>\n",
       "      <td>MONTEREY LIONS CLUB INC TO CREATE AND FOSTER A...</td>\n",
       "      <td>VII</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1592706</th>\n",
       "      <td>9.349215e+13</td>\n",
       "      <td>26012322</td>\n",
       "      <td>EFILE</td>\n",
       "      <td>Solicit public donations and using the proceed...</td>\n",
       "      <td>Support of local public schools</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2014.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>B</td>\n",
       "      <td>SUPPORT OF LOCAL PUBLIC SCHOOLS</td>\n",
       "      <td>SOLICIT PUBLIC DONATIONS AND USING THE PROCEED...</td>\n",
       "      <td>SUPPORT OF LOCAL PUBLIC SCHOOLS</td>\n",
       "      <td>SOLICIT PUBLIC DONATIONS AND USING THE PROCEED...</td>\n",
       "      <td>FRIENDS OF THE HANOVER-NORWICH SCHOOLSINC SUPP...</td>\n",
       "      <td>FRIENDS OF THE HANOVER-NORWICH SCHOOLSINC SUPP...</td>\n",
       "      <td>II</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>615991</th>\n",
       "      <td>9.349330e+13</td>\n",
       "      <td>112644562</td>\n",
       "      <td>EFILE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>Citizen Action of New York, Inc. is a not-for-...</td>\n",
       "      <td>COMMUNITY ORGANIZING, ADVOCACY AND PUBLIC EDUC...</td>\n",
       "      <td>...</td>\n",
       "      <td>2014.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>S</td>\n",
       "      <td>CITIZEN ACTION OF NEW YORK, INC. IS A NOT-FOR-...</td>\n",
       "      <td>COMMUNITY ORGANIZING, ADVOCACY AND PUBLIC EDUC...</td>\n",
       "      <td>CITIZEN ACTION OF NEW YORK , INC. IS A NOT-FOR...</td>\n",
       "      <td>COMMUNITY ORGANIZING , ADVOCACY AND PUBLIC EDU...</td>\n",
       "      <td>CITIZEN ACTION OF NEW YORK INC CITIZEN ACTION ...</td>\n",
       "      <td>CITIZEN ACTION OF NEW YORK INC CITIZEN ACTION ...</td>\n",
       "      <td>VII</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2108577</th>\n",
       "      <td>9.349326e+13</td>\n",
       "      <td>208404485</td>\n",
       "      <td>EFILE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>TO PROVIDE MUSICAL EDUCATION &amp; INSTRUMENTAL TR...</td>\n",
       "      <td>SYMPHONIC MUSIC EDUCATION PROGRAM AND THE CHAM...</td>\n",
       "      <td>...</td>\n",
       "      <td>2014.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>A</td>\n",
       "      <td>TO PROVIDE MUSICAL EDUCATION &amp; INSTRUMENTAL TR...</td>\n",
       "      <td>SYMPHONIC MUSIC EDUCATION PROGRAM AND THE CHAM...</td>\n",
       "      <td>TO PROVIDE MUSICAL EDUCATION &amp; INSTRUMENTAL TR...</td>\n",
       "      <td>SYMPHONIC MUSIC EDUCATION PROGRAM AND THE CHAM...</td>\n",
       "      <td>CLAP AND TAP CHAMBER ORCHESTRA TO PROVIDE MUSI...</td>\n",
       "      <td>CLAP AND TAP CHAMBER ORCHESTRA TO PROVIDE MUSI...</td>\n",
       "      <td>I</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1899863</th>\n",
       "      <td>9.349232e+13</td>\n",
       "      <td>850155431</td>\n",
       "      <td>EFILE</td>\n",
       "      <td>ANNUAL YEARBOOK AND WINDMILL PUBLICATION##CONV...</td>\n",
       "      <td>PROFESSIONAL FLORIST ASSOCIATION</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SCHOLARSHIPS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2014.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>A</td>\n",
       "      <td>PROFESSIONAL FLORIST ASSOCIATION</td>\n",
       "      <td>SCHOLARSHIPS; CONVENTION, SEMINARS, TRADE SHOW...</td>\n",
       "      <td>PROFESSIONAL FLORIST ASSOCIATION</td>\n",
       "      <td>SCHOLARSHIPS ; CONVENTION , SEMINARS , TRADE S...</td>\n",
       "      <td>WESTEXAS NEW MEXICO FLORIST ASSOC PROFESSIONAL...</td>\n",
       "      <td>WESTEXAS NEW MEXICO FLORIST ASSOC PROFESSIONAL...</td>\n",
       "      <td>I</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1421790</th>\n",
       "      <td>9.349331e+13</td>\n",
       "      <td>237055367</td>\n",
       "      <td>EFILE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>THE CENTER FOR HOUSING POLICY IS A RESEARCH OR...</td>\n",
       "      <td>RESEARCH, PUBLICATIONS, EVENTS &amp; ONLINE TOOLS ...</td>\n",
       "      <td>...</td>\n",
       "      <td>2014.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>L</td>\n",
       "      <td>THE CENTER FOR HOUSING POLICY IS A RESEARCH OR...</td>\n",
       "      <td>RESEARCH, PUBLICATIONS, EVENTS AND ONLINE TOOL...</td>\n",
       "      <td>THE CENTER FOR HOUSING POLICY IS A RESEARCH OR...</td>\n",
       "      <td>RESEARCH , PUBLICATIONS , EVENTS AND ONLINE TO...</td>\n",
       "      <td>CENTER FOR HOUSING POLICY THE CENTER FOR HOUSI...</td>\n",
       "      <td>CENTER FOR HOUSING POLICY THE CENTER FOR HOUSI...</td>\n",
       "      <td>V</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284173</th>\n",
       "      <td>9.349226e+13</td>\n",
       "      <td>274810858</td>\n",
       "      <td>EFILE</td>\n",
       "      <td>PRESERVATION OF MUSTARD HISTORY</td>\n",
       "      <td>MUSEUM ENDEAVORS TO FOSTER AN APPRECIATION FOR...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MUSEUM ENDEAVORS TO FOSTER AN APPRECIATION FOR...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>A</td>\n",
       "      <td>MUSEUM ENDEAVORS TO FOSTER AN APPRECIATION FOR...</td>\n",
       "      <td>MUSEUM ENDEAVORS TO FOSTER AN APPRECIATION FOR...</td>\n",
       "      <td>MUSEUM ENDEAVOURS TO FOSTER AN APPRECIATION FO...</td>\n",
       "      <td>MUSEUM ENDEAVOURS TO FOSTER AN APPRECIATION FO...</td>\n",
       "      <td>NATIONAL MUSTARD MUSEUM INC MUSEUM ENDEAVORS T...</td>\n",
       "      <td>NATIONAL MUSTARD MUSEUM INC MUSEUM ENDEAVOURS ...</td>\n",
       "      <td>I</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1501021</th>\n",
       "      <td>9.349303e+13</td>\n",
       "      <td>390963086</td>\n",
       "      <td>EFILE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>AFT-WISCONSIN FOSTERS COOPERATIVE ACTION AMONG...</td>\n",
       "      <td>THE AFT-WISCONSIN SOLIDARITY FUND IS A $2 PER ...</td>\n",
       "      <td>...</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>J</td>\n",
       "      <td>AFT-WISCONSIN FOSTERS COOPERATIVE ACTION AMONG...</td>\n",
       "      <td>THE 2013 CONVENTION IS THE ANNUAL MEETING OF L...</td>\n",
       "      <td>AFT-WISCONSIN FOSTERS COOPERATIVE ACTION AMONG...</td>\n",
       "      <td>THE 2013 CONVENTION IS THE ANNUAL MEETING OF L...</td>\n",
       "      <td>AMERICAN FEDERATION OF TEACHERS AFT-WISCONSIN ...</td>\n",
       "      <td>AMERICAN FEDERATION OF TEACHERS AFT-WISCONSIN ...</td>\n",
       "      <td>V</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>267676</th>\n",
       "      <td>9.349223e+13</td>\n",
       "      <td>264482327</td>\n",
       "      <td>EFILE</td>\n",
       "      <td>FINANCIAL ASSISTANCE:ASSISTANCE TO FAMILIES FO...</td>\n",
       "      <td>ASSISTANCE TO INDIVIDUALS WHO ARE COPING WITH ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>P</td>\n",
       "      <td>ASSISTANCE TO INDIVIDUALS WHO ARE COPING WITH ...</td>\n",
       "      <td>FINANCIAL ASSISTANCE:ASSISTANCE TO FAMILIES FO...</td>\n",
       "      <td>ASSISTANCE TO INDIVIDUALS WHO ARE COPING WITH ...</td>\n",
       "      <td>FINANCIAL ASSISTANCE : ASSISTANCE TO FAMILIES ...</td>\n",
       "      <td>HOSPICE SUPPORT FOUNDATION ASSISTANCE TO INDIV...</td>\n",
       "      <td>HOSPICE SUPPORT FOUNDATION ASSISTANCE TO INDIV...</td>\n",
       "      <td>V</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1797240</th>\n",
       "      <td>9.349320e+13</td>\n",
       "      <td>746063819</td>\n",
       "      <td>EFILE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>DISPLAY FINE ART TO THE GENERAL PUBLIC AND PRE...</td>\n",
       "      <td>THE ORGANIZATION PROVIDES FUNDS FOR MAINTAININ...</td>\n",
       "      <td>...</td>\n",
       "      <td>2014.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>A</td>\n",
       "      <td>DISPLAY FINE ART TO THE GENERAL PUBLIC AND PRE...</td>\n",
       "      <td>THE ORGANIZATION PROVIDES FUNDS FOR MAINTAININ...</td>\n",
       "      <td>DISPLAY FINE ART TO THE GENERAL PUBLIC AND PRE...</td>\n",
       "      <td>THE ORGANIZATION PROVIDES FUNDS FOR MAINTAININ...</td>\n",
       "      <td>INTERNATIONAL ASSOCIATION FOR THE VISUAL ARTS ...</td>\n",
       "      <td>INTERNATIONAL ASSOCIATION FOR THE VISUAL ARTS ...</td>\n",
       "      <td>I</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291532</th>\n",
       "      <td>9.349332e+13</td>\n",
       "      <td>42560456</td>\n",
       "      <td>EFILE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>THE LOCAL'S MISSION IS TO REPRESENT IT'S MEMBE...</td>\n",
       "      <td>ALLOWS THE ORGANIZATION TO NEGOTIATE CONTRACTS...</td>\n",
       "      <td>...</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>J</td>\n",
       "      <td>THE LOCAL'S MISSION IS TO REPRESENT IT'S MEMBE...</td>\n",
       "      <td>ALLOWS THE ORGANIZATION TO NEGOTIATE CONTRACTS...</td>\n",
       "      <td>THE LOCAL IS MISSION IS TO REPRESENT IT IS MEM...</td>\n",
       "      <td>ALLOWS THE ORGANIZATION TO NEGOTIATE CONTRACTS...</td>\n",
       "      <td>UNITE HERE LOCAL 26 HOTEL WORKERS UNION THE LO...</td>\n",
       "      <td>UNITE HERE LOCAL 26 HOTEL WORKERS UNION THE LO...</td>\n",
       "      <td>V</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1857153</th>\n",
       "      <td>9.349323e+13</td>\n",
       "      <td>954845170</td>\n",
       "      <td>EFILE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>OTHER PROGRAM SERVICES 4: LACBC offers bicycle...</td>\n",
       "      <td>Bicycling-related advocacy, education and outr...</td>\n",
       "      <td>LACBC organized campaigns for a more bikeable ...</td>\n",
       "      <td>...</td>\n",
       "      <td>2014.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>S</td>\n",
       "      <td>BICYCLING-RELATED ADVOCACY, EDUCATION AND OUTR...</td>\n",
       "      <td>OTHER PROGRAM SERVICES 4: LACBC OFFERS BICYCLE...</td>\n",
       "      <td>BICYCLING-RELATED ADVOCACY , EDUCATION AND OUT...</td>\n",
       "      <td>OTHER PROGRAM SERVICES 4 : LACK OFFERS BICYCLE...</td>\n",
       "      <td>LOS ANGELES COUNTY BICYCLE COALITION BICYCLING...</td>\n",
       "      <td>LOS ANGELES COUNTY BICYCLE COALITION BICYCLING...</td>\n",
       "      <td>VII</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132249</th>\n",
       "      <td>9.349302e+13</td>\n",
       "      <td>237131611</td>\n",
       "      <td>EFILE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>ENTITY IS AN AFFILIATE OF THE NATIONAL NOT-FOR...</td>\n",
       "      <td>The organization operates a facility on an on-...</td>\n",
       "      <td>...</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Y</td>\n",
       "      <td>ENTITY IS AN AFFILIATE OF THE NATIONAL NOT-FOR...</td>\n",
       "      <td>THE LOCAL ORGANIZATION HELPS DEFRAY THE COST O...</td>\n",
       "      <td>ENTITY IS AN AFFILIATE OF THE NATIONAL NOT-FOR...</td>\n",
       "      <td>THE LOCAL ORGANIZATION HELPS DEFRAY THE COST O...</td>\n",
       "      <td>FRATERNAL ORDER OF EAGLES 148 AERIE ENTITY IS ...</td>\n",
       "      <td>FRATERNAL ORDER OF EAGLES 148 AERIE ENTITY IS ...</td>\n",
       "      <td>IX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1336610</th>\n",
       "      <td>9.349231e+13</td>\n",
       "      <td>593438641</td>\n",
       "      <td>EFILE</td>\n",
       "      <td>Licensee of noncommercial educational radio st...</td>\n",
       "      <td>Educational Broadcasting</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2014.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>A</td>\n",
       "      <td>EDUCATIONAL BROADCASTING</td>\n",
       "      <td>LICENSEE OF NONCOMMERCIAL EDUCATIONAL RADIO ST...</td>\n",
       "      <td>EDUCATIONAL BROADCASTING</td>\n",
       "      <td>LICENSEE OF NON-COMMERCIAL EDUCATIONAL RADIO S...</td>\n",
       "      <td>DAYSTAR PUBLIC RADIO INC EDUCATIONAL BROADCAST...</td>\n",
       "      <td>DAYSTAR PUBLIC RADIO INC EDUCATIONAL BROADCAST...</td>\n",
       "      <td>I</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1056545</th>\n",
       "      <td>9.349211e+13</td>\n",
       "      <td>481079169</td>\n",
       "      <td>EFILE</td>\n",
       "      <td>PROVIDE HOUSING FACILITIES TO PEOPLE WITH LONG...</td>\n",
       "      <td>PROVIDE HOUSING FOR PEOPLE WITH SEVERE MENTAL ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>L</td>\n",
       "      <td>PROVIDE HOUSING FOR PEOPLE WITH SEVERE MENTAL ...</td>\n",
       "      <td>PROVIDE HOUSING FACILITIES TO PEOPLE WITH LONG...</td>\n",
       "      <td>PROVIDE HOUSING FOR PEOPLE WITH SEVERE MENTAL ...</td>\n",
       "      <td>PROVIDE HOUSING FACILITIES TO PEOPLE WITH LONG...</td>\n",
       "      <td>MEADOWLARK HOUSING INC PROVIDE HOUSING FOR PEO...</td>\n",
       "      <td>MEADOWLARK HOUSING INC PROVIDE HOUSING FOR PEO...</td>\n",
       "      <td>V</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2060818</th>\n",
       "      <td>9.349324e+13</td>\n",
       "      <td>431890025</td>\n",
       "      <td>EFILE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>UNION STATION KANSAS CITY, INC. SHALL BE RECOG...</td>\n",
       "      <td>TO BE RECOGNIZED AS THE REGION'S FINEST EDUCAT...</td>\n",
       "      <td>SEE SCHEDULE O##SEE SCHEDULE O##SEE SCHEDULE O</td>\n",
       "      <td>...</td>\n",
       "      <td>2014.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>A</td>\n",
       "      <td>TO BE RECOGNIZED AS THE REGION'S FINEST EDUCAT...</td>\n",
       "      <td>SEE SCHEDULE O; UNION STATION KANSAS CITY'S OT...</td>\n",
       "      <td>TO BE RECOGNIZED AS THE REGION IS FINEST EDUCA...</td>\n",
       "      <td>SEE SCHEDULE O ; UNION STATION KANSAS CITY IS ...</td>\n",
       "      <td>UNION STATION KANSAS CITY INC TO BE RECOGNIZED...</td>\n",
       "      <td>UNION STATION KANSAS CITY INC TO BE RECOGNIZED...</td>\n",
       "      <td>I</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148552</th>\n",
       "      <td>9.349223e+13</td>\n",
       "      <td>431687056</td>\n",
       "      <td>EFILE</td>\n",
       "      <td>EXEMPTION IS ZERO</td>\n",
       "      <td>CHAMBER OF COMMERCE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>S</td>\n",
       "      <td>CHAMBER OF COMMERCE</td>\n",
       "      <td>EXEMPTION IS ZERO</td>\n",
       "      <td>CHAMBER OF COMMERCE</td>\n",
       "      <td>EXEMPTION IS ZERO</td>\n",
       "      <td>VERSAILLES CHAMBER OF COMMERCE CHAMBER OF COMM...</td>\n",
       "      <td>VERSAILLES CHAMBER OF COMMERCE CHAMBER OF COMM...</td>\n",
       "      <td>VII</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2058713</th>\n",
       "      <td>9.349322e+13</td>\n",
       "      <td>930800140</td>\n",
       "      <td>EFILE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>To Support Providence Child Center.</td>\n",
       "      <td>Funding of operating expenses of, or payments ...</td>\n",
       "      <td>...</td>\n",
       "      <td>2014.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>P</td>\n",
       "      <td>TO SUPPORT PROVIDENCE CHILD CENTER.</td>\n",
       "      <td>FUNDING OF CAPITAL EXPENDITURES AND EQUIPMENT ...</td>\n",
       "      <td>TO SUPPORT PROVIDENCE CHILD CENTER .</td>\n",
       "      <td>FUNDING OF CAPITAL EXPENDITURES AND EQUIPMENT ...</td>\n",
       "      <td>PROVIDENCE CHILD CENTER FOUNDATION TO SUPPORT ...</td>\n",
       "      <td>PROVIDENCE CHILD CENTER FOUNDATION TO SUPPORT ...</td>\n",
       "      <td>V</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2162466</th>\n",
       "      <td>9.349313e+13</td>\n",
       "      <td>570777231</td>\n",
       "      <td>EFILE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>THE CLARENDON COUNTY CHAMBER OF COMMERCE PROMO...</td>\n",
       "      <td>MEMBERSHIP RETREAT - THE RETREAT PROVIDES NETW...</td>\n",
       "      <td>...</td>\n",
       "      <td>2014.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>S</td>\n",
       "      <td>THE CLARENDON COUNTY CHAMBER OF COMMERCE PROMO...</td>\n",
       "      <td>CLARENDON COUNTY STRIPED BASS FESTIVAL - THE A...</td>\n",
       "      <td>THE CLARENDON COUNTY CHAMBER OF COMMERCE PROMO...</td>\n",
       "      <td>CLARENDON COUNTY STRIPED BASS FESTIVAL - THE A...</td>\n",
       "      <td>CLARENDON CNTY CHAMBER OF COMMERCE THE CLAREND...</td>\n",
       "      <td>CLARENDON CNTY CHAMBER OF COMMERCE THE CLAREND...</td>\n",
       "      <td>VII</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1011657</th>\n",
       "      <td>9.349304e+13</td>\n",
       "      <td>581670951</td>\n",
       "      <td>EFILE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>HELP LOCAL ORGANIZATIONS IN RECRUITING INDUSTR...</td>\n",
       "      <td>INTERACTS WITH LOCAL GOVERNMENT REGARDING OPER...</td>\n",
       "      <td>...</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>S</td>\n",
       "      <td>HELP LOCAL ORGANIZATIONS IN RECRUITING INDUSTR...</td>\n",
       "      <td>INTERACTS WITH LOCAL GOVERNMENT REGARDING OPER...</td>\n",
       "      <td>HELP LOCAL ORGANIZATIONS IN RECRUITING INDUSTR...</td>\n",
       "      <td>INTERACTS WITH LOCAL GOVERNMENT REGARDING OPER...</td>\n",
       "      <td>JOHNSTON COUNTY ECONOMIC DEVELOPMENT CORPORATI...</td>\n",
       "      <td>JOHNSTON COUNTY ECONOMIC DEVELOPMENT CORPORATI...</td>\n",
       "      <td>VII</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1782576</th>\n",
       "      <td>9.349314e+13</td>\n",
       "      <td>208219397</td>\n",
       "      <td>EFILE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>SUPPORT POLK COUNTY'S LAW ENFORCEMENT COMMUNITY</td>\n",
       "      <td>SUPPORTED MEMBERS OF POLK COUNTY'S LAW ENFORCE...</td>\n",
       "      <td>...</td>\n",
       "      <td>2014.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>I</td>\n",
       "      <td>SUPPORT POLK COUNTY'S LAW ENFORCEMENT COMMUNITY</td>\n",
       "      <td>SUPPORTED MEMBERS OF POLK COUNTY'S LAW ENFORCE...</td>\n",
       "      <td>SUPPORT POLL COUNTY IS LAW ENFORCEMENT COMMUNITY</td>\n",
       "      <td>SUPPORTED MEMBERS OF POLL COUNTY IS LAW ENFORC...</td>\n",
       "      <td>POLK SHERIFFS CHARITIES INC SUPPORT POLK COUNT...</td>\n",
       "      <td>POLK SHERIFFS CHARITIES INC SUPPORT POLL COUNT...</td>\n",
       "      <td>V</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1060295</th>\n",
       "      <td>9.349314e+13</td>\n",
       "      <td>440194955</td>\n",
       "      <td>EFILE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>TO ENCOURAGE A HIGH STANDARD OF SKILL; FAIR ST...</td>\n",
       "      <td>MAINTAIN WORK AGREEMENTS WITH THE CONTRACTOR A...</td>\n",
       "      <td>...</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>J</td>\n",
       "      <td>TO ENCOURAGE A HIGH STANDARD OF SKILL; FAIR ST...</td>\n",
       "      <td>MAINTAIN WORK AGREEMENTS WITH THE CONTRACTOR A...</td>\n",
       "      <td>TO ENCOURAGE A HIGH STANDARD OF SKILL ; FAIR S...</td>\n",
       "      <td>MAINTAIN WORK AGREEMENTS WITH THE CONTRACTOR A...</td>\n",
       "      <td>OPERATIVE PLASTERERS AND CEMENT MASON TO ENCOU...</td>\n",
       "      <td>OPERATIVE PLASTERERS AND CEMENT MASON TO ENCOU...</td>\n",
       "      <td>V</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1072834</th>\n",
       "      <td>9.349231e+13</td>\n",
       "      <td>112972415</td>\n",
       "      <td>EFILE</td>\n",
       "      <td>HOLD TITLE TO REAL PROPERTY, COLLECT INCOME AN...</td>\n",
       "      <td>HOLD TITLE TO REAL PROPERTY, COLLECT INCOME AN...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2014.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>J</td>\n",
       "      <td>HOLD TITLE TO REAL PROPERTY, COLLECT INCOME AN...</td>\n",
       "      <td>HOLD TITLE TO REAL PROPERTY, COLLECT INCOME AN...</td>\n",
       "      <td>HOLD TITLE TO REAL PROPERTY , COLLECT INCOME A...</td>\n",
       "      <td>HOLD TITLE TO REAL PROPERTY , COLLECT INCOME A...</td>\n",
       "      <td>IRON WORKERS LOCAL 361 REALTY CORP HOLD TITLE ...</td>\n",
       "      <td>IRON WORKERS LOCAL 361 REALTY CORP HOLD TITLE ...</td>\n",
       "      <td>V</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1826480</th>\n",
       "      <td>9.349226e+13</td>\n",
       "      <td>205851043</td>\n",
       "      <td>EFILE</td>\n",
       "      <td>DURING 2013, WE SERVED INDIVIDUALS WITH DISABI...</td>\n",
       "      <td>LEG UP THERAPEUTIC RIDING CENTER USES EQUINE A...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LEG UP THERAPEUTIC RIDING CENTER USES EQUINE A...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2014.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>P</td>\n",
       "      <td>LEG UP THERAPEUTIC RIDING CENTER USES EQUINE A...</td>\n",
       "      <td>DURING 2013, WE SERVED INDIVIDUALS WITH DISABI...</td>\n",
       "      <td>LEG UP THERAPEUTIC RIDING CENTER USES EQUINE A...</td>\n",
       "      <td>DURING 2013 , WE SERVED INDIVIDUALS WITH DISAB...</td>\n",
       "      <td>LEG UP THERAPEUTIC RIDING CENTER LEG UP THERAP...</td>\n",
       "      <td>LEG UP THERAPEUTIC RIDING CENTER LEG UP THERAP...</td>\n",
       "      <td>V</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008855</th>\n",
       "      <td>9.349232e+13</td>\n",
       "      <td>731722337</td>\n",
       "      <td>EFILE</td>\n",
       "      <td>On September 16 2014 organization made a contr...</td>\n",
       "      <td>To make contributions to other 501c3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>G</td>\n",
       "      <td>TO MAKE CONTRIBUTIONS TO OTHER 501C3</td>\n",
       "      <td>ON SEPTEMBER 16 2014 ORGANIZATION MADE A CONTR...</td>\n",
       "      <td>TO MAKE CONTRIBUTIONS TO OTHER 50123</td>\n",
       "      <td>ON SEPTEMBER 16 2014 ORGANIZATION MADE A CONTR...</td>\n",
       "      <td>HEARING EERS INC TO MAKE CONTRIBUTIONS TO OTHE...</td>\n",
       "      <td>HEARING EERS INC TO MAKE CONTRIBUTIONS TO OTHE...</td>\n",
       "      <td>IV</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>154424 rows  27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  DLN        EIN FILING_TYPE  \\\n",
       "135646   9.349303e+13  640438336       EFILE   \n",
       "1341150  9.349215e+13  591446521       EFILE   \n",
       "1932821  9.349232e+13  222940095       EFILE   \n",
       "208342   9.349314e+13  231520310       EFILE   \n",
       "313275   9.349214e+13  611728536       EFILE   \n",
       "681187   9.349332e+13  731328565       EFILE   \n",
       "676725   9.349336e+13  410905139       EFILE   \n",
       "1840454  9.349316e+13  371088953       EFILE   \n",
       "2179220  9.349313e+13  820223203       EFILE   \n",
       "1805992  9.349323e+13  133603559       EFILE   \n",
       "1356658  9.349305e+13  460363353       EFILE   \n",
       "171819   9.349208e+13  752704955       EFILE   \n",
       "2221573  9.349313e+13  570984185       EFILE   \n",
       "1781056  9.349222e+13  451541516       EFILE   \n",
       "1442687  9.349323e+13  861078516       EFILE   \n",
       "1561973  9.349213e+13  203360448       EFILE   \n",
       "646964   9.349331e+13  910961051       EFILE   \n",
       "188279   9.349317e+13  454737931       EFILE   \n",
       "2125699  9.349232e+13  521713516       EFILE   \n",
       "465402   9.349212e+13  161557094       EFILE   \n",
       "2199188  9.349214e+13  346548260       EFILE   \n",
       "661026   9.349329e+13  570699091       EFILE   \n",
       "206397   9.349221e+13  462212022       EFILE   \n",
       "368237   9.349225e+13  953389180       EFILE   \n",
       "1841611  9.349322e+13  936031469       EFILE   \n",
       "2023553  9.349313e+13  476022615       EFILE   \n",
       "2083772  9.349223e+13  752440650       EFILE   \n",
       "1585174  9.349214e+13  860770510       EFILE   \n",
       "1840818  9.349318e+13  391189598       EFILE   \n",
       "977706   9.349232e+13   60994032       EFILE   \n",
       "...               ...        ...         ...   \n",
       "1579072  9.349322e+13  341627858       EFILE   \n",
       "2067791  9.349227e+13  410941665       EFILE   \n",
       "205187   9.349213e+13  463711546       EFILE   \n",
       "394451   9.349314e+13  412086323       EFILE   \n",
       "693763   9.349332e+13  752304155       EFILE   \n",
       "146083   9.349314e+13  621192763       EFILE   \n",
       "1592706  9.349215e+13   26012322       EFILE   \n",
       "615991   9.349330e+13  112644562       EFILE   \n",
       "2108577  9.349326e+13  208404485       EFILE   \n",
       "1899863  9.349232e+13  850155431       EFILE   \n",
       "1421790  9.349331e+13  237055367       EFILE   \n",
       "284173   9.349226e+13  274810858       EFILE   \n",
       "1501021  9.349303e+13  390963086       EFILE   \n",
       "267676   9.349223e+13  264482327       EFILE   \n",
       "1797240  9.349320e+13  746063819       EFILE   \n",
       "291532   9.349332e+13   42560456       EFILE   \n",
       "1857153  9.349323e+13  954845170       EFILE   \n",
       "132249   9.349302e+13  237131611       EFILE   \n",
       "1336610  9.349231e+13  593438641       EFILE   \n",
       "1056545  9.349211e+13  481079169       EFILE   \n",
       "2060818  9.349324e+13  431890025       EFILE   \n",
       "148552   9.349223e+13  431687056       EFILE   \n",
       "2058713  9.349322e+13  930800140       EFILE   \n",
       "2162466  9.349313e+13  570777231       EFILE   \n",
       "1011657  9.349304e+13  581670951       EFILE   \n",
       "1782576  9.349314e+13  208219397       EFILE   \n",
       "1060295  9.349314e+13  440194955       EFILE   \n",
       "1072834  9.349231e+13  112972415       EFILE   \n",
       "1826480  9.349226e+13  205851043       EFILE   \n",
       "2008855  9.349232e+13  731722337       EFILE   \n",
       "\n",
       "                       IRS990EZ_p3_DscrptnPrgrmSrvcAccmTxt  \\\n",
       "135646                                                 NaN   \n",
       "1341150  SUPPLY ASSISTANCE TO FLORIDA ELECTRICAL SUB-CO...   \n",
       "1932821              MAINTAINING A COMMUNITY RADIO STATION   \n",
       "208342                                                 NaN   \n",
       "313275   UZN MCA SET THE GROUND WORK IN 2016 WITH FRIEN...   \n",
       "681187                                                 NaN   \n",
       "676725                                                 NaN   \n",
       "1840454                                                NaN   \n",
       "2179220                                                NaN   \n",
       "1805992                                                NaN   \n",
       "1356658                                                NaN   \n",
       "171819   PROVIDED A RECREATIONAL FOOTBALL AND CHEERLEAD...   \n",
       "2221573                                                NaN   \n",
       "1781056  CONTRIBUTIONS MAKE IT POSSIBLE TO RAISE FUNDS ...   \n",
       "1442687                                                NaN   \n",
       "1561973  OPERATED MUSEUM AND EXHIBITS PROMOTING THE HIS...   \n",
       "646964                                                 NaN   \n",
       "188279                                                 NaN   \n",
       "2125699  Public Programs##History Museum and Conservati...   \n",
       "465402   Provided financial means to cover the cost of ...   \n",
       "2199188         SUPPORT THE BOARDMAN TOWNSHIP FIREFIGHTERS   \n",
       "661026                                                 NaN   \n",
       "206397   Purchased a parcel of land to be called Rotary...   \n",
       "368237   1. The Chamber organized and sponsored functio...   \n",
       "1841611                                                NaN   \n",
       "2023553                                                NaN   \n",
       "2083772  PROVIDED FLOWERS FOR FUNERALS; FOOD, LODGING, ...   \n",
       "1585174  Provided fire protection and other emergency s...   \n",
       "1840818                                                NaN   \n",
       "977706   Provide after school and family activities##Cu...   \n",
       "...                                                    ...   \n",
       "1579072                                                NaN   \n",
       "2067791  The Organization provides its 370 members with...   \n",
       "205187   TO PROMOTE SELF CONFIDENCE GOOD SPORTMANSHIP T...   \n",
       "394451                                                 NaN   \n",
       "693763                                                 NaN   \n",
       "146083                                                 NaN   \n",
       "1592706  Solicit public donations and using the proceed...   \n",
       "615991                                                 NaN   \n",
       "2108577                                                NaN   \n",
       "1899863  ANNUAL YEARBOOK AND WINDMILL PUBLICATION##CONV...   \n",
       "1421790                                                NaN   \n",
       "284173                     PRESERVATION OF MUSTARD HISTORY   \n",
       "1501021                                                NaN   \n",
       "267676   FINANCIAL ASSISTANCE:ASSISTANCE TO FAMILIES FO...   \n",
       "1797240                                                NaN   \n",
       "291532                                                 NaN   \n",
       "1857153                                                NaN   \n",
       "132249                                                 NaN   \n",
       "1336610  Licensee of noncommercial educational radio st...   \n",
       "1056545  PROVIDE HOUSING FACILITIES TO PEOPLE WITH LONG...   \n",
       "2060818                                                NaN   \n",
       "148552                                   EXEMPTION IS ZERO   \n",
       "2058713                                                NaN   \n",
       "2162466                                                NaN   \n",
       "1011657                                                NaN   \n",
       "1782576                                                NaN   \n",
       "1060295                                                NaN   \n",
       "1072834  HOLD TITLE TO REAL PROPERTY, COLLECT INCOME AN...   \n",
       "1826480  DURING 2013, WE SERVED INDIVIDUALS WITH DISABI...   \n",
       "2008855  On September 16 2014 organization made a contr...   \n",
       "\n",
       "                             IRS990EZ_p3_PrmryExmptPrpsTxt  \\\n",
       "135646                                                 NaN   \n",
       "1341150  TO SUPPLY ASSISTANCE TO FLORIDA ELECTRICAL SUB...   \n",
       "1932821  THE MISSION OF SEACOAST ARTS AND CULTURAL ALLI...   \n",
       "208342                                                 NaN   \n",
       "313275    FAMILY, EDUCATION, MUSIC & COMMUNITY ENRICHMENT.   \n",
       "681187                                                 NaN   \n",
       "676725                                                 NaN   \n",
       "1840454                                                NaN   \n",
       "2179220                                                NaN   \n",
       "1805992                                                NaN   \n",
       "1356658                                                NaN   \n",
       "171819            FOSTERING NATIONAL AMATEUR SPORTS COMPET   \n",
       "2221573                                                NaN   \n",
       "1781056  SUPPORT INNER CITY JUNIOR TENNIS AND EDUCATION...   \n",
       "1442687                                                NaN   \n",
       "1561973  OPERATE EXHIBITS AND MUSEUM FOR EDUCATIONAL PU...   \n",
       "646964                                                 NaN   \n",
       "188279                                                 NaN   \n",
       "2125699                Education - local community history   \n",
       "465402   To provide crucial emergency care to pets for ...   \n",
       "2199188         Support the Boardman Township Firefighters   \n",
       "661026                                                 NaN   \n",
       "206397   The Mendocino Rotary Foundation mission is to ...   \n",
       "368237   A. PRESERVING THE COMPETITIVE ENTERPRISE SYSTE...   \n",
       "1841611                                                NaN   \n",
       "2023553                                                NaN   \n",
       "2083772  TO ASSIST WIDOWS AND MEMBERS OF THE FIRE DEPAR...   \n",
       "1585174  Protect and preserve life and property and min...   \n",
       "1840818                                                NaN   \n",
       "977706   The organization is a school based parent teac...   \n",
       "...                                                    ...   \n",
       "1579072                                                NaN   \n",
       "2067791     Assist in locating employment for its members.   \n",
       "205187                             EDUCATION PHILANTHROPIC   \n",
       "394451                                                 NaN   \n",
       "693763                                                 NaN   \n",
       "146083                                                 NaN   \n",
       "1592706                    Support of local public schools   \n",
       "615991                                                 NaN   \n",
       "2108577                                                NaN   \n",
       "1899863                   PROFESSIONAL FLORIST ASSOCIATION   \n",
       "1421790                                                NaN   \n",
       "284173   MUSEUM ENDEAVORS TO FOSTER AN APPRECIATION FOR...   \n",
       "1501021                                                NaN   \n",
       "267676   ASSISTANCE TO INDIVIDUALS WHO ARE COPING WITH ...   \n",
       "1797240                                                NaN   \n",
       "291532                                                 NaN   \n",
       "1857153                                                NaN   \n",
       "132249                                                 NaN   \n",
       "1336610                           Educational Broadcasting   \n",
       "1056545  PROVIDE HOUSING FOR PEOPLE WITH SEVERE MENTAL ...   \n",
       "2060818                                                NaN   \n",
       "148552                                 CHAMBER OF COMMERCE   \n",
       "2058713                                                NaN   \n",
       "2162466                                                NaN   \n",
       "1011657                                                NaN   \n",
       "1782576                                                NaN   \n",
       "1060295                                                NaN   \n",
       "1072834  HOLD TITLE TO REAL PROPERTY, COLLECT INCOME AN...   \n",
       "1826480  LEG UP THERAPEUTIC RIDING CENTER USES EQUINE A...   \n",
       "2008855               To make contributions to other 501c3   \n",
       "\n",
       "        IRS990PF_p16b_RltnshpSttmntTxt IRS990PF_p9a_DscrptnTxt  \\\n",
       "135646                             NaN                     NaN   \n",
       "1341150                            NaN                     NaN   \n",
       "1932821                            NaN                     NaN   \n",
       "208342                             NaN                     NaN   \n",
       "313275                             NaN                     NaN   \n",
       "681187                             NaN                     NaN   \n",
       "676725                             NaN                     NaN   \n",
       "1840454                            NaN                     NaN   \n",
       "2179220                            NaN                     NaN   \n",
       "1805992                            NaN                     NaN   \n",
       "1356658                            NaN                     NaN   \n",
       "171819                             NaN                     NaN   \n",
       "2221573                            NaN                     NaN   \n",
       "1781056                            NaN                     NaN   \n",
       "1442687                            NaN                     NaN   \n",
       "1561973                            NaN                     NaN   \n",
       "646964                             NaN                     NaN   \n",
       "188279                             NaN                     NaN   \n",
       "2125699                            NaN                     NaN   \n",
       "465402                             NaN                     NaN   \n",
       "2199188                            NaN                     NaN   \n",
       "661026                             NaN                     NaN   \n",
       "206397                             NaN                     NaN   \n",
       "368237                             NaN                     NaN   \n",
       "1841611                            NaN                     NaN   \n",
       "2023553                            NaN                     NaN   \n",
       "2083772                            NaN                     NaN   \n",
       "1585174                            NaN                     NaN   \n",
       "1840818                            NaN                     NaN   \n",
       "977706                             NaN                     NaN   \n",
       "...                                ...                     ...   \n",
       "1579072                            NaN                     NaN   \n",
       "2067791                            NaN                     NaN   \n",
       "205187                             NaN                     NaN   \n",
       "394451                             NaN                     NaN   \n",
       "693763                             NaN                     NaN   \n",
       "146083                             NaN                     NaN   \n",
       "1592706                            NaN                     NaN   \n",
       "615991                             NaN                     NaN   \n",
       "2108577                            NaN                     NaN   \n",
       "1899863                            NaN                     NaN   \n",
       "1421790                            NaN                     NaN   \n",
       "284173                             NaN                     NaN   \n",
       "1501021                            NaN                     NaN   \n",
       "267676                             NaN                     NaN   \n",
       "1797240                            NaN                     NaN   \n",
       "291532                             NaN                     NaN   \n",
       "1857153                            NaN                     NaN   \n",
       "132249                             NaN                     NaN   \n",
       "1336610                            NaN                     NaN   \n",
       "1056545                            NaN                     NaN   \n",
       "2060818                            NaN                     NaN   \n",
       "148552                             NaN                     NaN   \n",
       "2058713                            NaN                     NaN   \n",
       "2162466                            NaN                     NaN   \n",
       "1011657                            NaN                     NaN   \n",
       "1782576                            NaN                     NaN   \n",
       "1060295                            NaN                     NaN   \n",
       "1072834                            NaN                     NaN   \n",
       "1826480                            NaN                     NaN   \n",
       "2008855                            NaN                     NaN   \n",
       "\n",
       "                                IRS990ScheduleO_ExplntnTxt  \\\n",
       "135646                                                       \n",
       "1341150  TO SUPPLY ASSISTANCE TO FLORIDA ELECTRICAL SUB...   \n",
       "1932821  THE MISSION OF SEACOAST ARTS AND CULTURAL ALLI...   \n",
       "208342                                                       \n",
       "313275                                                       \n",
       "681187                                                       \n",
       "676725                                                       \n",
       "1840454                                                      \n",
       "2179220                                                      \n",
       "1805992                                                      \n",
       "1356658                                                      \n",
       "171819                                                       \n",
       "2221573                                                      \n",
       "1781056                                                      \n",
       "1442687                                                      \n",
       "1561973                                                      \n",
       "646964                                                       \n",
       "188279                                                       \n",
       "2125699                                                      \n",
       "465402                                                       \n",
       "2199188                                                      \n",
       "661026                                                       \n",
       "206397                                                       \n",
       "368237                                                       \n",
       "1841611                                                      \n",
       "2023553                                                      \n",
       "2083772                                                      \n",
       "1585174                                                      \n",
       "1840818                                                      \n",
       "977706                                                       \n",
       "...                                                    ...   \n",
       "1579072                                                      \n",
       "2067791                                                      \n",
       "205187                                                       \n",
       "394451                                                       \n",
       "693763                                                       \n",
       "146083                                                       \n",
       "1592706                                                      \n",
       "615991                                                       \n",
       "2108577                                                      \n",
       "1899863                                       SCHOLARSHIPS   \n",
       "1421790                                                      \n",
       "284173   MUSEUM ENDEAVORS TO FOSTER AN APPRECIATION FOR...   \n",
       "1501021                                                      \n",
       "267676                                                       \n",
       "1797240                                                      \n",
       "291532                                                       \n",
       "1857153  OTHER PROGRAM SERVICES 4: LACBC offers bicycle...   \n",
       "132249                                                       \n",
       "1336610                                                      \n",
       "1056545                                                      \n",
       "2060818  UNION STATION KANSAS CITY, INC. SHALL BE RECOG...   \n",
       "148552                                                       \n",
       "2058713                                                      \n",
       "2162466                                                      \n",
       "1011657                                                      \n",
       "1782576                                                      \n",
       "1060295                                                      \n",
       "1072834                                                      \n",
       "1826480  LEG UP THERAPEUTIC RIDING CENTER USES EQUINE A...   \n",
       "2008855                                                      \n",
       "\n",
       "                                 IRS990_p1_ActvtyOrMssnDsc  \\\n",
       "135646   PROVIDE CHILD CARE SERVICES TO DISADVANTAGED C...   \n",
       "1341150                                                NaN   \n",
       "1932821                                                NaN   \n",
       "208342   TO PROVIDE MATERIALS AND SERVICES TO STIMULATE...   \n",
       "313275                                                 NaN   \n",
       "681187     PROVIDING ORCHESTRA PERFORMANCES FOR THE PUBLIC   \n",
       "676725                PROVIDES HEALTH AND WELFARE BENEFITS   \n",
       "1840454  PROVIDE AREA HOME BUILDERS AND GENERAL PUBLIC ...   \n",
       "2179220  PROVIDE IRRIGATION WATER TO FARMS AND HOMES AL...   \n",
       "1805992  TO OPERATE A COMMUNITY CENTER AND PROVIDE SUPP...   \n",
       "1356658  OUR MISSION IS TO EQUIP CHILDREN OF CHRISTIAN ...   \n",
       "171819                                                 NaN   \n",
       "2221573  SUPPORTIVE SERVICES TO INDIVIDUALS WITH MENTAL...   \n",
       "1781056                                                NaN   \n",
       "1442687  Set up and manage basketball Teams for the You...   \n",
       "1561973                                                NaN   \n",
       "646964   TO LEAD COLLABORATION AMONG LAW ENFORCEMENT EX...   \n",
       "188279   Osehra's mission is to build and support an op...   \n",
       "2125699                                                NaN   \n",
       "465402                                                 NaN   \n",
       "2199188                                                NaN   \n",
       "661026   PROVIDES EMERGENCY SHELTER TO HOMELESS OR DISL...   \n",
       "206397                                                 NaN   \n",
       "368237                                                 NaN   \n",
       "1841611  THE WILLAMETTANS IS A SOCIAL CLUB DESIGNED TO ...   \n",
       "2023553  To support and benefit the purposes of the Goo...   \n",
       "2083772                                                NaN   \n",
       "1585174                                                NaN   \n",
       "1840818  OUR MISSION IS TO PREVENT CRUELTY, ABUSE, NEGL...   \n",
       "977706                                                 NaN   \n",
       "...                                                    ...   \n",
       "1579072  To promote the development of parks, recreatio...   \n",
       "2067791                                                NaN   \n",
       "205187                                                 NaN   \n",
       "394451   THE HOSPITAL IS COMMITTED TO PROVIDING EXCELLE...   \n",
       "693763   PUBLIC EDUCATION ABOUT CRANIOFACIAL DEFORMITIE...   \n",
       "146083   TO CREATE AND FOSTER A SPIRIT OF UNDERSTANDING...   \n",
       "1592706                                                NaN   \n",
       "615991   Citizen Action of New York, Inc. is a not-for-...   \n",
       "2108577  TO PROVIDE MUSICAL EDUCATION & INSTRUMENTAL TR...   \n",
       "1899863                                                NaN   \n",
       "1421790  THE CENTER FOR HOUSING POLICY IS A RESEARCH OR...   \n",
       "284173                                                 NaN   \n",
       "1501021  AFT-WISCONSIN FOSTERS COOPERATIVE ACTION AMONG...   \n",
       "267676                                                 NaN   \n",
       "1797240  DISPLAY FINE ART TO THE GENERAL PUBLIC AND PRE...   \n",
       "291532   THE LOCAL'S MISSION IS TO REPRESENT IT'S MEMBE...   \n",
       "1857153  Bicycling-related advocacy, education and outr...   \n",
       "132249   ENTITY IS AN AFFILIATE OF THE NATIONAL NOT-FOR...   \n",
       "1336610                                                NaN   \n",
       "1056545                                                NaN   \n",
       "2060818  TO BE RECOGNIZED AS THE REGION'S FINEST EDUCAT...   \n",
       "148552                                                 NaN   \n",
       "2058713                To Support Providence Child Center.   \n",
       "2162466  THE CLARENDON COUNTY CHAMBER OF COMMERCE PROMO...   \n",
       "1011657  HELP LOCAL ORGANIZATIONS IN RECRUITING INDUSTR...   \n",
       "1782576    SUPPORT POLK COUNTY'S LAW ENFORCEMENT COMMUNITY   \n",
       "1060295  TO ENCOURAGE A HIGH STANDARD OF SKILL; FAIR ST...   \n",
       "1072834                                                NaN   \n",
       "1826480                                                NaN   \n",
       "2008855                                                NaN   \n",
       "\n",
       "                                            IRS990_p3_DscS  ...    YEAR  \\\n",
       "135646   THE ORGANIZATION PROVIDES COMPLETE CHILD CARE ...  ...  2015.0   \n",
       "1341150                                                NaN  ...  2014.0   \n",
       "1932821                                                NaN  ...  2014.0   \n",
       "208342   LIBRARY PROGRAMS - TO PROVIDE LIBRARY SERVICE,...  ...  2015.0   \n",
       "313275                                                 NaN  ...  2017.0   \n",
       "681187   SYMPHONY ORCHESTRA PERFORMANCES FOR THE GENERA...  ...  2014.0   \n",
       "676725   THE PLAN PROVIDES COMPREHENSIVE MEDICAL BENEFI...  ...  2014.0   \n",
       "1840454  HOME SHOW IS AN ANNUAL EVENT DEDICATED TO PROM...  ...  2014.0   \n",
       "2179220  THE COMPANY PROVIDED IRRIGATION WATER AND MAIN...  ...  2014.0   \n",
       "1805992  TO OPERATE A COMMUNITY CENTER AND PROVIDE SUPP...  ...  2014.0   \n",
       "1356658  OPERATE A PRIVATE SCHOOL FOR CHILDREN IN GRADE...  ...  2015.0   \n",
       "171819                                                 NaN  ...  2015.0   \n",
       "2221573  CONTINUUM OF CARE (COC) HOUSING PROGRAMS UTILI...  ...  2014.0   \n",
       "1781056                                                NaN  ...  2014.0   \n",
       "1442687  Set basketball teams for the youth of Newtown,...  ...  2015.0   \n",
       "1561973                                                NaN  ...  2014.0   \n",
       "646964   SEX OFFENDER ADDRESS VERIFICATION - PROVIDE GR...  ...  2014.0   \n",
       "188279   Annual ConferenceOSEHRA's annual open Source S...  ...  2015.0   \n",
       "2125699                                                NaN  ...  2014.0   \n",
       "465402                                                 NaN  ...  2016.0   \n",
       "2199188                                                NaN  ...  2014.0   \n",
       "661026   PROVIDES EMERGENCY SHELTER AND FOOD TO HOMELES...  ...  2014.0   \n",
       "206397                                                 NaN  ...  2015.0   \n",
       "368237                                                 NaN  ...  2015.0   \n",
       "1841611  SOCIAL CLUB SERVICES PROVIDED BY MEMBERS FOR T...  ...  2014.0   \n",
       "2023553  Grants to the Good News Broadcasting Associati...  ...  2015.0   \n",
       "2083772                                                NaN  ...  2014.0   \n",
       "1585174                                                NaN  ...  2014.0   \n",
       "1840818  ALL RELATED EXPENSES WERE DEDICATED TO A SINGL...  ...  2014.0   \n",
       "977706                                                 NaN  ...  2017.0   \n",
       "...                                                    ...  ...     ...   \n",
       "1579072  Outdoor classroom at Penitentary Glen 30 bench...  ...  2014.0   \n",
       "2067791                                                NaN  ...  2014.0   \n",
       "205187                                                 NaN  ...  2015.0   \n",
       "394451   BOONEVILLE COMMUNITY HOSPITAL (BCH) WAS A CRIT...  ...  2015.0   \n",
       "693763   PROVIDED HOSPITAL, SURGICAL SERVICES, SUPPLIES...  ...  2014.0   \n",
       "146083   TO HELP PROVIDE VISION CARE TO ALL THOSE WHO C...  ...  2014.0   \n",
       "1592706                                                NaN  ...  2014.0   \n",
       "615991   COMMUNITY ORGANIZING, ADVOCACY AND PUBLIC EDUC...  ...  2014.0   \n",
       "2108577  SYMPHONIC MUSIC EDUCATION PROGRAM AND THE CHAM...  ...  2014.0   \n",
       "1899863                                                NaN  ...  2014.0   \n",
       "1421790  RESEARCH, PUBLICATIONS, EVENTS & ONLINE TOOLS ...  ...  2014.0   \n",
       "284173                                                 NaN  ...  2017.0   \n",
       "1501021  THE AFT-WISCONSIN SOLIDARITY FUND IS A $2 PER ...  ...  2015.0   \n",
       "267676                                                 NaN  ...  2017.0   \n",
       "1797240  THE ORGANIZATION PROVIDES FUNDS FOR MAINTAININ...  ...  2014.0   \n",
       "291532   ALLOWS THE ORGANIZATION TO NEGOTIATE CONTRACTS...  ...  2017.0   \n",
       "1857153  LACBC organized campaigns for a more bikeable ...  ...  2014.0   \n",
       "132249   The organization operates a facility on an on-...  ...  2015.0   \n",
       "1336610                                                NaN  ...  2014.0   \n",
       "1056545                                                NaN  ...  2015.0   \n",
       "2060818     SEE SCHEDULE O##SEE SCHEDULE O##SEE SCHEDULE O  ...  2014.0   \n",
       "148552                                                 NaN  ...  2015.0   \n",
       "2058713  Funding of operating expenses of, or payments ...  ...  2014.0   \n",
       "2162466  MEMBERSHIP RETREAT - THE RETREAT PROVIDES NETW...  ...  2014.0   \n",
       "1011657  INTERACTS WITH LOCAL GOVERNMENT REGARDING OPER...  ...  2015.0   \n",
       "1782576  SUPPORTED MEMBERS OF POLK COUNTY'S LAW ENFORCE...  ...  2014.0   \n",
       "1060295  MAINTAIN WORK AGREEMENTS WITH THE CONTRACTOR A...  ...  2015.0   \n",
       "1072834                                                NaN  ...  2014.0   \n",
       "1826480                                                NaN  ...  2014.0   \n",
       "2008855                                                NaN  ...  2016.0   \n",
       "\n",
       "         95_and_before  NTEE1  \\\n",
       "135646             0.0      B   \n",
       "1341150            1.0      W   \n",
       "1932821            1.0      A   \n",
       "208342             1.0      B   \n",
       "313275             0.0      A   \n",
       "681187             1.0      A   \n",
       "676725             1.0      Y   \n",
       "1840454            1.0      L   \n",
       "2179220            1.0      K   \n",
       "1805992            1.0      P   \n",
       "1356658            1.0      B   \n",
       "171819             0.0      N   \n",
       "2221573            1.0      F   \n",
       "1781056            0.0      N   \n",
       "1442687            0.0      N   \n",
       "1561973            0.0      A   \n",
       "646964             1.0      I   \n",
       "188279             0.0      E   \n",
       "2125699            1.0      A   \n",
       "465402             0.0      D   \n",
       "2199188            1.0      M   \n",
       "661026             0.0      L   \n",
       "206397             0.0      S   \n",
       "368237             1.0      S   \n",
       "1841611            1.0      N   \n",
       "2023553            1.0      X   \n",
       "2083772            1.0      M   \n",
       "1585174            1.0      M   \n",
       "1840818            1.0      D   \n",
       "977706             1.0      B   \n",
       "...                ...    ...   \n",
       "1579072            1.0      N   \n",
       "2067791            0.0      J   \n",
       "205187             0.0      N   \n",
       "394451             0.0      E   \n",
       "693763             1.0      Q   \n",
       "146083             0.0      S   \n",
       "1592706            0.0      B   \n",
       "615991             1.0      S   \n",
       "2108577            0.0      A   \n",
       "1899863            1.0      A   \n",
       "1421790            1.0      L   \n",
       "284173             0.0      A   \n",
       "1501021            1.0      J   \n",
       "267676             0.0      P   \n",
       "1797240            1.0      A   \n",
       "291532             1.0      J   \n",
       "1857153            0.0      S   \n",
       "132249             1.0      Y   \n",
       "1336610            0.0      A   \n",
       "1056545            1.0      L   \n",
       "2060818            0.0      A   \n",
       "148552             1.0      S   \n",
       "2058713            1.0      P   \n",
       "2162466            1.0      S   \n",
       "1011657            1.0      S   \n",
       "1782576            0.0      I   \n",
       "1060295            1.0      J   \n",
       "1072834            1.0      J   \n",
       "1826480            0.0      P   \n",
       "2008855            0.0      G   \n",
       "\n",
       "                                                   mission  \\\n",
       "135646   PROVIDE CHILD CARE SERVICES TO DISADVANTAGED C...   \n",
       "1341150  TO SUPPLY ASSISTANCE TO FLORIDA ELECTRICAL SUB...   \n",
       "1932821  THE MISSION OF SEACOAST ARTS AND CULTURAL ALLI...   \n",
       "208342   TO PROVIDE MATERIALS AND SERVICES TO STIMULATE...   \n",
       "313275    FAMILY, EDUCATION, MUSIC & COMMUNITY ENRICHMENT.   \n",
       "681187     PROVIDING ORCHESTRA PERFORMANCES FOR THE PUBLIC   \n",
       "676725                PROVIDES HEALTH AND WELFARE BENEFITS   \n",
       "1840454  PROVIDE AREA HOME BUILDERS AND GENERAL PUBLIC ...   \n",
       "2179220  PROVIDE IRRIGATION WATER TO FARMS AND HOMES AL...   \n",
       "1805992  TO OPERATE A COMMUNITY CENTER AND PROVIDE SUPP...   \n",
       "1356658  OUR MISSION IS TO EQUIP CHILDREN OF CHRISTIAN ...   \n",
       "171819            FOSTERING NATIONAL AMATEUR SPORTS COMPET   \n",
       "2221573  SUPPORTIVE SERVICES TO INDIVIDUALS WITH MENTAL...   \n",
       "1781056  SUPPORT INNER CITY JUNIOR TENNIS AND EDUCATION...   \n",
       "1442687  SET UP AND MANAGE BASKETBALL TEAMS FOR THE YOU...   \n",
       "1561973  OPERATE EXHIBITS AND MUSEUM FOR EDUCATIONAL PU...   \n",
       "646964   TO LEAD COLLABORATION AMONG LAW ENFORCEMENT EX...   \n",
       "188279   OSEHRA'S MISSION IS TO BUILD AND SUPPORT AN OP...   \n",
       "2125699                EDUCATION - LOCAL COMMUNITY HISTORY   \n",
       "465402   TO PROVIDE CRUCIAL EMERGENCY CARE TO PETS FOR ...   \n",
       "2199188         SUPPORT THE BOARDMAN TOWNSHIP FIREFIGHTERS   \n",
       "661026   PROVIDES EMERGENCY SHELTER TO HOMELESS OR DISL...   \n",
       "206397   THE MENDOCINO ROTARY FOUNDATION MISSION IS TO ...   \n",
       "368237   A. PRESERVING THE COMPETITIVE ENTERPRISE SYSTE...   \n",
       "1841611  THE WILLAMETTANS IS A SOCIAL CLUB DESIGNED TO ...   \n",
       "2023553  TO SUPPORT AND BENEFIT THE PURPOSES OF THE GOO...   \n",
       "2083772  TO ASSIST WIDOWS AND MEMBERS OF THE FIRE DEPAR...   \n",
       "1585174  PROTECT AND PRESERVE LIFE AND PROPERTY AND MIN...   \n",
       "1840818  OUR MISSION IS TO PREVENT CRUELTY, ABUSE, NEGL...   \n",
       "977706   THE ORGANIZATION IS A SCHOOL BASED PARENT TEAC...   \n",
       "...                                                    ...   \n",
       "1579072  TO PROMOTE THE DEVELOPMENT OF PARKS, RECREATIO...   \n",
       "2067791     ASSIST IN LOCATING EMPLOYMENT FOR ITS MEMBERS.   \n",
       "205187                             EDUCATION PHILANTHROPIC   \n",
       "394451   THE HOSPITAL IS COMMITTED TO PROVIDING EXCELLE...   \n",
       "693763   PUBLIC EDUCATION ABOUT CRANIOFACIAL DEFORMITIE...   \n",
       "146083   TO CREATE AND FOSTER A SPIRIT OF UNDERSTANDING...   \n",
       "1592706                    SUPPORT OF LOCAL PUBLIC SCHOOLS   \n",
       "615991   CITIZEN ACTION OF NEW YORK, INC. IS A NOT-FOR-...   \n",
       "2108577  TO PROVIDE MUSICAL EDUCATION & INSTRUMENTAL TR...   \n",
       "1899863                   PROFESSIONAL FLORIST ASSOCIATION   \n",
       "1421790  THE CENTER FOR HOUSING POLICY IS A RESEARCH OR...   \n",
       "284173   MUSEUM ENDEAVORS TO FOSTER AN APPRECIATION FOR...   \n",
       "1501021  AFT-WISCONSIN FOSTERS COOPERATIVE ACTION AMONG...   \n",
       "267676   ASSISTANCE TO INDIVIDUALS WHO ARE COPING WITH ...   \n",
       "1797240  DISPLAY FINE ART TO THE GENERAL PUBLIC AND PRE...   \n",
       "291532   THE LOCAL'S MISSION IS TO REPRESENT IT'S MEMBE...   \n",
       "1857153  BICYCLING-RELATED ADVOCACY, EDUCATION AND OUTR...   \n",
       "132249   ENTITY IS AN AFFILIATE OF THE NATIONAL NOT-FOR...   \n",
       "1336610                           EDUCATIONAL BROADCASTING   \n",
       "1056545  PROVIDE HOUSING FOR PEOPLE WITH SEVERE MENTAL ...   \n",
       "2060818  TO BE RECOGNIZED AS THE REGION'S FINEST EDUCAT...   \n",
       "148552                                 CHAMBER OF COMMERCE   \n",
       "2058713                TO SUPPORT PROVIDENCE CHILD CENTER.   \n",
       "2162466  THE CLARENDON COUNTY CHAMBER OF COMMERCE PROMO...   \n",
       "1011657  HELP LOCAL ORGANIZATIONS IN RECRUITING INDUSTR...   \n",
       "1782576    SUPPORT POLK COUNTY'S LAW ENFORCEMENT COMMUNITY   \n",
       "1060295  TO ENCOURAGE A HIGH STANDARD OF SKILL; FAIR ST...   \n",
       "1072834  HOLD TITLE TO REAL PROPERTY, COLLECT INCOME AN...   \n",
       "1826480  LEG UP THERAPEUTIC RIDING CENTER USES EQUINE A...   \n",
       "2008855               TO MAKE CONTRIBUTIONS TO OTHER 501C3   \n",
       "\n",
       "                                                 prgrm_dsc  \\\n",
       "135646   THE ORGANIZATION PROVIDES COMPLETE CHILD CARE ...   \n",
       "1341150  SUPPLY ASSISTANCE TO FLORIDA ELECTRICAL SUB-CO...   \n",
       "1932821  THE MISSION OF SEACOAST ARTS AND CULTURAL ALLI...   \n",
       "208342   LIBRARY PROGRAMS - TO PROVIDE LIBRARY SERVICE,...   \n",
       "313275   UZN MCA SET THE GROUND WORK IN 2016 WITH FRIEN...   \n",
       "681187   SYMPHONY ORCHESTRA PERFORMANCES FOR THE GENERA...   \n",
       "676725   THE PLAN PROVIDES COMPREHENSIVE MEDICAL BENEFI...   \n",
       "1840454  MONTHLY DINNERS ARE HELD FOR MEMBERS IN WHICH ...   \n",
       "2179220  THE COMPANY PROVIDED IRRIGATION WATER AND MAIN...   \n",
       "1805992  TO OPERATE A COMMUNITY CENTER AND PROVIDE SUPP...   \n",
       "1356658  OPERATE A PRIVATE SCHOOL FOR CHILDREN IN GRADE...   \n",
       "171819   PROVIDED A RECREATIONAL FOOTBALL AND CHEERLEAD...   \n",
       "2221573  MIRCI GROUP HOMES CONSISTS OF TWO SIX-BEDROOM ...   \n",
       "1781056  CONTRIBUTIONS MAKE IT POSSIBLE TO RAISE FUNDS ...   \n",
       "1442687  SET BASKETBALL TEAMS FOR THE YOUTH OF NEWTOWN,...   \n",
       "1561973  OPERATED MUSEUM AND EXHIBITS PROMOTING THE HIS...   \n",
       "646964   SEX OFFENDER ADDRESS VERIFICATION - PROVIDE GR...   \n",
       "188279   OPEN SOURCE COMMUNITY SERVICESOSEHRA PROVIDES ...   \n",
       "2125699  HISTORY MUSEUM AND CONSERVATION PROGRAMS; PUBL...   \n",
       "465402   PROVIDED FINANCIAL MEANS TO COVER THE COST OF ...   \n",
       "2199188         SUPPORT THE BOARDMAN TOWNSHIP FIREFIGHTERS   \n",
       "661026   PROVIDES EMERGENCY SHELTER AND FOOD TO HOMELES...   \n",
       "206397   PURCHASED A PARCEL OF LAND TO BE CALLED ROTARY...   \n",
       "368237   1. THE CHAMBER ORGANIZED AND SPONSORED FUNCTIO...   \n",
       "1841611  SOCIAL CLUB SERVICES PROVIDED BY MEMBERS FOR T...   \n",
       "2023553  GRANTS TO THE GOOD NEWS BROADCASTING ASSOCIATI...   \n",
       "2083772  PROVIDED FLOWERS FOR FUNERALS; FOOD, LODGING, ...   \n",
       "1585174  PROVIDED FIRE PROTECTION AND OTHER EMERGENCY S...   \n",
       "1840818  ALL RELATED EXPENSES WERE DEDICATED TO A SINGL...   \n",
       "977706   PROVIDE AFTER SCHOOL AND FAMILY ACTIVITIES; PR...   \n",
       "...                                                    ...   \n",
       "1579072  GORGE PANEL REPLACEMENT AT PG; OUTDOOR CLASSRO...   \n",
       "2067791  THE ORGANIZATION PROVIDES ITS 370 MEMBERS WITH...   \n",
       "205187   TO PROMOTE SELF CONFIDENCE GOOD SPORTMANSHIP T...   \n",
       "394451   BOONEVILLE COMMUNITY HOSPITAL (BCH) WAS A CRIT...   \n",
       "693763   PROVIDED SURGICAL FELLOWSHIPS AND OTHER FORMS ...   \n",
       "146083   TO HELP PROVIDE VISION CARE TO ALL THOSE WHO C...   \n",
       "1592706  SOLICIT PUBLIC DONATIONS AND USING THE PROCEED...   \n",
       "615991   COMMUNITY ORGANIZING, ADVOCACY AND PUBLIC EDUC...   \n",
       "2108577  SYMPHONIC MUSIC EDUCATION PROGRAM AND THE CHAM...   \n",
       "1899863  SCHOLARSHIPS; CONVENTION, SEMINARS, TRADE SHOW...   \n",
       "1421790  RESEARCH, PUBLICATIONS, EVENTS AND ONLINE TOOL...   \n",
       "284173   MUSEUM ENDEAVORS TO FOSTER AN APPRECIATION FOR...   \n",
       "1501021  THE 2013 CONVENTION IS THE ANNUAL MEETING OF L...   \n",
       "267676   FINANCIAL ASSISTANCE:ASSISTANCE TO FAMILIES FO...   \n",
       "1797240  THE ORGANIZATION PROVIDES FUNDS FOR MAINTAININ...   \n",
       "291532   ALLOWS THE ORGANIZATION TO NEGOTIATE CONTRACTS...   \n",
       "1857153  OTHER PROGRAM SERVICES 4: LACBC OFFERS BICYCLE...   \n",
       "132249   THE LOCAL ORGANIZATION HELPS DEFRAY THE COST O...   \n",
       "1336610  LICENSEE OF NONCOMMERCIAL EDUCATIONAL RADIO ST...   \n",
       "1056545  PROVIDE HOUSING FACILITIES TO PEOPLE WITH LONG...   \n",
       "2060818  SEE SCHEDULE O; UNION STATION KANSAS CITY'S OT...   \n",
       "148552                                   EXEMPTION IS ZERO   \n",
       "2058713  FUNDING OF CAPITAL EXPENDITURES AND EQUIPMENT ...   \n",
       "2162466  CLARENDON COUNTY STRIPED BASS FESTIVAL - THE A...   \n",
       "1011657  INTERACTS WITH LOCAL GOVERNMENT REGARDING OPER...   \n",
       "1782576  SUPPORTED MEMBERS OF POLK COUNTY'S LAW ENFORCE...   \n",
       "1060295  MAINTAIN WORK AGREEMENTS WITH THE CONTRACTOR A...   \n",
       "1072834  HOLD TITLE TO REAL PROPERTY, COLLECT INCOME AN...   \n",
       "1826480  DURING 2013, WE SERVED INDIVIDUALS WITH DISABI...   \n",
       "2008855  ON SEPTEMBER 16 2014 ORGANIZATION MADE A CONTR...   \n",
       "\n",
       "                                          mission_spellchk  \\\n",
       "135646   PROVIDE CHILD CARE SERVICES TO DISADVANTAGED C...   \n",
       "1341150  TO SUPPLY ASSISTANCE TO FLORIDA ELECTRICAL SUB...   \n",
       "1932821  THE MISSION OF SEACOAST ARTS AND CULTURAL ALLI...   \n",
       "208342   TO PROVIDE MATERIALS AND SERVICES TO STIMULATE...   \n",
       "313275   FAMILY , EDUCATION , MUSIC & COMMUNITY ENRICHM...   \n",
       "681187     PROVIDING ORCHESTRA PERFORMANCES FOR THE PUBLIC   \n",
       "676725                PROVIDES HEALTH AND WELFARE BENEFITS   \n",
       "1840454  PROVIDE AREA HOME BUILDERS AND GENERAL PUBLIC ...   \n",
       "2179220  PROVIDE IRRIGATION WATER TO FARMS AND HOMES AL...   \n",
       "1805992  TO OPERATE A COMMUNITY CENTER AND PROVIDE SUPP...   \n",
       "1356658  OUR MISSION IS TO EQUIP CHILDREN OF CHRISTIAN ...   \n",
       "171819           FOSTERING NATIONAL AMATEUR SPORTS COMPETE   \n",
       "2221573  SUPPORTIVE SERVICES TO INDIVIDUALS WITH MENTAL...   \n",
       "1781056  SUPPORT INNER CITY JUNIOR TENNIS AND EDUCATION...   \n",
       "1442687  SET UP AND MANAGE BASKETBALL TEAMS FOR THE YOU...   \n",
       "1561973  OPERATE EXHIBITS AND MUSEUM FOR EDUCATIONAL PU...   \n",
       "646964   TO LEAD COLLABORATION AMONG LAW ENFORCEMENT EX...   \n",
       "188279   OPERA IS MISSION IS TO BUILD AND SUPPORT AN OP...   \n",
       "2125699                EDUCATION - LOCAL COMMUNITY HISTORY   \n",
       "465402   TO PROVIDE CRUCIAL EMERGENCY CARE TO PETS FOR ...   \n",
       "2199188         SUPPORT THE BOARDMAN TOWNSHIP FIREFIGHTERS   \n",
       "661026   PROVIDES EMERGENCY SHELTER TO HOMELESS OR DISL...   \n",
       "206397   THE MENDOCINO ROTARY FOUNDATION MISSION IS TO ...   \n",
       "368237   A . PRESERVING THE COMPETITIVE ENTERPRISE SYST...   \n",
       "1841611  THE WILLAMETTANS IS A SOCIAL CLUB DESIGNED TO ...   \n",
       "2023553  TO SUPPORT AND BENEFIT THE PURPOSES OF THE GOO...   \n",
       "2083772  TO ASSIST WIDOWS AND MEMBERS OF THE FIRE DEPAR...   \n",
       "1585174  PROTECT AND PRESERVE LIFE AND PROPERTY AND MIN...   \n",
       "1840818  OUR MISSION IS TO PREVENT CRUELTY , ABUSE , NE...   \n",
       "977706   THE ORGANIZATION IS A SCHOOL BASED PARENT TEAC...   \n",
       "...                                                    ...   \n",
       "1579072  TO PROMOTE THE DEVELOPMENT OF PARKS , RECREATI...   \n",
       "2067791    ASSIST IN LOCATING EMPLOYMENT FOR ITS MEMBERS .   \n",
       "205187                             EDUCATION PHILANTHROPIC   \n",
       "394451   THE HOSPITAL IS COMMITTED TO PROVIDING EXCELLE...   \n",
       "693763   PUBLIC EDUCATION ABOUT CRANIOFACIAL DEFORMITIE...   \n",
       "146083   TO CREATE AND FOSTER A SPIRIT OF UNDERSTANDING...   \n",
       "1592706                    SUPPORT OF LOCAL PUBLIC SCHOOLS   \n",
       "615991   CITIZEN ACTION OF NEW YORK , INC. IS A NOT-FOR...   \n",
       "2108577  TO PROVIDE MUSICAL EDUCATION & INSTRUMENTAL TR...   \n",
       "1899863                   PROFESSIONAL FLORIST ASSOCIATION   \n",
       "1421790  THE CENTER FOR HOUSING POLICY IS A RESEARCH OR...   \n",
       "284173   MUSEUM ENDEAVOURS TO FOSTER AN APPRECIATION FO...   \n",
       "1501021  AFT-WISCONSIN FOSTERS COOPERATIVE ACTION AMONG...   \n",
       "267676   ASSISTANCE TO INDIVIDUALS WHO ARE COPING WITH ...   \n",
       "1797240  DISPLAY FINE ART TO THE GENERAL PUBLIC AND PRE...   \n",
       "291532   THE LOCAL IS MISSION IS TO REPRESENT IT IS MEM...   \n",
       "1857153  BICYCLING-RELATED ADVOCACY , EDUCATION AND OUT...   \n",
       "132249   ENTITY IS AN AFFILIATE OF THE NATIONAL NOT-FOR...   \n",
       "1336610                           EDUCATIONAL BROADCASTING   \n",
       "1056545  PROVIDE HOUSING FOR PEOPLE WITH SEVERE MENTAL ...   \n",
       "2060818  TO BE RECOGNIZED AS THE REGION IS FINEST EDUCA...   \n",
       "148552                                 CHAMBER OF COMMERCE   \n",
       "2058713               TO SUPPORT PROVIDENCE CHILD CENTER .   \n",
       "2162466  THE CLARENDON COUNTY CHAMBER OF COMMERCE PROMO...   \n",
       "1011657  HELP LOCAL ORGANIZATIONS IN RECRUITING INDUSTR...   \n",
       "1782576   SUPPORT POLL COUNTY IS LAW ENFORCEMENT COMMUNITY   \n",
       "1060295  TO ENCOURAGE A HIGH STANDARD OF SKILL ; FAIR S...   \n",
       "1072834  HOLD TITLE TO REAL PROPERTY , COLLECT INCOME A...   \n",
       "1826480  LEG UP THERAPEUTIC RIDING CENTER USES EQUINE A...   \n",
       "2008855               TO MAKE CONTRIBUTIONS TO OTHER 50123   \n",
       "\n",
       "                                        prgrm_dsc_spellchk  \\\n",
       "135646   THE ORGANIZATION PROVIDES COMPLETE CHILD CARE ...   \n",
       "1341150  SUPPLY ASSISTANCE TO FLORIDA ELECTRICAL SUB-CO...   \n",
       "1932821  THE MISSION OF SEACOAST ARTS AND CULTURAL ALLI...   \n",
       "208342   LIBRARY PROGRAMS - TO PROVIDE LIBRARY SERVICE ...   \n",
       "313275   UN MCA SET THE GROUND WORK IN 2016 WITH FRIEND...   \n",
       "681187   SYMPHONY ORCHESTRA PERFORMANCES FOR THE GENERA...   \n",
       "676725   THE PLAN PROVIDES COMPREHENSIVE MEDICAL BENEFI...   \n",
       "1840454  MONTHLY DINNERS ARE HELD FOR MEMBERS IN WHICH ...   \n",
       "2179220  THE COMPANY PROVIDED IRRIGATION WATER AND MAIN...   \n",
       "1805992  TO OPERATE A COMMUNITY CENTER AND PROVIDE SUPP...   \n",
       "1356658  OPERATE A PRIVATE SCHOOL FOR CHILDREN IN GRADE...   \n",
       "171819   PROVIDED A RECREATIONAL FOOTBALL AND CHEERLEAD...   \n",
       "2221573  MERCI GROUP HOMES CONSISTS OF TWO SIX-BEDROOM ...   \n",
       "1781056  CONTRIBUTIONS MAKE IT POSSIBLE TO RAISE FUNDS ...   \n",
       "1442687  SET BASKETBALL TEAMS FOR THE YOUTH OF NEWTOWN ...   \n",
       "1561973  OPERATED MUSEUM AND EXHIBITS PROMOTING THE HIS...   \n",
       "646964   SEX OFFENDER ADDRESS VERIFICATION - PROVIDE GR...   \n",
       "188279   OPEN SOURCE COMMUNITY SERVICESOSEHRA PROVIDES ...   \n",
       "2125699  HISTORY MUSEUM AND CONSERVATION PROGRAMS ; PUB...   \n",
       "465402   PROVIDED FINANCIAL MEANS TO COVER THE COST OF ...   \n",
       "2199188         SUPPORT THE BOARDMAN TOWNSHIP FIREFIGHTERS   \n",
       "661026   PROVIDES EMERGENCY SHELTER AND FOOD TO HOMELES...   \n",
       "206397   PURCHASED A PARCEL OF LAND TO BE CALLED ROTARY...   \n",
       "368237   1 . THE CHAMBER ORGANIZED AND SPONSORED FUNCTI...   \n",
       "1841611  SOCIAL CLUB SERVICES PROVIDED BY MEMBERS FOR T...   \n",
       "2023553  GRANTS TO THE GOOD NEWS BROADCASTING ASSOCIATI...   \n",
       "2083772  PROVIDED FLOWERS FOR FUNERALS ; FOOD , LODGING...   \n",
       "1585174  PROVIDED FIRE PROTECTION AND OTHER EMERGENCY S...   \n",
       "1840818  ALL RELATED EXPENSES WERE DEDICATED TO A SINGL...   \n",
       "977706   PROVIDE AFTER SCHOOL AND FAMILY ACTIVITIES ; P...   \n",
       "...                                                    ...   \n",
       "1579072  GORGE PANEL REPLACEMENT AT PG ; OUTDOOR CLASSR...   \n",
       "2067791  THE ORGANIZATION PROVIDES ITS 370 MEMBERS WITH...   \n",
       "205187   TO PROMOTE SELF CONFIDENCE GOOD SPORTSMANSHIP ...   \n",
       "394451   BONNEVILLE COMMUNITY HOSPITAL ( BC ) WAS A CRI...   \n",
       "693763   PROVIDED SURGICAL FELLOWSHIPS AND OTHER FORMS ...   \n",
       "146083   TO HELP PROVIDE VISION CARE TO ALL THOSE WHO C...   \n",
       "1592706  SOLICIT PUBLIC DONATIONS AND USING THE PROCEED...   \n",
       "615991   COMMUNITY ORGANIZING , ADVOCACY AND PUBLIC EDU...   \n",
       "2108577  SYMPHONIC MUSIC EDUCATION PROGRAM AND THE CHAM...   \n",
       "1899863  SCHOLARSHIPS ; CONVENTION , SEMINARS , TRADE S...   \n",
       "1421790  RESEARCH , PUBLICATIONS , EVENTS AND ONLINE TO...   \n",
       "284173   MUSEUM ENDEAVOURS TO FOSTER AN APPRECIATION FO...   \n",
       "1501021  THE 2013 CONVENTION IS THE ANNUAL MEETING OF L...   \n",
       "267676   FINANCIAL ASSISTANCE : ASSISTANCE TO FAMILIES ...   \n",
       "1797240  THE ORGANIZATION PROVIDES FUNDS FOR MAINTAININ...   \n",
       "291532   ALLOWS THE ORGANIZATION TO NEGOTIATE CONTRACTS...   \n",
       "1857153  OTHER PROGRAM SERVICES 4 : LACK OFFERS BICYCLE...   \n",
       "132249   THE LOCAL ORGANIZATION HELPS DEFRAY THE COST O...   \n",
       "1336610  LICENSEE OF NON-COMMERCIAL EDUCATIONAL RADIO S...   \n",
       "1056545  PROVIDE HOUSING FACILITIES TO PEOPLE WITH LONG...   \n",
       "2060818  SEE SCHEDULE O ; UNION STATION KANSAS CITY IS ...   \n",
       "148552                                   EXEMPTION IS ZERO   \n",
       "2058713  FUNDING OF CAPITAL EXPENDITURES AND EQUIPMENT ...   \n",
       "2162466  CLARENDON COUNTY STRIPED BASS FESTIVAL - THE A...   \n",
       "1011657  INTERACTS WITH LOCAL GOVERNMENT REGARDING OPER...   \n",
       "1782576  SUPPORTED MEMBERS OF POLL COUNTY IS LAW ENFORC...   \n",
       "1060295  MAINTAIN WORK AGREEMENTS WITH THE CONTRACTOR A...   \n",
       "1072834  HOLD TITLE TO REAL PROPERTY , COLLECT INCOME A...   \n",
       "1826480  DURING 2013 , WE SERVED INDIVIDUALS WITH DISAB...   \n",
       "2008855  ON SEPTEMBER 16 2014 ORGANIZATION MADE A CONTR...   \n",
       "\n",
       "                                             mission_prgrm  \\\n",
       "135646   SINGING RIVER EDUCATION ASSOCIATION PROVIDE CH...   \n",
       "1341150  FLORIDA ASSOC OF ELECTRICAL CONTRACTORS INC TO...   \n",
       "1932821  SEACOAST ARTS AND CULTURAL ALLIANCE THE MISSIO...   \n",
       "208342   BUCKS COUNTY FREE LIBRARY TO PROVIDE MATERIALS...   \n",
       "313275   UZN MULTI CULTURAL ARTS CORPORATION FAMILY, ED...   \n",
       "681187   OKLAHOMA PHILHARMONIC SOCIETY INC PROVIDING OR...   \n",
       "676725   MINNEAPOLIS RETAIL MEAT CUTTERS AND FOOD HANDL...   \n",
       "1840454  HOME BUILDERS ASSOC OF GREATER SW IL PROVIDE A...   \n",
       "2179220  RUDY IRRIGATION CANAL COMPANY PROVIDE IRRIGATI...   \n",
       "1805992  LOFT- THE LESBIAN AND GAY COMMUNITY SERVICES C...   \n",
       "1356658  RAPID CITY CHRISTIAN EDUCATION ASSOC OUR MISSI...   \n",
       "171819   LAKE CITIES FOOTBALL AND CHEERLEADING ASSOCIAT...   \n",
       "2221573  MENTAL ILLNESS RECOVERY CENTER INC SUPPORTIVE ...   \n",
       "1781056  SOUTHERN CONNECTICUT ALLIANCE FOR INNER- CITY ...   \n",
       "1442687  NEWTOWN YOUTH BASKETBALL ASSOCIATION CO SET UP...   \n",
       "1561973  STEEL PLANT MUSEUM INC OPERATE EXHIBITS AND MU...   \n",
       "646964   WASHINGTON ASSOCIATION OF SHERIFFS AND POLICE ...   \n",
       "188279   OPEN SOURCE ELECTRONIC HEALTH RECORD ALLIANCE ...   \n",
       "2125699  LAUREL HISTORICAL SOCIETY INC EDUCATION - LOCA...   \n",
       "465402   NIAGARA FRONTIER VETERINARY SOCIETY PET EMERGE...   \n",
       "2199188  BOARDMAN FIREFIGHTERS LOCAL 1176 IAFF SUPPORT ...   \n",
       "661026   FAMILY SHELTER INC PROVIDES EMERGENCY SHELTER ...   \n",
       "206397   MENDOCINO ROTARY FOUNDATION THE MENDOCINO ROTA...   \n",
       "368237   LINCOLN HEIGHTS CHAMBER OF COMMERCE A. PRESERV...   \n",
       "1841611  WILLAMETTANS THE WILLAMETTANS IS A SOCIAL CLUB...   \n",
       "2023553  BACK TO THE BIBLE FOUNDATION TO SUPPORT AND BE...   \n",
       "2083772  CHRISTIAN FIREFIGHTERS ASSOCIATION DALLAS DEPA...   \n",
       "1585174  ELEPHANT HEAD VOLUNTEER FIRE DEPARTMENT INC PR...   \n",
       "1840818  HUMANE SOCIETY OF PORTAGE COUNTY I OUR MISSION...   \n",
       "977706   PTA WEST WOODS CT CONGRESSPTA THE ORGANIZATION...   \n",
       "...                                                    ...   \n",
       "1579072  LAKE PARKS FOUNDATION TO PROMOTE THE DEVELOPME...   \n",
       "2067791  UNITED BROTHERHOOD OF MILLWRIGHTS 1348 ASSIST ...   \n",
       "205187   MIDCREST PANTHERS FOOTBALL ORG LTD EDUCATION P...   \n",
       "394451   BOONEVILLE COMMUNITY HOSPITAL THE HOSPITAL IS ...   \n",
       "693763   WORLD CRANIOFACIAL FOUNDATION PUBLIC EDUCATION...   \n",
       "146083   MONTEREY LIONS CLUB INC TO CREATE AND FOSTER A...   \n",
       "1592706  FRIENDS OF THE HANOVER-NORWICH SCHOOLSINC SUPP...   \n",
       "615991   CITIZEN ACTION OF NEW YORK INC CITIZEN ACTION ...   \n",
       "2108577  CLAP AND TAP CHAMBER ORCHESTRA TO PROVIDE MUSI...   \n",
       "1899863  WESTEXAS NEW MEXICO FLORIST ASSOC PROFESSIONAL...   \n",
       "1421790  CENTER FOR HOUSING POLICY THE CENTER FOR HOUSI...   \n",
       "284173   NATIONAL MUSTARD MUSEUM INC MUSEUM ENDEAVORS T...   \n",
       "1501021  AMERICAN FEDERATION OF TEACHERS AFT-WISCONSIN ...   \n",
       "267676   HOSPICE SUPPORT FOUNDATION ASSISTANCE TO INDIV...   \n",
       "1797240  INTERNATIONAL ASSOCIATION FOR THE VISUAL ARTS ...   \n",
       "291532   UNITE HERE LOCAL 26 HOTEL WORKERS UNION THE LO...   \n",
       "1857153  LOS ANGELES COUNTY BICYCLE COALITION BICYCLING...   \n",
       "132249   FRATERNAL ORDER OF EAGLES 148 AERIE ENTITY IS ...   \n",
       "1336610  DAYSTAR PUBLIC RADIO INC EDUCATIONAL BROADCAST...   \n",
       "1056545  MEADOWLARK HOUSING INC PROVIDE HOUSING FOR PEO...   \n",
       "2060818  UNION STATION KANSAS CITY INC TO BE RECOGNIZED...   \n",
       "148552   VERSAILLES CHAMBER OF COMMERCE CHAMBER OF COMM...   \n",
       "2058713  PROVIDENCE CHILD CENTER FOUNDATION TO SUPPORT ...   \n",
       "2162466  CLARENDON CNTY CHAMBER OF COMMERCE THE CLAREND...   \n",
       "1011657  JOHNSTON COUNTY ECONOMIC DEVELOPMENT CORPORATI...   \n",
       "1782576  POLK SHERIFFS CHARITIES INC SUPPORT POLK COUNT...   \n",
       "1060295  OPERATIVE PLASTERERS AND CEMENT MASON TO ENCOU...   \n",
       "1072834  IRON WORKERS LOCAL 361 REALTY CORP HOLD TITLE ...   \n",
       "1826480  LEG UP THERAPEUTIC RIDING CENTER LEG UP THERAP...   \n",
       "2008855  HEARING EERS INC TO MAKE CONTRIBUTIONS TO OTHE...   \n",
       "\n",
       "                                    mission_prgrm_spellchk broad_cat  \n",
       "135646   SINGING RIVER EDUCATION ASSOCIATION PROVIDE CH...        II  \n",
       "1341150  FLORIDA ASSOC OF ELECTRICAL CONTRACTORS INC TO...       VII  \n",
       "1932821  SEACOAST ARTS AND CULTURAL ALLIANCE THE MISSIO...         I  \n",
       "208342   BUCKS COUNTY FREE LIBRARY TO PROVIDE MATERIALS...        II  \n",
       "313275   UZN MULTI CULTURAL ARTS CORPORATION FAMILY , E...         I  \n",
       "681187   OKLAHOMA PHILHARMONIC SOCIETY INC PROVIDING OR...         I  \n",
       "676725   MINNEAPOLIS RETAIL MEAT CUTTERS AND FOOD HANDL...        IX  \n",
       "1840454  HOME BUILDERS ASSOC OF GREATER SW IL PROVIDE A...         V  \n",
       "2179220  RUDY IRRIGATION CANAL COMPANY PROVIDE IRRIGATI...         V  \n",
       "1805992  LOFT- THE LESBIAN AND GAY COMMUNITY SERVICES C...         V  \n",
       "1356658  RAPID CITY CHRISTIAN EDUCATION ASSOC OUR MISSI...        II  \n",
       "171819   LAKE CITIES FOOTBALL AND CHEERLEADING ASSOCIAT...         V  \n",
       "2221573  MENTAL ILLNESS RECOVERY CENTER INC SUPPORTIVE ...        IV  \n",
       "1781056  SOUTHERN CONNECTICUT ALLIANCE FOR INNER- CITY ...         V  \n",
       "1442687  NEWTOWN YOUTH BASKETBALL ASSOCIATION CO SET UP...         V  \n",
       "1561973  STEEL PLANT MUSEUM INC OPERATE EXHIBITS AND MU...         I  \n",
       "646964   WASHINGTON ASSOCIATION OF SHERIFFS AND POLICE ...         V  \n",
       "188279   OPEN SOURCE ELECTRONIC HEALTH RECORD ALLIANCE ...        IV  \n",
       "2125699  LAUREL HISTORICAL SOCIETY INC EDUCATION - LOCA...         I  \n",
       "465402   NIAGARA FRONTIER VETERINARY SOCIETY PET EMERGE...       III  \n",
       "2199188  BOARDMAN FIREFIGHTERS LOCAL 1176 IAFF SUPPORT ...         V  \n",
       "661026   FAMILY SHELTER INC PROVIDES EMERGENCY SHELTER ...         V  \n",
       "206397   MENDOCINO ROTARY FOUNDATION THE MENDOCINO ROTA...       VII  \n",
       "368237   LINCOLN HEIGHTS CHAMBER OF COMMERCE A . PRESER...       VII  \n",
       "1841611  WILLAMETTANS THE WILLAMETTANS IS A SOCIAL CLUB...         V  \n",
       "2023553  BACK TO THE BIBLE FOUNDATION TO SUPPORT AND BE...      VIII  \n",
       "2083772  CHRISTIAN FIREFIGHTERS ASSOCIATION DALLAS DEPA...         V  \n",
       "1585174  ELEPHANT HEAD VOLUNTEER FIRE DEPARTMENT INC PR...         V  \n",
       "1840818  HUMANE SOCIETY OF PORTAGE COUNTY I OUR MISSION...       III  \n",
       "977706   PTA WEST WOODS CT CONGRESSPTA THE ORGANIZATION...        II  \n",
       "...                                                    ...       ...  \n",
       "1579072  LAKE PARKS FOUNDATION TO PROMOTE THE DEVELOPME...         V  \n",
       "2067791  UNITED BROTHERHOOD OF MILLWRIGHTS 1348 ASSIST ...         V  \n",
       "205187   MIDCREST PANTHERS FOOTBALL ORG LTD EDUCATION P...         V  \n",
       "394451   BOONEVILLE COMMUNITY HOSPITAL THE HOSPITAL IS ...        IV  \n",
       "693763   WORLD CRANIOFACIAL FOUNDATION PUBLIC EDUCATION...        VI  \n",
       "146083   MONTEREY LIONS CLUB INC TO CREATE AND FOSTER A...       VII  \n",
       "1592706  FRIENDS OF THE HANOVER-NORWICH SCHOOLSINC SUPP...        II  \n",
       "615991   CITIZEN ACTION OF NEW YORK INC CITIZEN ACTION ...       VII  \n",
       "2108577  CLAP AND TAP CHAMBER ORCHESTRA TO PROVIDE MUSI...         I  \n",
       "1899863  WESTEXAS NEW MEXICO FLORIST ASSOC PROFESSIONAL...         I  \n",
       "1421790  CENTER FOR HOUSING POLICY THE CENTER FOR HOUSI...         V  \n",
       "284173   NATIONAL MUSTARD MUSEUM INC MUSEUM ENDEAVOURS ...         I  \n",
       "1501021  AMERICAN FEDERATION OF TEACHERS AFT-WISCONSIN ...         V  \n",
       "267676   HOSPICE SUPPORT FOUNDATION ASSISTANCE TO INDIV...         V  \n",
       "1797240  INTERNATIONAL ASSOCIATION FOR THE VISUAL ARTS ...         I  \n",
       "291532   UNITE HERE LOCAL 26 HOTEL WORKERS UNION THE LO...         V  \n",
       "1857153  LOS ANGELES COUNTY BICYCLE COALITION BICYCLING...       VII  \n",
       "132249   FRATERNAL ORDER OF EAGLES 148 AERIE ENTITY IS ...        IX  \n",
       "1336610  DAYSTAR PUBLIC RADIO INC EDUCATIONAL BROADCAST...         I  \n",
       "1056545  MEADOWLARK HOUSING INC PROVIDE HOUSING FOR PEO...         V  \n",
       "2060818  UNION STATION KANSAS CITY INC TO BE RECOGNIZED...         I  \n",
       "148552   VERSAILLES CHAMBER OF COMMERCE CHAMBER OF COMM...       VII  \n",
       "2058713  PROVIDENCE CHILD CENTER FOUNDATION TO SUPPORT ...         V  \n",
       "2162466  CLARENDON CNTY CHAMBER OF COMMERCE THE CLAREND...       VII  \n",
       "1011657  JOHNSTON COUNTY ECONOMIC DEVELOPMENT CORPORATI...       VII  \n",
       "1782576  POLK SHERIFFS CHARITIES INC SUPPORT POLL COUNT...         V  \n",
       "1060295  OPERATIVE PLASTERERS AND CEMENT MASON TO ENCOU...         V  \n",
       "1072834  IRON WORKERS LOCAL 361 REALTY CORP HOLD TITLE ...         V  \n",
       "1826480  LEG UP THERAPEUTIC RIDING CENTER LEG UP THERAP...         V  \n",
       "2008855  HEARING EERS INC TO MAKE CONTRIBUTIONS TO OTHE...        IV  \n",
       "\n",
       "[154424 rows x 27 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "broad_cat\n",
      "I       13687\n",
      "II      20694\n",
      "III      6060\n",
      "IV      13488\n",
      "IX       5331\n",
      "V       37213\n",
      "VI       1637\n",
      "VII     21774\n",
      "VIII     3655\n",
      "Name: EIN, dtype: int64 \n",
      "\n",
      " broad_cat\n",
      "I       3323\n",
      "II      5133\n",
      "III     1502\n",
      "IV      3348\n",
      "IX      1309\n",
      "V       9521\n",
      "VI       350\n",
      "VII     5488\n",
      "VIII     911\n",
      "Name: EIN, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# # Check if the sampling criteria can be satisfied.\n",
    "# small_num=0\n",
    "# while small_num<500: # Make sure each category in training dataset has at least 500 records.\n",
    "#     trainDF, valDF = model_selection.train_test_split(df_train, test_size=.2)\n",
    "#     small_num=trainDF.groupby('broad_cat').count().sort_values('EIN').iloc[0]['EIN']\n",
    "\n",
    "trainDF, testDF = model_selection.train_test_split(df_train, test_size=.2)\n",
    "# See the composition by broad category.\n",
    "print(trainDF.groupby('broad_cat').count()['EIN'], '\\n'*2, testDF.groupby('broad_cat').count()['EIN'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "### Prepare classifier function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redefine vectorizer.\n",
    "def text_vectorizer(tokenizer_type=None, vectorizer_type=None):\n",
    "    ########################################################\n",
    "    ######### Define and choose tokenizers #################\n",
    "    def porter_tokenizer(str_input): # '''Pay attention to the input: this is string input, not token!'''\n",
    "        tokens = word_tokenize(str_input)\n",
    "        return [PorterStemmer().stem(token) for token in tokens]\n",
    "    # Lemmatize using POS tags, assume improving accuracy.\n",
    "    # Ref: \n",
    "    #   - https://stackoverflow.com/questions/15586721/wordnet-lemmatization-and-pos-tagging-in-python\n",
    "    #   - https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\n",
    "    def get_wordnet_pos(treebank_tag):\n",
    "        if treebank_tag.startswith('J'):\n",
    "            return wordnet.ADJ\n",
    "        elif treebank_tag.startswith('V'):\n",
    "            return wordnet.VERB\n",
    "        elif treebank_tag.startswith('N'):\n",
    "            return wordnet.NOUN\n",
    "        elif treebank_tag.startswith('R'):\n",
    "            return wordnet.ADV\n",
    "        else:\n",
    "            return wordnet.NOUN\n",
    "    def lemma_tokenizer(str_input): # '''Pay attention to the input: this is string input, not token!'''\n",
    "        tokens=word_tokenize(str_input)\n",
    "        return [WordNetLemmatizer().lemmatize(word=word, pos=get_wordnet_pos(pos)) for word, pos in nltk.pos_tag(tokens)]\n",
    "    # Choose tokenizer using parameter passed.\n",
    "    if tokenizer_type=='lemma':\n",
    "        tokenizer=lemma_tokenizer\n",
    "    elif tokenizer_type=='porter':\n",
    "        tokenizer=porter_tokenizer\n",
    "    ########################################################\n",
    "    ######### Define and choose vectorizer #################\n",
    "    # 1. Use word level, character level does not make sense for current situation.\n",
    "    # 2. Use count (freq) and tf-idf vectorizer. see: \n",
    "    # Bengfort, B., Bilbro, R., & Ojeda, T. (2018). Applied Text Analysis with Python: Enabling Language-Aware Data Products with Machine Learning (1 edition). Beijing Boston Farnham Sebastopol Tokyo: OReilly Media.\n",
    "    # Page: 67.\n",
    "    if vectorizer_type=='count':\n",
    "        ##### Token counts #####\n",
    "        vectorizer = CountVectorizer(stop_words='english', \n",
    "                                     tokenizer=tokenizer, \n",
    "                                     analyzer='word'\n",
    "                                    )\n",
    "        return vectorizer\n",
    "    elif vectorizer_type=='tfidf':\n",
    "        ##### TF-IDF #####\n",
    "        vectorizer = TfidfVectorizer(stop_words='english', \n",
    "                                     tokenizer=tokenizer, \n",
    "                                     analyzer='word'\n",
    "                                    )\n",
    "        return vectorizer\n",
    "    ########################################################\n",
    "\n",
    "# Define resample strategy.\n",
    "def func_resample(method, sampling_strategy, x_train_vect, y_train, categorical_features):\n",
    "    if method=='ADASYN':\n",
    "        from imblearn.over_sampling import ADASYN\n",
    "        resample = ADASYN(sampling_strategy=sampling_strategy, random_state=42)\n",
    "    elif method=='RandomOverSampler':\n",
    "        from imblearn.over_sampling import RandomOverSampler\n",
    "        resample = RandomOverSampler(sampling_strategy=sampling_strategy, random_state=42)\n",
    "    elif method=='SMOTE':\n",
    "        from imblearn.over_sampling import SMOTE\n",
    "        resample = SMOTE(sampling_strategy=sampling_strategy, random_state=42)\n",
    "    # SMOTENC not used.\n",
    "    # --> 947         X_continuous = check_array(X_continuous, accept_sparse=['csr', 'csc'])\n",
    "    # ...\n",
    "    # ValueError: Found array with 0 feature(s) (shape=(123539, 0)) while a minimum of 1 is required.\n",
    "    # Must have continuous feature, but all features (columns) in x_train_vect are specified as categorical.\n",
    "#     elif method=='SMOTENC':\n",
    "#         from imblearn.over_sampling import SMOTENC\n",
    "#         resample = SMOTENC(sampling_strategy=sampling_strategy, random_state=42, categorical_features=categorical_features)\n",
    "    elif method=='SMOTEENN':\n",
    "        from imblearn.combine import SMOTEENN\n",
    "        resample = SMOTEENN(sampling_strategy=sampling_strategy, random_state=42)\n",
    "    elif method=='SMOTETomek':\n",
    "        from imblearn.combine import SMOTETomek\n",
    "        resample = SMOTETomek(sampling_strategy=sampling_strategy, random_state=42)\n",
    "    x_train_vect_res, y_train_res = resample.fit_resample(x_train_vect, y_train)\n",
    "    return [x_train_vect_res, y_train_res]\n",
    "\n",
    "# Compile the workflow as a function.\n",
    "def func_classifier(param_list):\n",
    "    global gmean, iba, lb\n",
    "    # Pass parameters.\n",
    "    x_train, y_train, x_test, y_test, resample_method, sampling_strategy, classifier, tokenizer, vect_type = param_list\n",
    "    # Encode text input.\n",
    "    vectorizer=text_vectorizer(tokenizer_type=tokenizer, vectorizer_type=vect_type)\n",
    "    vectorizer.fit(x_train.append(x_test)) # Fit on all texts.\n",
    "    x_train_vect=vectorizer.transform(x_train)\n",
    "    x_test_vect=vectorizer.transform(x_test)\n",
    "    # Encode class labels -- Not necessary for NB/RF algorithms: https://stats.stackexchange.com/questions/288095/what-algorithms-require-one-hot-encoding\n",
    "    # See test results below.\n",
    "#     y_train_vect=le.fit_transform(y_train)\n",
    "#     y_test_vect=le.fit_transform(y_test)\n",
    "    # Resample imbalanced dataset.\n",
    "    categorical_features=list(range(0, x_train_vect.shape[1])) # All indices are categorical.\n",
    "    resample_x_y=func_resample(method=resample_method, sampling_strategy=sampling_strategy,\n",
    "                               x_train_vect=x_train_vect, y_train=y_train, categorical_features=categorical_features)\n",
    "    classifier.fit(resample_x_y[0], resample_x_y[1])\n",
    "    predictions = classifier.predict(x_test_vect)\n",
    "    gmean = iba(alpha=0.1, squared=True)(gmean)\n",
    "\n",
    "    return {'resample_method':resample_method,\n",
    "            'sampling_strategy':sampling_strategy,\n",
    "            'classifier':str(classifier), \n",
    "            'tokenizer':tokenizer, \n",
    "            'vect_type':vect_type, \n",
    "            'weighted_acc': gmean(y_true=y_test, y_pred=predictions, average='weighted')\n",
    "           }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total parameter combinations: 240\n"
     ]
    }
   ],
   "source": [
    "param_llist=[]\n",
    "x_train, y_train, x_test, y_test = [trainDF['mission_prgrm_spellchk'], trainDF['broad_cat'],\n",
    "                                    testDF['mission_prgrm_spellchk'], testDF['broad_cat']]\n",
    "for resample_method in ['ADASYN', 'RandomOverSampler', 'SMOTE', 'SMOTEENN', 'SMOTETomek']:\n",
    "    for sampling_strategy in ['minority','not minority','not majority','all']:\n",
    "        for classifier in [naive_bayes.MultinomialNB(), naive_bayes.ComplementNB(), ensemble.RandomForestClassifier()]:\n",
    "            for tokenizer in ['lemma', 'porter']:\n",
    "                for vect_type in ['count', 'tfidf']:\n",
    "                    param_llist+=[[x_train, y_train, x_test, y_test, resample_method, sampling_strategy, classifier, tokenizer, vect_type]]\n",
    "print('total parameter combinations:', len(param_llist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_llist_1, param_llist_2, param_llist_3, param_llist_4, param_llist_5 = [param_llist[0:48], param_llist[48:48*2], param_llist[48*2:48*3], \n",
    "                                                                             param_llist[48*3:48*4], param_llist[48*4:48*5],\n",
    "                                                                            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkPoolWorker-21:\n",
      "Process ForkPoolWorker-29:\n",
      "Process ForkPoolWorker-30:\n",
      "Process ForkPoolWorker-41:\n",
      "Process ForkPoolWorker-1:\n",
      "Process ForkPoolWorker-33:\n",
      "Process ForkPoolWorker-14:\n",
      "Process ForkPoolWorker-40:\n",
      "Process ForkPoolWorker-26:\n",
      "Process ForkPoolWorker-46:\n",
      "Process ForkPoolWorker-45:\n",
      "Process ForkPoolWorker-19:\n",
      "Process ForkPoolWorker-37:\n",
      "Process ForkPoolWorker-43:\n",
      "Process ForkPoolWorker-18:\n",
      "Process ForkPoolWorker-38:\n",
      "Process ForkPoolWorker-6:\n",
      "Process ForkPoolWorker-25:\n",
      "Process ForkPoolWorker-35:\n",
      "Process ForkPoolWorker-24:\n",
      "Process ForkPoolWorker-42:\n",
      "Process ForkPoolWorker-34:\n",
      "Process ForkPoolWorker-9:\n",
      "Process ForkPoolWorker-20:\n",
      "Process ForkPoolWorker-5:\n",
      "Process ForkPoolWorker-22:\n",
      "Process ForkPoolWorker-17:\n",
      "Process ForkPoolWorker-2:\n",
      "Process ForkPoolWorker-39:\n",
      "Process ForkPoolWorker-47:\n",
      "Process ForkPoolWorker-36:\n",
      "Process ForkPoolWorker-31:\n",
      "Process ForkPoolWorker-48:\n",
      "Process ForkPoolWorker-16:\n",
      "Process ForkPoolWorker-3:\n",
      "Process ForkPoolWorker-28:\n",
      "Process ForkPoolWorker-44:\n",
      "Process ForkPoolWorker-7:\n",
      "Process ForkPoolWorker-15:\n",
      "Process ForkPoolWorker-23:\n",
      "Process ForkPoolWorker-27:\n",
      "Process ForkPoolWorker-8:\n",
      "Process ForkPoolWorker-32:\n",
      "Process ForkPoolWorker-4:\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/root/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/root/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/root/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 44, in mapstar\n",
      "    return list(map(*args))\n",
      "  File \"<ipython-input-5-aea87e0afd3d>\", line 88, in func_classifier\n",
      "    vectorizer.fit(x_train.append(x_test)) # Fit on all texts.\n",
      "Process ForkPoolWorker-12:\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\", line 998, in fit\n",
      "    self.fit_transform(raw_documents)\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\", line 1032, in fit_transform\n",
      "    self.fixed_vocabulary_)\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\", line 942, in _count_vocab\n",
      "    for feature in analyze(doc):\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\", line 328, in <lambda>\n",
      "    tokenize(preprocess(self.decode(doc))), stop_words)\n",
      "  File \"<ipython-input-5-aea87e0afd3d>\", line 7, in porter_tokenizer\n",
      "    return [PorterStemmer().stem(token) for token in tokens]\n",
      "  File \"<ipython-input-5-aea87e0afd3d>\", line 7, in <listcomp>\n",
      "    return [PorterStemmer().stem(token) for token in tokens]\n",
      "Process ForkPoolWorker-11:\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/root/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/root/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/root/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/root/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/root/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/root/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/root/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/root/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 44, in mapstar\n",
      "    return list(map(*args))\n",
      "  File \"/root/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/root/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/root/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/root/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/root/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/root/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/root/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/root/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/root/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/root/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/root/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/root/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/root/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/root/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/root/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/root/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"<ipython-input-5-aea87e0afd3d>\", line 88, in func_classifier\n",
      "    vectorizer.fit(x_train.append(x_test)) # Fit on all texts.\n",
      "  File \"/root/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/root/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/root/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/root/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/root/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/root/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/root/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/root/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/root/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/root/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/root/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/root/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/root/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/root/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/root/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 44, in mapstar\n",
      "    return list(map(*args))\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\", line 998, in fit\n",
      "    self.fit_transform(raw_documents)\n",
      "  File \"/root/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/root/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/root/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/root/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/root/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/root/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 44, in mapstar\n",
      "    return list(map(*args))\n",
      "  File \"/root/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/root/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/root/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/root/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 44, in mapstar\n",
      "    return list(map(*args))\n",
      "  File \"/root/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 44, in mapstar\n",
      "    return list(map(*args))\n",
      "  File \"<ipython-input-5-aea87e0afd3d>\", line 88, in func_classifier\n",
      "    vectorizer.fit(x_train.append(x_test)) # Fit on all texts.\n",
      "  File \"/root/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 44, in mapstar\n",
      "    return list(map(*args))\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\", line 1032, in fit_transform\n",
      "    self.fixed_vocabulary_)\n",
      "  File \"/root/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 44, in mapstar\n",
      "    return list(map(*args))\n",
      "  File \"/root/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 44, in mapstar\n",
      "    return list(map(*args))\n",
      "  File \"/root/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 44, in mapstar\n",
      "    return list(map(*args))\n",
      "  File \"/root/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 44, in mapstar\n",
      "    return list(map(*args))\n",
      "  File \"/root/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 44, in mapstar\n",
      "    return list(map(*args))\n",
      "  File \"/root/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 44, in mapstar\n",
      "    return list(map(*args))\n",
      "  File \"/root/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 44, in mapstar\n",
      "    return list(map(*args))\n",
      "  File \"<ipython-input-5-aea87e0afd3d>\", line 88, in func_classifier\n",
      "    vectorizer.fit(x_train.append(x_test)) # Fit on all texts.\n",
      "  File \"/root/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"<ipython-input-5-aea87e0afd3d>\", line 88, in func_classifier\n",
      "    vectorizer.fit(x_train.append(x_test)) # Fit on all texts.\n",
      "  File \"/root/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 44, in mapstar\n",
      "    return list(map(*args))\n",
      "  File \"<ipython-input-5-aea87e0afd3d>\", line 88, in func_classifier\n",
      "    vectorizer.fit(x_train.append(x_test)) # Fit on all texts.\n",
      "  File \"/root/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 44, in mapstar\n",
      "    return list(map(*args))\n",
      "  File \"<ipython-input-5-aea87e0afd3d>\", line 88, in func_classifier\n",
      "    vectorizer.fit(x_train.append(x_test)) # Fit on all texts.\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\", line 942, in _count_vocab\n",
      "    for feature in analyze(doc):\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\", line 998, in fit\n",
      "    self.fit_transform(raw_documents)\n",
      "  File \"<ipython-input-5-aea87e0afd3d>\", line 88, in func_classifier\n",
      "    vectorizer.fit(x_train.append(x_test)) # Fit on all texts.\n",
      "  File \"<ipython-input-5-aea87e0afd3d>\", line 88, in func_classifier\n",
      "    vectorizer.fit(x_train.append(x_test)) # Fit on all texts.\n",
      "  File \"<ipython-input-5-aea87e0afd3d>\", line 88, in func_classifier\n",
      "    vectorizer.fit(x_train.append(x_test)) # Fit on all texts.\n",
      "  File \"<ipython-input-5-aea87e0afd3d>\", line 88, in func_classifier\n",
      "    vectorizer.fit(x_train.append(x_test)) # Fit on all texts.\n",
      "  File \"<ipython-input-5-aea87e0afd3d>\", line 88, in func_classifier\n",
      "    vectorizer.fit(x_train.append(x_test)) # Fit on all texts.\n",
      "  File \"<ipython-input-5-aea87e0afd3d>\", line 88, in func_classifier\n",
      "    vectorizer.fit(x_train.append(x_test)) # Fit on all texts.\n",
      "  File \"<ipython-input-5-aea87e0afd3d>\", line 88, in func_classifier\n",
      "    vectorizer.fit(x_train.append(x_test)) # Fit on all texts.\n",
      "  File \"/root/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 44, in mapstar\n",
      "    return list(map(*args))\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\", line 998, in fit\n",
      "    self.fit_transform(raw_documents)\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\", line 1584, in fit\n",
      "    X = super(TfidfVectorizer, self).fit_transform(raw_documents)\n",
      "  File \"<ipython-input-5-aea87e0afd3d>\", line 88, in func_classifier\n",
      "    vectorizer.fit(x_train.append(x_test)) # Fit on all texts.\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\", line 998, in fit\n",
      "    self.fit_transform(raw_documents)\n",
      "  File \"<ipython-input-5-aea87e0afd3d>\", line 88, in func_classifier\n",
      "    vectorizer.fit(x_train.append(x_test)) # Fit on all texts.\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\", line 1032, in fit_transform\n",
      "    self.fixed_vocabulary_)\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\", line 328, in <lambda>\n",
      "    tokenize(preprocess(self.decode(doc))), stop_words)\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\", line 1584, in fit\n",
      "    X = super(TfidfVectorizer, self).fit_transform(raw_documents)\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\", line 998, in fit\n",
      "    self.fit_transform(raw_documents)\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\", line 1584, in fit\n",
      "    X = super(TfidfVectorizer, self).fit_transform(raw_documents)\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\", line 1584, in fit\n",
      "    X = super(TfidfVectorizer, self).fit_transform(raw_documents)\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\", line 1584, in fit\n",
      "    X = super(TfidfVectorizer, self).fit_transform(raw_documents)\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\", line 1032, in fit_transform\n",
      "    self.fixed_vocabulary_)\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\", line 998, in fit\n",
      "    self.fit_transform(raw_documents)\n",
      "  File \"<ipython-input-5-aea87e0afd3d>\", line 88, in func_classifier\n",
      "    vectorizer.fit(x_train.append(x_test)) # Fit on all texts.\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\", line 1032, in fit_transform\n",
      "    self.fixed_vocabulary_)\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\", line 998, in fit\n",
      "    self.fit_transform(raw_documents)\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\", line 1032, in fit_transform\n",
      "    self.fixed_vocabulary_)\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\", line 942, in _count_vocab\n",
      "    for feature in analyze(doc):\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\", line 1032, in fit_transform\n",
      "    self.fixed_vocabulary_)\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\", line 1032, in fit_transform\n",
      "    self.fixed_vocabulary_)\n",
      "  File \"<ipython-input-5-aea87e0afd3d>\", line 7, in porter_tokenizer\n",
      "    return [PorterStemmer().stem(token) for token in tokens]\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\", line 1032, in fit_transform\n",
      "    self.fixed_vocabulary_)\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\", line 1032, in fit_transform\n",
      "    self.fixed_vocabulary_)\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\", line 1032, in fit_transform\n",
      "    self.fixed_vocabulary_)\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\", line 998, in fit\n",
      "    self.fit_transform(raw_documents)\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\", line 998, in fit\n",
      "    self.fit_transform(raw_documents)\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\", line 1584, in fit\n",
      "    X = super(TfidfVectorizer, self).fit_transform(raw_documents)\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\", line 1032, in fit_transform\n",
      "    self.fixed_vocabulary_)\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\", line 998, in fit\n",
      "    self.fit_transform(raw_documents)\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\", line 942, in _count_vocab\n",
      "    for feature in analyze(doc):\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\", line 942, in _count_vocab\n",
      "    for feature in analyze(doc):\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\", line 1032, in fit_transform\n",
      "    self.fixed_vocabulary_)\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\", line 942, in _count_vocab\n",
      "    for feature in analyze(doc):\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\", line 328, in <lambda>\n",
      "    tokenize(preprocess(self.decode(doc))), stop_words)\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\", line 942, in _count_vocab\n",
      "    for feature in analyze(doc):\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\", line 942, in _count_vocab\n",
      "    for feature in analyze(doc):\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\", line 942, in _count_vocab\n",
      "    for feature in analyze(doc):\n",
      "  File \"<ipython-input-5-aea87e0afd3d>\", line 7, in <listcomp>\n",
      "    return [PorterStemmer().stem(token) for token in tokens]\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\", line 942, in _count_vocab\n",
      "    for feature in analyze(doc):\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\", line 942, in _count_vocab\n",
      "    for feature in analyze(doc):\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\", line 1032, in fit_transform\n",
      "    self.fixed_vocabulary_)\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\", line 1032, in fit_transform\n",
      "    self.fixed_vocabulary_)\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\", line 1032, in fit_transform\n",
      "    self.fixed_vocabulary_)\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\", line 942, in _count_vocab\n",
      "    for feature in analyze(doc):\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\", line 328, in <lambda>\n",
      "    tokenize(preprocess(self.decode(doc))), stop_words)\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\", line 1032, in fit_transform\n",
      "    self.fixed_vocabulary_)\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\", line 328, in <lambda>\n",
      "    tokenize(preprocess(self.decode(doc))), stop_words)\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\", line 328, in <lambda>\n",
      "    tokenize(preprocess(self.decode(doc))), stop_words)\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\", line 942, in _count_vocab\n",
      "    for feature in analyze(doc):\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\", line 328, in <lambda>\n",
      "    tokenize(preprocess(self.decode(doc))), stop_words)\n",
      "  File \"<ipython-input-5-aea87e0afd3d>\", line 7, in porter_tokenizer\n",
      "    return [PorterStemmer().stem(token) for token in tokens]\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\", line 328, in <lambda>\n",
      "    tokenize(preprocess(self.decode(doc))), stop_words)\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/nltk/stem/porter.py\", line 670, in stem\n",
      "    stem = self._step2(stem)\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\", line 942, in _count_vocab\n",
      "    for feature in analyze(doc):\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\", line 328, in <lambda>\n",
      "    tokenize(preprocess(self.decode(doc))), stop_words)\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\", line 328, in <lambda>\n",
      "    tokenize(preprocess(self.decode(doc))), stop_words)\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\", line 328, in <lambda>\n",
      "    tokenize(preprocess(self.decode(doc))), stop_words)\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\", line 942, in _count_vocab\n",
      "    for feature in analyze(doc):\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\", line 328, in <lambda>\n",
      "    tokenize(preprocess(self.decode(doc))), stop_words)\n",
      "  File \"<ipython-input-5-aea87e0afd3d>\", line 7, in porter_tokenizer\n",
      "    return [PorterStemmer().stem(token) for token in tokens]\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\", line 942, in _count_vocab\n",
      "    for feature in analyze(doc):\n",
      "  File \"<ipython-input-5-aea87e0afd3d>\", line 7, in porter_tokenizer\n",
      "    return [PorterStemmer().stem(token) for token in tokens]\n",
      "  File \"<ipython-input-5-aea87e0afd3d>\", line 7, in porter_tokenizer\n",
      "    return [PorterStemmer().stem(token) for token in tokens]\n",
      "  File \"<ipython-input-5-aea87e0afd3d>\", line 7, in porter_tokenizer\n",
      "    return [PorterStemmer().stem(token) for token in tokens]\n",
      "  File \"<ipython-input-5-aea87e0afd3d>\", line 7, in <listcomp>\n",
      "    return [PorterStemmer().stem(token) for token in tokens]\n",
      "  File \"<ipython-input-5-aea87e0afd3d>\", line 7, in porter_tokenizer\n",
      "    return [PorterStemmer().stem(token) for token in tokens]\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\", line 328, in <lambda>\n",
      "    tokenize(preprocess(self.decode(doc))), stop_words)\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/nltk/stem/porter.py\", line 514, in _step2\n",
      "    return self._apply_rule_list(word, rules)\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\", line 328, in <lambda>\n",
      "    tokenize(preprocess(self.decode(doc))), stop_words)\n",
      "  File \"<ipython-input-5-aea87e0afd3d>\", line 7, in porter_tokenizer\n",
      "    return [PorterStemmer().stem(token) for token in tokens]\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\", line 942, in _count_vocab\n",
      "    for feature in analyze(doc):\n",
      "Process ForkPoolWorker-13:\n",
      "  File \"<ipython-input-5-aea87e0afd3d>\", line 7, in porter_tokenizer\n",
      "    return [PorterStemmer().stem(token) for token in tokens]\n",
      "  File \"<ipython-input-5-aea87e0afd3d>\", line 7, in porter_tokenizer\n",
      "    return [PorterStemmer().stem(token) for token in tokens]\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\", line 328, in <lambda>\n",
      "    tokenize(preprocess(self.decode(doc))), stop_words)\n",
      "  File \"<ipython-input-5-aea87e0afd3d>\", line 7, in <listcomp>\n",
      "    return [PorterStemmer().stem(token) for token in tokens]\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\", line 328, in <lambda>\n",
      "    tokenize(preprocess(self.decode(doc))), stop_words)\n",
      "  File \"<ipython-input-5-aea87e0afd3d>\", line 7, in <listcomp>\n",
      "    return [PorterStemmer().stem(token) for token in tokens]\n",
      "  File \"<ipython-input-5-aea87e0afd3d>\", line 7, in <listcomp>\n",
      "    return [PorterStemmer().stem(token) for token in tokens]\n",
      "  File \"<ipython-input-5-aea87e0afd3d>\", line 7, in porter_tokenizer\n",
      "    return [PorterStemmer().stem(token) for token in tokens]\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/nltk/stem/porter.py\", line 671, in stem\n",
      "    stem = self._step3(stem)\n",
      "  File \"<ipython-input-5-aea87e0afd3d>\", line 7, in <listcomp>\n",
      "    return [PorterStemmer().stem(token) for token in tokens]\n",
      "  File \"<ipython-input-5-aea87e0afd3d>\", line 7, in <listcomp>\n",
      "    return [PorterStemmer().stem(token) for token in tokens]\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-5-aea87e0afd3d>\", line 7, in porter_tokenizer\n",
      "    return [PorterStemmer().stem(token) for token in tokens]\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-5-aea87e0afd3d>\", line 7, in porter_tokenizer\n",
      "    return [PorterStemmer().stem(token) for token in tokens]\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/nltk/stem/porter.py\", line 267, in _apply_rule_list\n",
      "    if word.endswith(suffix):\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\", line 328, in <lambda>\n",
      "    tokenize(preprocess(self.decode(doc))), stop_words)\n",
      "  File \"<ipython-input-5-aea87e0afd3d>\", line 7, in <listcomp>\n",
      "    return [PorterStemmer().stem(token) for token in tokens]\n",
      "  File \"<ipython-input-5-aea87e0afd3d>\", line 7, in <listcomp>\n",
      "    return [PorterStemmer().stem(token) for token in tokens]\n",
      "  File \"<ipython-input-5-aea87e0afd3d>\", line 7, in porter_tokenizer\n",
      "    return [PorterStemmer().stem(token) for token in tokens]\n",
      "  File \"<ipython-input-5-aea87e0afd3d>\", line 7, in porter_tokenizer\n",
      "    return [PorterStemmer().stem(token) for token in tokens]\n",
      "  File \"<ipython-input-5-aea87e0afd3d>\", line 7, in <listcomp>\n",
      "    return [PorterStemmer().stem(token) for token in tokens]\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/nltk/stem/porter.py\", line 122, in __init__\n",
      "    for val in irregular_forms[key]:\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/nltk/stem/porter.py\", line 673, in stem\n",
      "    stem = self._step5a(stem)\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/nltk/stem/porter.py\", line 540, in _step3\n",
      "    ('ness', '', self._has_positive_measure),\n",
      "  File \"<ipython-input-5-aea87e0afd3d>\", line 7, in <listcomp>\n",
      "    return [PorterStemmer().stem(token) for token in tokens]\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/nltk/stem/porter.py\", line 670, in stem\n",
      "    stem = self._step2(stem)\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/nltk/stem/porter.py\", line 672, in stem\n",
      "    stem = self._step4(stem)\n",
      "  File \"/root/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/nltk/stem/porter.py\", line 655, in stem\n",
      "    def stem(self, word):\n",
      "  File \"/root/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/root/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/root/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"<ipython-input-5-aea87e0afd3d>\", line 7, in <listcomp>\n",
      "    return [PorterStemmer().stem(token) for token in tokens]\n",
      "  File \"/root/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"<ipython-input-5-aea87e0afd3d>\", line 7, in <listcomp>\n",
      "    return [PorterStemmer().stem(token) for token in tokens]\n",
      "  File \"/root/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"<ipython-input-5-aea87e0afd3d>\", line 7, in porter_tokenizer\n",
      "    return [PorterStemmer().stem(token) for token in tokens]\n",
      "KeyboardInterrupt\n",
      "  File \"<ipython-input-5-aea87e0afd3d>\", line 7, in <listcomp>\n",
      "    return [PorterStemmer().stem(token) for token in tokens]\n",
      "KeyboardInterrupt\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/nltk/stem/porter.py\", line 634, in _step5a\n",
      "    if self._measure(stem) > 1:\n",
      "KeyboardInterrupt\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/nltk/stem/porter.py\", line 600, in _step4\n",
      "    ('ize', '', measure_gt_1),\n",
      "KeyboardInterrupt\n",
      "  File \"/root/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/root/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/nltk/stem/porter.py\", line 514, in _step2\n",
      "    return self._apply_rule_list(word, rules)\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/nltk/stem/porter.py\", line 672, in stem\n",
      "    stem = self._step4(stem)\n",
      "  File \"/root/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/nltk/stem/porter.py\", line 673, in stem\n",
      "    stem = self._step5a(stem)\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/nltk/stem/porter.py\", line 670, in stem\n",
      "    stem = self._step2(stem)\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/nltk/stem/porter.py\", line 90, in __init__\n",
      "    self.MARTIN_EXTENSIONS,\n",
      "  File \"/root/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/root/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/nltk/stem/porter.py\", line 670, in stem\n",
      "    stem = self._step2(stem)\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/nltk/stem/porter.py\", line 671, in stem\n",
      "    stem = self._step3(stem)\n",
      "  File \"<ipython-input-5-aea87e0afd3d>\", line 7, in <listcomp>\n",
      "    return [PorterStemmer().stem(token) for token in tokens]\n",
      "  File \"/root/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/nltk/stem/porter.py\", line 669, in stem\n",
      "    stem = self._step1c(stem)\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/nltk/stem/porter.py\", line 188, in _measure\n",
      "    for i in range(len(stem)):\n",
      "  File \"/root/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/root/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/nltk/stem/porter.py\", line 260, in _apply_rule_list\n",
      "    if suffix == '*d' and self._ends_double_consonant(word):\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/nltk/stem/porter.py\", line 267, in _apply_rule_list\n",
      "    if word.endswith(suffix):\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/nltk/stem/porter.py\", line 600, in _step4\n",
      "    ('ize', '', measure_gt_1),\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/nltk/stem/porter.py\", line 514, in _step2\n",
      "    return self._apply_rule_list(word, rules)\n",
      "  File \"<ipython-input-5-aea87e0afd3d>\", line 7, in <listcomp>\n",
      "    return [PorterStemmer().stem(token) for token in tokens]\n",
      "  File \"/root/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "KeyboardInterrupt\n",
      "  File \"/root/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/nltk/stem/porter.py\", line 498, in _step2\n",
      "    ('biliti', 'ble', self._has_positive_measure),\n",
      "  File \"/root/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/nltk/stem/porter.py\", line 99, in __init__\n",
      "    self.mode = mode\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/nltk/stem/porter.py\", line 540, in _step3\n",
      "    ('ness', '', self._has_positive_measure),\n",
      "  File \"/root/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/root/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 44, in mapstar\n",
      "    return list(map(*args))\n",
      "  File \"/root/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 44, in mapstar\n",
      "    return list(map(*args))\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/nltk/stem/porter.py\", line 267, in _apply_rule_list\n",
      "    if word.endswith(suffix):\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/nltk/stem/porter.py\", line 267, in _apply_rule_list\n",
      "    if word.endswith(suffix):\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/nltk/stem/porter.py\", line 123, in __init__\n",
      "    self.pool[val] = key\n",
      "  File \"/root/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 44, in mapstar\n",
      "    return list(map(*args))\n",
      "  File \"/root/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 44, in mapstar\n",
      "    return list(map(*args))\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/root/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 44, in mapstar\n",
      "    return list(map(*args))\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/nltk/stem/porter.py\", line 258, in _apply_rule_list\n",
      "    for rule in rules:\n",
      "  File \"/root/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 44, in mapstar\n",
      "    return list(map(*args))\n",
      "  File \"<ipython-input-5-aea87e0afd3d>\", line 88, in func_classifier\n",
      "    vectorizer.fit(x_train.append(x_test)) # Fit on all texts.\n",
      "KeyboardInterrupt\n",
      "  File \"<ipython-input-5-aea87e0afd3d>\", line 88, in func_classifier\n",
      "    vectorizer.fit(x_train.append(x_test)) # Fit on all texts.\n",
      "  File \"<ipython-input-5-aea87e0afd3d>\", line 88, in func_classifier\n",
      "    vectorizer.fit(x_train.append(x_test)) # Fit on all texts.\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"<ipython-input-5-aea87e0afd3d>\", line 88, in func_classifier\n",
      "    vectorizer.fit(x_train.append(x_test)) # Fit on all texts.\n",
      "  File \"<ipython-input-5-aea87e0afd3d>\", line 88, in func_classifier\n",
      "    vectorizer.fit(x_train.append(x_test)) # Fit on all texts.\n",
      "  File \"<ipython-input-5-aea87e0afd3d>\", line 88, in func_classifier\n",
      "    vectorizer.fit(x_train.append(x_test)) # Fit on all texts.\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\", line 1584, in fit\n",
      "    X = super(TfidfVectorizer, self).fit_transform(raw_documents)\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\", line 1584, in fit\n",
      "    X = super(TfidfVectorizer, self).fit_transform(raw_documents)\n",
      "KeyboardInterrupt\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\", line 1584, in fit\n",
      "    X = super(TfidfVectorizer, self).fit_transform(raw_documents)\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\", line 1584, in fit\n",
      "    X = super(TfidfVectorizer, self).fit_transform(raw_documents)\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\", line 998, in fit\n",
      "    self.fit_transform(raw_documents)\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\", line 1032, in fit_transform\n",
      "    self.fixed_vocabulary_)\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\", line 998, in fit\n",
      "    self.fit_transform(raw_documents)\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\", line 1032, in fit_transform\n",
      "    self.fixed_vocabulary_)\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\", line 942, in _count_vocab\n",
      "    for feature in analyze(doc):\n",
      "  File \"/root/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\", line 1032, in fit_transform\n",
      "    self.fixed_vocabulary_)\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\", line 942, in _count_vocab\n",
      "    for feature in analyze(doc):\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\", line 328, in <lambda>\n",
      "    tokenize(preprocess(self.decode(doc))), stop_words)\n",
      "  File \"/root/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\", line 1032, in fit_transform\n",
      "    self.fixed_vocabulary_)\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\", line 328, in <lambda>\n",
      "    tokenize(preprocess(self.decode(doc))), stop_words)\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-5-aea87e0afd3d>\", line 6, in porter_tokenizer\n",
      "    tokens = word_tokenize(str_input)\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\", line 942, in _count_vocab\n",
      "    for feature in analyze(doc):\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\", line 1032, in fit_transform\n",
      "    self.fixed_vocabulary_)\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\", line 1032, in fit_transform\n",
      "    self.fixed_vocabulary_)\n",
      "  File \"/root/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/nltk/tokenize/__init__.py\", line 145, in word_tokenize\n",
      "    token for sent in sentences for token in _treebank_word_tokenizer.tokenize(sent)\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\", line 942, in _count_vocab\n",
      "    for feature in analyze(doc):\n",
      "  File \"/root/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\", line 328, in <lambda>\n",
      "    tokenize(preprocess(self.decode(doc))), stop_words)\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\", line 942, in _count_vocab\n",
      "    for feature in analyze(doc):\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/nltk/tokenize/__init__.py\", line 145, in <listcomp>\n",
      "    token for sent in sentences for token in _treebank_word_tokenizer.tokenize(sent)\n",
      "  File \"/root/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 44, in mapstar\n",
      "    return list(map(*args))\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\", line 328, in <lambda>\n",
      "    tokenize(preprocess(self.decode(doc))), stop_words)\n",
      "  File \"<ipython-input-5-aea87e0afd3d>\", line 6, in porter_tokenizer\n",
      "    tokens = word_tokenize(str_input)\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/nltk/tokenize/treebank.py\", line 143, in tokenize\n",
      "    text = regexp.sub(r' \\1 \\2 ', text)\n",
      "  File \"<ipython-input-5-aea87e0afd3d>\", line 6, in porter_tokenizer\n",
      "    tokens = word_tokenize(str_input)\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/nltk/tokenize/__init__.py\", line 145, in word_tokenize\n",
      "    token for sent in sentences for token in _treebank_word_tokenizer.tokenize(sent)\n",
      "KeyboardInterrupt\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/nltk/tokenize/__init__.py\", line 145, in word_tokenize\n",
      "    token for sent in sentences for token in _treebank_word_tokenizer.tokenize(sent)\n",
      "  File \"<ipython-input-5-aea87e0afd3d>\", line 24, in lemma_tokenizer\n",
      "    tokens=word_tokenize(str_input)\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\", line 942, in _count_vocab\n",
      "    for feature in analyze(doc):\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/nltk/tokenize/__init__.py\", line 145, in <listcomp>\n",
      "    token for sent in sentences for token in _treebank_word_tokenizer.tokenize(sent)\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/nltk/tokenize/__init__.py\", line 145, in <listcomp>\n",
      "    token for sent in sentences for token in _treebank_word_tokenizer.tokenize(sent)\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\", line 328, in <lambda>\n",
      "    tokenize(preprocess(self.decode(doc))), stop_words)\n",
      "  File \"<ipython-input-5-aea87e0afd3d>\", line 88, in func_classifier\n",
      "    vectorizer.fit(x_train.append(x_test)) # Fit on all texts.\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/nltk/tokenize/__init__.py\", line 145, in word_tokenize\n",
      "    token for sent in sentences for token in _treebank_word_tokenizer.tokenize(sent)\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\", line 328, in <lambda>\n",
      "    tokenize(preprocess(self.decode(doc))), stop_words)\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/nltk/tokenize/treebank.py\", line 143, in tokenize\n",
      "    text = regexp.sub(r' \\1 \\2 ', text)\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/nltk/tokenize/treebank.py\", line 122, in tokenize\n",
      "    text = regexp.sub(substitution, text)\n",
      "  File \"<ipython-input-5-aea87e0afd3d>\", line 24, in lemma_tokenizer\n",
      "    tokens=word_tokenize(str_input)\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\", line 1584, in fit\n",
      "    X = super(TfidfVectorizer, self).fit_transform(raw_documents)\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/nltk/tokenize/__init__.py\", line 145, in <listcomp>\n",
      "    token for sent in sentences for token in _treebank_word_tokenizer.tokenize(sent)\n",
      "  File \"<ipython-input-5-aea87e0afd3d>\", line 24, in lemma_tokenizer\n",
      "    tokens=word_tokenize(str_input)\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\", line 1032, in fit_transform\n",
      "    self.fixed_vocabulary_)\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/nltk/tokenize/treebank.py\", line 119, in tokenize\n",
      "    text = regexp.sub(substitution, text)\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/nltk/tokenize/__init__.py\", line 145, in word_tokenize\n",
      "    token for sent in sentences for token in _treebank_word_tokenizer.tokenize(sent)\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/nltk/tokenize/__init__.py\", line 145, in word_tokenize\n",
      "    token for sent in sentences for token in _treebank_word_tokenizer.tokenize(sent)\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\", line 942, in _count_vocab\n",
      "    for feature in analyze(doc):\n",
      "KeyboardInterrupt\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/nltk/tokenize/__init__.py\", line 145, in <listcomp>\n",
      "    token for sent in sentences for token in _treebank_word_tokenizer.tokenize(sent)\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/nltk/tokenize/__init__.py\", line 145, in <listcomp>\n",
      "    token for sent in sentences for token in _treebank_word_tokenizer.tokenize(sent)\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\", line 328, in <lambda>\n",
      "    tokenize(preprocess(self.decode(doc))), stop_words)\n",
      "  File \"/root/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/nltk/tokenize/treebank.py\", line 152, in tokenize\n",
      "    return text if return_str else text.split()\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/nltk/tokenize/treebank.py\", line 143, in tokenize\n",
      "    text = regexp.sub(r' \\1 \\2 ', text)\n",
      "  File \"<ipython-input-5-aea87e0afd3d>\", line 6, in porter_tokenizer\n",
      "    tokens = word_tokenize(str_input)\n",
      "KeyboardInterrupt\n",
      "  File \"/root/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "KeyboardInterrupt\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/nltk/tokenize/__init__.py\", line 143, in word_tokenize\n",
      "    sentences = [text] if preserve_line else sent_tokenize(text, language)\n",
      "  File \"/root/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 44, in mapstar\n",
      "    return list(map(*args))\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/nltk/tokenize/__init__.py\", line 105, in sent_tokenize\n",
      "    return tokenizer.tokenize(text)\n",
      "  File \"<ipython-input-5-aea87e0afd3d>\", line 88, in func_classifier\n",
      "    vectorizer.fit(x_train.append(x_test)) # Fit on all texts.\n",
      "  File \"/root/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/root/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/root/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/root/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/root/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/root/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/root/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/root/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/root/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/root/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/root/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/root/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\", line 1584, in fit\n",
      "    X = super(TfidfVectorizer, self).fit_transform(raw_documents)\n",
      "  File \"/root/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/root/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/root/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/nltk/tokenize/punkt.py\", line 1269, in tokenize\n",
      "    return list(self.sentences_from_text(text, realign_boundaries))\n",
      "  File \"/root/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/root/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/root/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/root/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/root/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/root/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/root/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/root/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/root/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/nltk/tokenize/punkt.py\", line 1323, in sentences_from_text\n",
      "    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]\n",
      "  File \"/root/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/root/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/root/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/root/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/root/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/root/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/nltk/tokenize/punkt.py\", line 1323, in <listcomp>\n",
      "    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]\n",
      "  File \"/root/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/root/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 44, in mapstar\n",
      "    return list(map(*args))\n",
      "  File \"/root/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/root/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 44, in mapstar\n",
      "    return list(map(*args))\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/nltk/tokenize/punkt.py\", line 1313, in span_tokenize\n",
      "    for sl in slices:\n",
      "  File \"/root/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 44, in mapstar\n",
      "    return list(map(*args))\n",
      "  File \"<ipython-input-5-aea87e0afd3d>\", line 88, in func_classifier\n",
      "    vectorizer.fit(x_train.append(x_test)) # Fit on all texts.\n",
      "  File \"/root/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 44, in mapstar\n",
      "    return list(map(*args))\n",
      "  File \"/root/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/root/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/root/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/root/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/root/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/root/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 44, in mapstar\n",
      "    return list(map(*args))\n",
      "  File \"/root/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/root/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/root/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 44, in mapstar\n",
      "    return list(map(*args))\n",
      "  File \"/root/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/root/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/root/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 44, in mapstar\n",
      "    return list(map(*args))\n",
      "  File \"/root/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/root/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"<ipython-input-5-aea87e0afd3d>\", line 88, in func_classifier\n",
      "    vectorizer.fit(x_train.append(x_test)) # Fit on all texts.\n",
      "  File \"/root/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 44, in mapstar\n",
      "    return list(map(*args))\n",
      "  File \"<ipython-input-5-aea87e0afd3d>\", line 88, in func_classifier\n",
      "    vectorizer.fit(x_train.append(x_test)) # Fit on all texts.\n",
      "  File \"/root/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 44, in mapstar\n",
      "    return list(map(*args))\n",
      "  File \"/root/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 44, in mapstar\n",
      "    return list(map(*args))\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\", line 1584, in fit\n",
      "    X = super(TfidfVectorizer, self).fit_transform(raw_documents)\n",
      "  File \"<ipython-input-5-aea87e0afd3d>\", line 88, in func_classifier\n",
      "    vectorizer.fit(x_train.append(x_test)) # Fit on all texts.\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\", line 998, in fit\n",
      "    self.fit_transform(raw_documents)\n",
      "  File \"<ipython-input-5-aea87e0afd3d>\", line 88, in func_classifier\n",
      "    vectorizer.fit(x_train.append(x_test)) # Fit on all texts.\n",
      "  File \"<ipython-input-5-aea87e0afd3d>\", line 88, in func_classifier\n",
      "    vectorizer.fit(x_train.append(x_test)) # Fit on all texts.\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\", line 998, in fit\n",
      "    self.fit_transform(raw_documents)\n",
      "  File \"<ipython-input-5-aea87e0afd3d>\", line 88, in func_classifier\n",
      "    vectorizer.fit(x_train.append(x_test)) # Fit on all texts.\n",
      "  File \"<ipython-input-5-aea87e0afd3d>\", line 88, in func_classifier\n",
      "    vectorizer.fit(x_train.append(x_test)) # Fit on all texts.\n",
      "  File \"/root/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/root/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-5-aea87e0afd3d>\", line 88, in func_classifier\n",
      "    vectorizer.fit(x_train.append(x_test)) # Fit on all texts.\n",
      "  File \"<ipython-input-5-aea87e0afd3d>\", line 88, in func_classifier\n",
      "    vectorizer.fit(x_train.append(x_test)) # Fit on all texts.\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\", line 1032, in fit_transform\n",
      "    self.fixed_vocabulary_)\n",
      "  File \"/root/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/root/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\", line 998, in fit\n",
      "    self.fit_transform(raw_documents)\n",
      "  File \"/root/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/nltk/tokenize/punkt.py\", line 1354, in _realign_boundaries\n",
      "    for sl1, sl2 in _pair_iter(slices):\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\", line 1584, in fit\n",
      "    X = super(TfidfVectorizer, self).fit_transform(raw_documents)\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\", line 998, in fit\n",
      "    self.fit_transform(raw_documents)\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\", line 1032, in fit_transform\n",
      "    self.fixed_vocabulary_)\n",
      "  File \"/root/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 44, in mapstar\n",
      "    return list(map(*args))\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\", line 1032, in fit_transform\n",
      "    self.fixed_vocabulary_)\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\", line 998, in fit\n",
      "    self.fit_transform(raw_documents)\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\", line 998, in fit\n",
      "    self.fit_transform(raw_documents)\n",
      "  File \"/root/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/root/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\", line 942, in _count_vocab\n",
      "    for feature in analyze(doc):\n",
      "  File \"/root/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 44, in mapstar\n",
      "    return list(map(*args))\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\", line 998, in fit\n",
      "    self.fit_transform(raw_documents)\n",
      "  File \"/root/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 44, in mapstar\n",
      "    return list(map(*args))\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\", line 1032, in fit_transform\n",
      "    self.fixed_vocabulary_)\n",
      "  File \"<ipython-input-5-aea87e0afd3d>\", line 88, in func_classifier\n",
      "    vectorizer.fit(x_train.append(x_test)) # Fit on all texts.\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\", line 942, in _count_vocab\n",
      "    for feature in analyze(doc):\n",
      "  File \"/root/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\", line 942, in _count_vocab\n",
      "    for feature in analyze(doc):\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\", line 1032, in fit_transform\n",
      "    self.fixed_vocabulary_)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-99b2652c65e6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mPool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m48\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mresult_dicts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc_classifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_llist_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[1;32m    264\u001b[0m         \u001b[0;32min\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mthat\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mreturned\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m         '''\n\u001b[0;32m--> 266\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapstar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstarmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 638\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    639\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 635\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 551\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    552\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"/root/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\", line 1032, in fit_transform\n",
      "    self.fixed_vocabulary_)\n",
      "  File \"/root/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/root/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 44, in mapstar\n",
      "    return list(map(*args))\n",
      "  File \"/root/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 44, in mapstar\n",
      "    return list(map(*args))\n",
      "  File \"<ipython-input-5-aea87e0afd3d>\", line 88, in func_classifier\n",
      "    vectorizer.fit(x_train.append(x_test)) # Fit on all texts.\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\", line 942, in _count_vocab\n",
      "    for feature in analyze(doc):\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\", line 328, in <lambda>\n",
      "    tokenize(preprocess(self.decode(doc))), stop_words)\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\", line 328, in <lambda>\n",
      "    tokenize(preprocess(self.decode(doc))), stop_words)\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\", line 998, in fit\n",
      "    self.fit_transform(raw_documents)\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\", line 1032, in fit_transform\n",
      "    self.fixed_vocabulary_)\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\", line 942, in _count_vocab\n",
      "    for feature in analyze(doc):\n",
      "  File \"/root/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/root/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\", line 328, in <lambda>\n",
      "    tokenize(preprocess(self.decode(doc))), stop_words)\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\", line 328, in <lambda>\n",
      "    tokenize(preprocess(self.decode(doc))), stop_words)\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\", line 328, in <lambda>\n",
      "    tokenize(preprocess(self.decode(doc))), stop_words)\n",
      "  File \"<ipython-input-5-aea87e0afd3d>\", line 25, in lemma_tokenizer\n",
      "    return [WordNetLemmatizer().lemmatize(word=word, pos=get_wordnet_pos(pos)) for word, pos in nltk.pos_tag(tokens)]\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\", line 1584, in fit\n",
      "    X = super(TfidfVectorizer, self).fit_transform(raw_documents)\n",
      "  File \"/root/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"<ipython-input-5-aea87e0afd3d>\", line 25, in lemma_tokenizer\n",
      "    return [WordNetLemmatizer().lemmatize(word=word, pos=get_wordnet_pos(pos)) for word, pos in nltk.pos_tag(tokens)]\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\", line 942, in _count_vocab\n",
      "    for feature in analyze(doc):\n",
      "  File \"<ipython-input-5-aea87e0afd3d>\", line 25, in lemma_tokenizer\n",
      "    return [WordNetLemmatizer().lemmatize(word=word, pos=get_wordnet_pos(pos)) for word, pos in nltk.pos_tag(tokens)]\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/nltk/tag/__init__.py\", line 162, in pos_tag\n",
      "    return _pos_tag(tokens, tagset, tagger, lang)\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\", line 1032, in fit_transform\n",
      "    self.fixed_vocabulary_)\n",
      "  File \"/root/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 44, in mapstar\n",
      "    return list(map(*args))\n",
      "  File \"<ipython-input-5-aea87e0afd3d>\", line 6, in porter_tokenizer\n",
      "    tokens = word_tokenize(str_input)\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\", line 328, in <lambda>\n",
      "    tokenize(preprocess(self.decode(doc))), stop_words)\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/nltk/tag/__init__.py\", line 162, in pos_tag\n",
      "    return _pos_tag(tokens, tagset, tagger, lang)\n",
      "  File \"<ipython-input-5-aea87e0afd3d>\", line 88, in func_classifier\n",
      "    vectorizer.fit(x_train.append(x_test)) # Fit on all texts.\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\", line 942, in _count_vocab\n",
      "    for feature in analyze(doc):\n",
      "  File \"/root/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 44, in mapstar\n",
      "    return list(map(*args))\n",
      "  File \"/root/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 44, in mapstar\n",
      "    return list(map(*args))\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/nltk/tag/__init__.py\", line 119, in _pos_tag\n",
      "    tagged_tokens = tagger.tag(tokens)\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\", line 1584, in fit\n",
      "    X = super(TfidfVectorizer, self).fit_transform(raw_documents)\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\", line 942, in _count_vocab\n",
      "    for feature in analyze(doc):\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/nltk/tokenize/punkt.py\", line 318, in _pair_iter\n",
      "    for el in it:\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\", line 1032, in fit_transform\n",
      "    self.fixed_vocabulary_)\n",
      "  File \"<ipython-input-5-aea87e0afd3d>\", line 88, in func_classifier\n",
      "    vectorizer.fit(x_train.append(x_test)) # Fit on all texts.\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/nltk/tokenize/__init__.py\", line 143, in word_tokenize\n",
      "    sentences = [text] if preserve_line else sent_tokenize(text, language)\n",
      "  File \"/root/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"<ipython-input-5-aea87e0afd3d>\", line 88, in func_classifier\n",
      "    vectorizer.fit(x_train.append(x_test)) # Fit on all texts.\n",
      "  File \"/root/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/nltk/tag/__init__.py\", line 119, in _pos_tag\n",
      "    tagged_tokens = tagger.tag(tokens)\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\", line 1032, in fit_transform\n",
      "    self.fixed_vocabulary_)\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\", line 328, in <lambda>\n",
      "    tokenize(preprocess(self.decode(doc))), stop_words)\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\", line 1584, in fit\n",
      "    X = super(TfidfVectorizer, self).fit_transform(raw_documents)\n",
      "  File \"<ipython-input-5-aea87e0afd3d>\", line 88, in func_classifier\n",
      "    vectorizer.fit(x_train.append(x_test)) # Fit on all texts.\n",
      "  File \"<ipython-input-5-aea87e0afd3d>\", line 25, in lemma_tokenizer\n",
      "    return [WordNetLemmatizer().lemmatize(word=word, pos=get_wordnet_pos(pos)) for word, pos in nltk.pos_tag(tokens)]\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/nltk/tag/perceptron.py\", line 162, in tag\n",
      "    tag = self.model.predict(features)\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\", line 328, in <lambda>\n",
      "    tokenize(preprocess(self.decode(doc))), stop_words)\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\", line 942, in _count_vocab\n",
      "    for feature in analyze(doc):\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\", line 1584, in fit\n",
      "    X = super(TfidfVectorizer, self).fit_transform(raw_documents)\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/nltk/tokenize/__init__.py\", line 105, in sent_tokenize\n",
      "    return tokenizer.tokenize(text)\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/nltk/tokenize/punkt.py\", line 1328, in _slices_from_text\n",
      "    context = match.group() + match.group('after_tok')\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/nltk/tag/perceptron.py\", line 162, in tag\n",
      "    tag = self.model.predict(features)\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\", line 942, in _count_vocab\n",
      "    for feature in analyze(doc):\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\", line 998, in fit\n",
      "    self.fit_transform(raw_documents)\n",
      "  File \"/root/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 44, in mapstar\n",
      "    return list(map(*args))\n",
      "  File \"<ipython-input-5-aea87e0afd3d>\", line 25, in lemma_tokenizer\n",
      "    return [WordNetLemmatizer().lemmatize(word=word, pos=get_wordnet_pos(pos)) for word, pos in nltk.pos_tag(tokens)]\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\", line 1032, in fit_transform\n",
      "    self.fixed_vocabulary_)\n",
      "  File \"<ipython-input-5-aea87e0afd3d>\", line 88, in func_classifier\n",
      "    vectorizer.fit(x_train.append(x_test)) # Fit on all texts.\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\", line 1584, in fit\n",
      "    X = super(TfidfVectorizer, self).fit_transform(raw_documents)\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/nltk/tag/perceptron.py\", line 59, in predict\n",
      "    return max(self.classes, key=lambda label: (scores[label], label))\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/nltk/tag/__init__.py\", line 162, in pos_tag\n",
      "    return _pos_tag(tokens, tagset, tagger, lang)\n",
      "  File \"<ipython-input-5-aea87e0afd3d>\", line 25, in lemma_tokenizer\n",
      "    return [WordNetLemmatizer().lemmatize(word=word, pos=get_wordnet_pos(pos)) for word, pos in nltk.pos_tag(tokens)]\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\", line 328, in <lambda>\n",
      "    tokenize(preprocess(self.decode(doc))), stop_words)\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/nltk/tokenize/punkt.py\", line 1269, in tokenize\n",
      "    return list(self.sentences_from_text(text, realign_boundaries))\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\", line 328, in <lambda>\n",
      "    tokenize(preprocess(self.decode(doc))), stop_words)\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/nltk/tag/perceptron.py\", line 57, in predict\n",
      "    scores[label] += value * weight\n",
      "  File \"<ipython-input-5-aea87e0afd3d>\", line 88, in func_classifier\n",
      "    vectorizer.fit(x_train.append(x_test)) # Fit on all texts.\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\", line 942, in _count_vocab\n",
      "    for feature in analyze(doc):\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/nltk/tag/__init__.py\", line 162, in pos_tag\n",
      "    return _pos_tag(tokens, tagset, tagger, lang)\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\", line 1584, in fit\n",
      "    X = super(TfidfVectorizer, self).fit_transform(raw_documents)\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\", line 1032, in fit_transform\n",
      "    self.fixed_vocabulary_)\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\", line 1032, in fit_transform\n",
      "    self.fixed_vocabulary_)\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/nltk/tag/perceptron.py\", line 59, in <lambda>\n",
      "    return max(self.classes, key=lambda label: (scores[label], label))\n",
      "  File \"/root/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/nltk/tokenize/punkt.py\", line 1323, in sentences_from_text\n",
      "    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/nltk/tag/__init__.py\", line 119, in _pos_tag\n",
      "    tagged_tokens = tagger.tag(tokens)\n",
      "  File \"<ipython-input-5-aea87e0afd3d>\", line 25, in lemma_tokenizer\n",
      "    return [WordNetLemmatizer().lemmatize(word=word, pos=get_wordnet_pos(pos)) for word, pos in nltk.pos_tag(tokens)]\n",
      "KeyboardInterrupt\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\", line 328, in <lambda>\n",
      "    tokenize(preprocess(self.decode(doc))), stop_words)\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/nltk/tag/__init__.py\", line 119, in _pos_tag\n",
      "    tagged_tokens = tagger.tag(tokens)\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\", line 1032, in fit_transform\n",
      "    self.fixed_vocabulary_)\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\", line 942, in _count_vocab\n",
      "    for feature in analyze(doc):\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\", line 942, in _count_vocab\n",
      "    for feature in analyze(doc):\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\", line 1584, in fit\n",
      "    X = super(TfidfVectorizer, self).fit_transform(raw_documents)\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/nltk/tokenize/punkt.py\", line 1323, in <listcomp>\n",
      "    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/nltk/tag/__init__.py\", line 162, in pos_tag\n",
      "    return _pos_tag(tokens, tagset, tagger, lang)\n",
      "  File \"<ipython-input-5-aea87e0afd3d>\", line 6, in porter_tokenizer\n",
      "    tokens = word_tokenize(str_input)\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/nltk/tag/perceptron.py\", line 162, in tag\n",
      "    tag = self.model.predict(features)\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\", line 942, in _count_vocab\n",
      "    for feature in analyze(doc):\n",
      "Process ForkPoolWorker-10:\n",
      "  File \"/root/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\", line 1032, in fit_transform\n",
      "    self.fixed_vocabulary_)\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/nltk/tag/__init__.py\", line 162, in pos_tag\n",
      "    return _pos_tag(tokens, tagset, tagger, lang)\n",
      "  File \"/root/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\", line 1032, in fit_transform\n",
      "    self.fixed_vocabulary_)\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\", line 328, in <lambda>\n",
      "    tokenize(preprocess(self.decode(doc))), stop_words)\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/nltk/tokenize/__init__.py\", line 145, in word_tokenize\n",
      "    token for sent in sentences for token in _treebank_word_tokenizer.tokenize(sent)\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\", line 1032, in fit_transform\n",
      "    self.fixed_vocabulary_)\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/nltk/tag/__init__.py\", line 119, in _pos_tag\n",
      "    tagged_tokens = tagger.tag(tokens)\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/nltk/tag/__init__.py\", line 162, in pos_tag\n",
      "    return _pos_tag(tokens, tagset, tagger, lang)\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\", line 328, in <lambda>\n",
      "    tokenize(preprocess(self.decode(doc))), stop_words)\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/nltk/tag/perceptron.py\", line 53, in predict\n",
      "    if feat not in self.weights or value == 0:\n",
      "  File \"/root/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\", line 942, in _count_vocab\n",
      "    for feature in analyze(doc):\n",
      "  File \"/root/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\", line 942, in _count_vocab\n",
      "    for feature in analyze(doc):\n",
      "  File \"/root/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/nltk/tokenize/__init__.py\", line 145, in <listcomp>\n",
      "    token for sent in sentences for token in _treebank_word_tokenizer.tokenize(sent)\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/nltk/tag/perceptron.py\", line 162, in tag\n",
      "    tag = self.model.predict(features)\n",
      "  File \"<ipython-input-5-aea87e0afd3d>\", line 25, in lemma_tokenizer\n",
      "    return [WordNetLemmatizer().lemmatize(word=word, pos=get_wordnet_pos(pos)) for word, pos in nltk.pos_tag(tokens)]\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\", line 942, in _count_vocab\n",
      "    for feature in analyze(doc):\n",
      "  File \"/root/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 44, in mapstar\n",
      "    return list(map(*args))\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\", line 328, in <lambda>\n",
      "    tokenize(preprocess(self.decode(doc))), stop_words)\n",
      "  File \"/root/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 44, in mapstar\n",
      "    return list(map(*args))\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\", line 328, in <lambda>\n",
      "    tokenize(preprocess(self.decode(doc))), stop_words)\n",
      "  File \"/root/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/nltk/tag/perceptron.py\", line 57, in predict\n",
      "    scores[label] += value * weight\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\", line 328, in <lambda>\n",
      "    tokenize(preprocess(self.decode(doc))), stop_words)\n",
      "  File \"<ipython-input-5-aea87e0afd3d>\", line 25, in lemma_tokenizer\n",
      "    return [WordNetLemmatizer().lemmatize(word=word, pos=get_wordnet_pos(pos)) for word, pos in nltk.pos_tag(tokens)]\n",
      "  File \"<ipython-input-5-aea87e0afd3d>\", line 25, in lemma_tokenizer\n",
      "    return [WordNetLemmatizer().lemmatize(word=word, pos=get_wordnet_pos(pos)) for word, pos in nltk.pos_tag(tokens)]\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/nltk/tag/__init__.py\", line 162, in pos_tag\n",
      "    return _pos_tag(tokens, tagset, tagger, lang)\n",
      "  File \"<ipython-input-5-aea87e0afd3d>\", line 88, in func_classifier\n",
      "    vectorizer.fit(x_train.append(x_test)) # Fit on all texts.\n",
      "  File \"<ipython-input-5-aea87e0afd3d>\", line 25, in lemma_tokenizer\n",
      "    return [WordNetLemmatizer().lemmatize(word=word, pos=get_wordnet_pos(pos)) for word, pos in nltk.pos_tag(tokens)]\n",
      "  File \"<ipython-input-5-aea87e0afd3d>\", line 25, in lemma_tokenizer\n",
      "    return [WordNetLemmatizer().lemmatize(word=word, pos=get_wordnet_pos(pos)) for word, pos in nltk.pos_tag(tokens)]\n",
      "  File \"/root/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/nltk/tag/__init__.py\", line 119, in _pos_tag\n",
      "    tagged_tokens = tagger.tag(tokens)\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/nltk/tag/__init__.py\", line 162, in pos_tag\n",
      "    return _pos_tag(tokens, tagset, tagger, lang)\n",
      "KeyboardInterrupt\n",
      "  File \"/root/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 44, in mapstar\n",
      "    return list(map(*args))\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/nltk/tag/perceptron.py\", line 162, in tag\n",
      "    tag = self.model.predict(features)\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/nltk/tag/__init__.py\", line 119, in _pos_tag\n",
      "    tagged_tokens = tagger.tag(tokens)\n",
      "  File \"/root/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/nltk/tag/__init__.py\", line 162, in pos_tag\n",
      "    return _pos_tag(tokens, tagset, tagger, lang)\n",
      "  File \"<ipython-input-5-aea87e0afd3d>\", line 88, in func_classifier\n",
      "    vectorizer.fit(x_train.append(x_test)) # Fit on all texts.\n",
      "  File \"/root/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\", line 998, in fit\n",
      "    self.fit_transform(raw_documents)\n",
      "  File \"/root/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/nltk/tag/perceptron.py\", line 162, in tag\n",
      "    tag = self.model.predict(features)\n",
      "  File \"<ipython-input-5-aea87e0afd3d>\", line 88, in func_classifier\n",
      "    vectorizer.fit(x_train.append(x_test)) # Fit on all texts.\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/nltk/tag/__init__.py\", line 119, in _pos_tag\n",
      "    tagged_tokens = tagger.tag(tokens)\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"<ipython-input-5-aea87e0afd3d>\", line 25, in lemma_tokenizer\n",
      "    return [WordNetLemmatizer().lemmatize(word=word, pos=get_wordnet_pos(pos)) for word, pos in nltk.pos_tag(tokens)]\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\", line 328, in <lambda>\n",
      "    tokenize(preprocess(self.decode(doc))), stop_words)\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\", line 1032, in fit_transform\n",
      "    self.fixed_vocabulary_)\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/nltk/tag/perceptron.py\", line 57, in predict\n",
      "    scores[label] += value * weight\n",
      "  File \"/root/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 44, in mapstar\n",
      "    return list(map(*args))\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/nltk/tag/__init__.py\", line 119, in _pos_tag\n",
      "    tagged_tokens = tagger.tag(tokens)\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\", line 1032, in fit_transform\n",
      "    self.fixed_vocabulary_)\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/nltk/tag/perceptron.py\", line 59, in predict\n",
      "    return max(self.classes, key=lambda label: (scores[label], label))\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\", line 1584, in fit\n",
      "    X = super(TfidfVectorizer, self).fit_transform(raw_documents)\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/nltk/tag/perceptron.py\", line 162, in tag\n",
      "    tag = self.model.predict(features)\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/nltk/tag/__init__.py\", line 162, in pos_tag\n",
      "    return _pos_tag(tokens, tagset, tagger, lang)\n",
      "  File \"<ipython-input-5-aea87e0afd3d>\", line 25, in lemma_tokenizer\n",
      "    return [WordNetLemmatizer().lemmatize(word=word, pos=get_wordnet_pos(pos)) for word, pos in nltk.pos_tag(tokens)]\n",
      "  File \"<ipython-input-5-aea87e0afd3d>\", line 88, in func_classifier\n",
      "    vectorizer.fit(x_train.append(x_test)) # Fit on all texts.\n",
      "KeyboardInterrupt\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\", line 942, in _count_vocab\n",
      "    for feature in analyze(doc):\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\", line 1032, in fit_transform\n",
      "    self.fixed_vocabulary_)\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/nltk/tag/perceptron.py\", line 57, in predict\n",
      "    scores[label] += value * weight\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/nltk/tag/__init__.py\", line 119, in _pos_tag\n",
      "    tagged_tokens = tagger.tag(tokens)\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/nltk/tag/perceptron.py\", line 59, in <lambda>\n",
      "    return max(self.classes, key=lambda label: (scores[label], label))\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\", line 328, in <lambda>\n",
      "    tokenize(preprocess(self.decode(doc))), stop_words)\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\", line 942, in _count_vocab\n",
      "    for feature in analyze(doc):\n",
      "KeyboardInterrupt\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/nltk/tag/perceptron.py\", line 162, in tag\n",
      "    tag = self.model.predict(features)\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/nltk/tag/perceptron.py\", line 161, in tag\n",
      "    features = self._get_features(i, word, context, prev, prev2)\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/nltk/tokenize/punkt.py\", line 1313, in span_tokenize\n",
      "    for sl in slices:\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\", line 1032, in fit_transform\n",
      "    self.fixed_vocabulary_)\n",
      "  File \"<ipython-input-5-aea87e0afd3d>\", line 25, in lemma_tokenizer\n",
      "    return [WordNetLemmatizer().lemmatize(word=word, pos=get_wordnet_pos(pos)) for word, pos in nltk.pos_tag(tokens)]\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\", line 998, in fit\n",
      "    self.fit_transform(raw_documents)\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/nltk/tokenize/treebank.py\", line 122, in tokenize\n",
      "    text = regexp.sub(substitution, text)\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\", line 1584, in fit\n",
      "    X = super(TfidfVectorizer, self).fit_transform(raw_documents)\n",
      "KeyboardInterrupt\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\", line 328, in <lambda>\n",
      "    tokenize(preprocess(self.decode(doc))), stop_words)\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/nltk/tag/perceptron.py\", line 57, in predict\n",
      "    scores[label] += value * weight\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/nltk/tag/perceptron.py\", line 270, in _get_features\n",
      "    add('i+1 word', context[i + 1])\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\", line 1032, in fit_transform\n",
      "    self.fixed_vocabulary_)\n",
      "  File \"/root/anaconda3/lib/python3.6/re.py\", line 330, in filter\n",
      "    def filter(match, template=template):\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\", line 1032, in fit_transform\n",
      "    self.fixed_vocabulary_)\n",
      "  File \"<ipython-input-5-aea87e0afd3d>\", line 25, in lemma_tokenizer\n",
      "    return [WordNetLemmatizer().lemmatize(word=word, pos=get_wordnet_pos(pos)) for word, pos in nltk.pos_tag(tokens)]\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/nltk/tokenize/punkt.py\", line 1354, in _realign_boundaries\n",
      "    for sl1, sl2 in _pair_iter(slices):\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\", line 942, in _count_vocab\n",
      "    for feature in analyze(doc):\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\", line 328, in <lambda>\n",
      "    tokenize(preprocess(self.decode(doc))), stop_words)\n",
      "  File \"<ipython-input-5-aea87e0afd3d>\", line 25, in <listcomp>\n",
      "    return [WordNetLemmatizer().lemmatize(word=word, pos=get_wordnet_pos(pos)) for word, pos in nltk.pos_tag(tokens)]\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\", line 942, in _count_vocab\n",
      "    for feature in analyze(doc):\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\", line 942, in _count_vocab\n",
      "    for feature in analyze(doc):\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"<ipython-input-5-aea87e0afd3d>\", line 25, in lemma_tokenizer\n",
      "    return [WordNetLemmatizer().lemmatize(word=word, pos=get_wordnet_pos(pos)) for word, pos in nltk.pos_tag(tokens)]\n",
      "  File \"<ipython-input-5-aea87e0afd3d>\", line 88, in func_classifier\n",
      "    vectorizer.fit(x_train.append(x_test)) # Fit on all texts.\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/nltk/tokenize/punkt.py\", line 318, in _pair_iter\n",
      "    for el in it:\n",
      "KeyboardInterrupt\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/nltk/stem/wordnet.py\", line 41, in lemmatize\n",
      "    lemmas = wordnet._morphy(word, pos)\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\", line 328, in <lambda>\n",
      "    tokenize(preprocess(self.decode(doc))), stop_words)\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/nltk/tag/__init__.py\", line 162, in pos_tag\n",
      "    return _pos_tag(tokens, tagset, tagger, lang)\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/nltk/tokenize/punkt.py\", line 1327, in _slices_from_text\n",
      "    for match in self._lang_vars.period_context_re().finditer(text):\n",
      "  File \"<ipython-input-5-aea87e0afd3d>\", line 25, in lemma_tokenizer\n",
      "    return [WordNetLemmatizer().lemmatize(word=word, pos=get_wordnet_pos(pos)) for word, pos in nltk.pos_tag(tokens)]\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/nltk/tag/__init__.py\", line 119, in _pos_tag\n",
      "    tagged_tokens = tagger.tag(tokens)\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/nltk/tag/perceptron.py\", line 162, in tag\n",
      "    tag = self.model.predict(features)\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/nltk/tag/perceptron.py\", line 162, in tag\n",
      "    tag = self.model.predict(features)\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\", line 328, in <lambda>\n",
      "    tokenize(preprocess(self.decode(doc))), stop_words)\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/nltk/tag/__init__.py\", line 162, in pos_tag\n",
      "    return _pos_tag(tokens, tagset, tagger, lang)\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/nltk/tag/__init__.py\", line 162, in pos_tag\n",
      "    return _pos_tag(tokens, tagset, tagger, lang)\n",
      "  File \"<ipython-input-5-aea87e0afd3d>\", line 25, in <listcomp>\n",
      "    return [WordNetLemmatizer().lemmatize(word=word, pos=get_wordnet_pos(pos)) for word, pos in nltk.pos_tag(tokens)]\n",
      "KeyboardInterrupt\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/nltk/corpus/reader/wordnet.py\", line 1876, in _morphy\n",
      "    def _morphy(self, form, pos, check_exceptions=True):\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/nltk/tag/perceptron.py\", line 59, in predict\n",
      "    return max(self.classes, key=lambda label: (scores[label], label))\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/nltk/tag/perceptron.py\", line 254, in add\n",
      "    features[' '.join((name,) + tuple(args))] += 1\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/nltk/tag/__init__.py\", line 119, in _pos_tag\n",
      "    tagged_tokens = tagger.tag(tokens)\n",
      "  File \"<ipython-input-5-aea87e0afd3d>\", line 25, in lemma_tokenizer\n",
      "    return [WordNetLemmatizer().lemmatize(word=word, pos=get_wordnet_pos(pos)) for word, pos in nltk.pos_tag(tokens)]\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/nltk/tag/__init__.py\", line 162, in pos_tag\n",
      "    return _pos_tag(tokens, tagset, tagger, lang)\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/nltk/tag/perceptron.py\", line 59, in <lambda>\n",
      "    return max(self.classes, key=lambda label: (scores[label], label))\n",
      "KeyboardInterrupt\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/nltk/tag/perceptron.py\", line 57, in predict\n",
      "    scores[label] += value * weight\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/nltk/tag/__init__.py\", line 119, in _pos_tag\n",
      "    tagged_tokens = tagger.tag(tokens)\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/nltk/tag/__init__.py\", line 162, in pos_tag\n",
      "    return _pos_tag(tokens, tagset, tagger, lang)\n",
      "  File \"<ipython-input-5-aea87e0afd3d>\", line 6, in porter_tokenizer\n",
      "    tokens = word_tokenize(str_input)\n",
      "KeyboardInterrupt\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/nltk/tag/perceptron.py\", line 162, in tag\n",
      "    tag = self.model.predict(features)\n",
      "KeyboardInterrupt\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/nltk/tag/__init__.py\", line 162, in pos_tag\n",
      "    return _pos_tag(tokens, tagset, tagger, lang)\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/nltk/tag/__init__.py\", line 119, in _pos_tag\n",
      "    tagged_tokens = tagger.tag(tokens)\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/nltk/tag/perceptron.py\", line 57, in predict\n",
      "    scores[label] += value * weight\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/nltk/tag/__init__.py\", line 119, in _pos_tag\n",
      "    tagged_tokens = tagger.tag(tokens)\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/nltk/tag/perceptron.py\", line 162, in tag\n",
      "    tag = self.model.predict(features)\n",
      "KeyboardInterrupt\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/nltk/tokenize/__init__.py\", line 145, in word_tokenize\n",
      "    token for sent in sentences for token in _treebank_word_tokenizer.tokenize(sent)\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\", line 1584, in fit\n",
      "    X = super(TfidfVectorizer, self).fit_transform(raw_documents)\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/nltk/tag/__init__.py\", line 119, in _pos_tag\n",
      "    tagged_tokens = tagger.tag(tokens)\n",
      "KeyboardInterrupt\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/nltk/tag/perceptron.py\", line 162, in tag\n",
      "    tag = self.model.predict(features)\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/nltk/stem/wordnet.py\", line 41, in lemmatize\n",
      "    lemmas = wordnet._morphy(word, pos)\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/nltk/tokenize/__init__.py\", line 145, in <listcomp>\n",
      "    token for sent in sentences for token in _treebank_word_tokenizer.tokenize(sent)\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\", line 1032, in fit_transform\n",
      "    self.fixed_vocabulary_)\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/nltk/tag/perceptron.py\", line 57, in predict\n",
      "    scores[label] += value * weight\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/nltk/tokenize/treebank.py\", line 143, in tokenize\n",
      "    text = regexp.sub(r' \\1 \\2 ', text)\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/nltk/corpus/reader/wordnet.py\", line 1915, in _morphy\n",
      "    results = filter_forms([form] + forms)\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/nltk/tag/__init__.py\", line 119, in _pos_tag\n",
      "    tagged_tokens = tagger.tag(tokens)\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\", line 942, in _count_vocab\n",
      "    for feature in analyze(doc):\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/nltk/tag/perceptron.py\", line 162, in tag\n",
      "    tag = self.model.predict(features)\n",
      "  File \"/root/anaconda3/lib/python3.6/re.py\", line 324, in _subx\n",
      "    def _subx(pattern, template):\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\", line 942, in _count_vocab\n",
      "    for feature in analyze(doc):\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/nltk/corpus/reader/wordnet.py\", line 1901, in filter_forms\n",
      "    if form not in seen:\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\", line 328, in <lambda>\n",
      "    tokenize(preprocess(self.decode(doc))), stop_words)\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\", line 328, in <lambda>\n",
      "    tokenize(preprocess(self.decode(doc))), stop_words)\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/nltk/tag/perceptron.py\", line 162, in tag\n",
      "    tag = self.model.predict(features)\n",
      "  File \"<ipython-input-5-aea87e0afd3d>\", line 24, in lemma_tokenizer\n",
      "    tokens=word_tokenize(str_input)\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/nltk/tag/perceptron.py\", line 59, in predict\n",
      "    return max(self.classes, key=lambda label: (scores[label], label))\n",
      "KeyboardInterrupt\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/nltk/tag/perceptron.py\", line 56, in predict\n",
      "    for label, weight in weights.items():\n",
      "KeyboardInterrupt\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/nltk/tag/perceptron.py\", line 59, in <lambda>\n",
      "    return max(self.classes, key=lambda label: (scores[label], label))\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/nltk/tag/perceptron.py\", line 162, in tag\n",
      "    tag = self.model.predict(features)\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/nltk/tag/perceptron.py\", line 57, in predict\n",
      "    scores[label] += value * weight\n",
      "KeyboardInterrupt\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/nltk/tokenize/__init__.py\", line 145, in word_tokenize\n",
      "    token for sent in sentences for token in _treebank_word_tokenizer.tokenize(sent)\n",
      "  File \"<ipython-input-5-aea87e0afd3d>\", line 25, in lemma_tokenizer\n",
      "    return [WordNetLemmatizer().lemmatize(word=word, pos=get_wordnet_pos(pos)) for word, pos in nltk.pos_tag(tokens)]\n",
      "KeyboardInterrupt\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/nltk/tag/perceptron.py\", line 59, in predict\n",
      "    return max(self.classes, key=lambda label: (scores[label], label))\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/nltk/tokenize/__init__.py\", line 145, in <listcomp>\n",
      "    token for sent in sentences for token in _treebank_word_tokenizer.tokenize(sent)\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/nltk/tokenize/treebank.py\", line 122, in tokenize\n",
      "    text = regexp.sub(substitution, text)\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/nltk/tag/perceptron.py\", line 59, in <lambda>\n",
      "    return max(self.classes, key=lambda label: (scores[label], label))\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/nltk/tag/__init__.py\", line 162, in pos_tag\n",
      "    return _pos_tag(tokens, tagset, tagger, lang)\n",
      "  File \"/root/anaconda3/lib/python3.6/re.py\", line 330, in filter\n",
      "    def filter(match, template=template):\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/nltk/tag/__init__.py\", line 119, in _pos_tag\n",
      "    tagged_tokens = tagger.tag(tokens)\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/nltk/tag/perceptron.py\", line 162, in tag\n",
      "    tag = self.model.predict(features)\n",
      "  File \"/root/anaconda3/lib/python3.6/site-packages/nltk/tag/perceptron.py\", line 57, in predict\n",
      "    scores[label] += value * weight\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "p=Pool(48)\n",
    "result_dicts=p.map(func_classifier, param_llist_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(result_dicts).to_pickle('../../output/df_result_dicts_broad_cat_1.pkl')\n",
    "pd.DataFrame(result_dicts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Python\n",
    "## Try encoding/not encoding label, almost identical.\n",
    "# Label not encoded experiment 1.\n",
    "In : func_classifier(param_list=param_llist[3])\n",
    "Out: {'resample_method': 'ADASYN',\n",
    "      'sampling_strategy': 'minority',\n",
    "      'classifier': 'MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)',\n",
    "      'tokenizer': 'porter',\n",
    "      'vect_type': 'tfidf',\n",
    "      'weighted_acc': 0.49227231236613467}\n",
    "    \n",
    "# Label not encoded experiment 2.\n",
    "In : func_classifier(param_list=param_llist[3])\n",
    "Out: {'resample_method': 'ADASYN',\n",
    "      'sampling_strategy': 'minority',\n",
    "      'classifier': 'MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)',\n",
    "      'tokenizer': 'porter',\n",
    "      'vect_type': 'tfidf',\n",
    "      'weighted_acc': 0.5016141725625118}\n",
    "\n",
    "# Label encoded experiment 1.\n",
    "In : func_classifier(param_list=param_llist[3])\n",
    "Out: {'resample_method': 'ADASYN',\n",
    "      'sampling_strategy': 'minority',\n",
    "      'classifier': 'MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)',\n",
    "      'tokenizer': 'porter',\n",
    "      'vect_type': 'tfidf',\n",
    "      'weighted_acc': 0.5070028997209108}\n",
    "    \n",
    "# Label encoded experiment 2.\n",
    "In : func_classifier(param_list=param_llist[3])\n",
    "Out: {'resample_method': 'ADASYN',\n",
    "      'sampling_strategy': 'minority',\n",
    "      'classifier': 'MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)',\n",
    "      'tokenizer': 'porter',\n",
    "      'vect_type': 'tfidf',\n",
    "      'weighted_acc': 0.5015052917827343}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_classifier(param_list):\n",
    "    global trainDF, valDF\n",
    "    input_text, classifier, tokenizer, vect_type, average_mtd = param_list\n",
    "    \n",
    "    ##########################################################\n",
    "    ################ Prepare dataframe for ML ################\n",
    "    #### Sample ####\n",
    "    # Build training and testing data frame.\n",
    "    x_train=trainDF[input_text]\n",
    "    y_train=trainDF['broad_cat']\n",
    "    x_valid=valDF[input_text]\n",
    "    y_valid=valDF['broad_cat']\n",
    "    ################ Prepare dataframe for ML ################\n",
    "    ##########################################################\n",
    "\n",
    "    ##########################################################\n",
    "    ################ Define tokenizer ################\n",
    "\n",
    "    def porter_tokenizer(str_input): # '''Pay attention to the input: this is string input, not token!'''\n",
    "        tokens = word_tokenize(str_input)\n",
    "        return [PorterStemmer().stem(token) for token in tokens]\n",
    "    \n",
    "    # Lemmatize using POS tags, assume to improve accuracy.\n",
    "    # Ref: \n",
    "    #   - https://stackoverflow.com/questions/15586721/wordnet-lemmatization-and-pos-tagging-in-python\n",
    "    #   - https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\n",
    "    def get_wordnet_pos(treebank_tag):\n",
    "        if treebank_tag.startswith('J'):\n",
    "            return wordnet.ADJ\n",
    "        elif treebank_tag.startswith('V'):\n",
    "            return wordnet.VERB\n",
    "        elif treebank_tag.startswith('N'):\n",
    "            return wordnet.NOUN\n",
    "        elif treebank_tag.startswith('R'):\n",
    "            return wordnet.ADV\n",
    "        else:\n",
    "            return wordnet.NOUN\n",
    "\n",
    "    def lemma_tokenizer(str_input): # '''Pay attention to the input: this is string input, not token!'''\n",
    "        tokens=word_tokenize(str_input)\n",
    "        return [WordNetLemmatizer().lemmatize(word=word, pos=get_wordnet_pos(pos)) for word, pos in nltk.pos_tag(tokens)]\n",
    "            \n",
    "    if tokenizer=='lemma':\n",
    "        tokenizer=lemma_tokenizer\n",
    "    elif tokenizer=='porter':\n",
    "        tokenizer=porter_tokenizer\n",
    "    ################ Define tokenizer ################\n",
    "    ##########################################################\n",
    "    \n",
    "    ##########################################################\n",
    "    ######### Text Vectorization and Transformation ##########\n",
    "    # 1. Use Porter Stemmer.\n",
    "    # 2. Use word level, character level does not make sense for current situation.\n",
    "    # 3. Use count (freq) and tf-idf vectorizer. see: \n",
    "    # Bengfort, B., Bilbro, R., & Ojeda, T. (2018). Applied Text Analysis with Python: Enabling Language-Aware Data Products with Machine Learning (1 edition). Beijing Boston Farnham Sebastopol Tokyo: OReilly Media.\n",
    "    # Page: 67.\n",
    "    \n",
    "    if vect_type=='count':\n",
    "        ##### Token counts #####\n",
    "        # create the transform\n",
    "        vectorizer = CountVectorizer(stop_words='english', \n",
    "                                     tokenizer=tokenizer, \n",
    "                                     analyzer='word'\n",
    "                                    )\n",
    "        # tokenize and build vocab.\n",
    "        vectorizer.fit(x_train)\n",
    "        # Encode document: transform the training and validation data using count vectorizer object\n",
    "        x_train_vect =  vectorizer.transform(x_train)\n",
    "        x_valid_vect =  vectorizer.transform(x_valid)\n",
    "    elif vect_type=='tfidf':\n",
    "        ##### TF-IDF #####\n",
    "        # create the transform\n",
    "        vectorizer = TfidfVectorizer(stop_words='english', \n",
    "                                     tokenizer=tokenizer, \n",
    "                                     analyzer='word'\n",
    "                                    )\n",
    "        # tokenize and build vocab\n",
    "        vectorizer.fit(x_train)\n",
    "        # Encode document: transform the training and validation data using count vectorizer object\n",
    "        x_train_vect =  vectorizer.transform(x_train)\n",
    "        x_valid_vect =  vectorizer.transform(x_valid)\n",
    "\n",
    "    ######### Text Vectorization and Transformation ##########\n",
    "    ##########################################################\n",
    "    \n",
    "    classifier.fit(x_train_vect, y_train)\n",
    "    predictions = classifier.predict(x_valid_vect)\n",
    "    return {'input_text':input_text,\n",
    "            'classifier':str(classifier), \n",
    "            'tokenizer':tokenizer.__name__, \n",
    "            'vect_type':vect_type, \n",
    "            'average_mtd':average_mtd,\n",
    "            'accuracy':metrics.accuracy_score(predictions, y_valid), \n",
    "            'precision':metrics.precision_score(y_pred=predictions, y_true=y_valid, average=average_mtd),\n",
    "            'recall':metrics.recall_score(y_pred=predictions, y_true=y_valid, average=average_mtd),\n",
    "            'f1':metrics.f1_score(y_pred=predictions, y_true=y_valid, average=average_mtd)\n",
    "           }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a list of parameters.\n",
    "param_llist=[]\n",
    "for input_text in ['mission', 'prgrm_dsc', 'mission_prgrm', 'mission_spellchk', 'prgrm_dsc_spellchk', 'mission_prgrm_spellchk']:\n",
    "    for classifier in [naive_bayes.MultinomialNB(), naive_bayes.ComplementNB()]:\n",
    "        for tokenizer in ['lemma', 'porter']:\n",
    "            for vect_type in ['count', 'tfidf']:\n",
    "                for average_mtd in ['macro', 'weighted']:\n",
    "                    param_llist+=[[input_text, classifier, tokenizer, vect_type, average_mtd]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "p=Pool(24)\n",
    "df_performance_nb=pd.DataFrame(p.map(func_classifier, param_llist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>average_mtd</th>\n",
       "      <th>classifier</th>\n",
       "      <th>f1</th>\n",
       "      <th>input_text</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>tokenizer</th>\n",
       "      <th>vect_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>0.778145</td>\n",
       "      <td>weighted</td>\n",
       "      <td>ComplementNB(alpha=1.0, class_prior=None, fit_...</td>\n",
       "      <td>0.773984</td>\n",
       "      <td>mission_prgrm_spellchk</td>\n",
       "      <td>0.774852</td>\n",
       "      <td>0.778145</td>\n",
       "      <td>lemma_tokenizer</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>0.778145</td>\n",
       "      <td>macro</td>\n",
       "      <td>ComplementNB(alpha=1.0, class_prior=None, fit_...</td>\n",
       "      <td>0.699526</td>\n",
       "      <td>mission_prgrm_spellchk</td>\n",
       "      <td>0.742740</td>\n",
       "      <td>0.685534</td>\n",
       "      <td>lemma_tokenizer</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.777497</td>\n",
       "      <td>macro</td>\n",
       "      <td>ComplementNB(alpha=1.0, class_prior=None, fit_...</td>\n",
       "      <td>0.695233</td>\n",
       "      <td>mission_prgrm</td>\n",
       "      <td>0.750631</td>\n",
       "      <td>0.676116</td>\n",
       "      <td>lemma_tokenizer</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.777497</td>\n",
       "      <td>weighted</td>\n",
       "      <td>ComplementNB(alpha=1.0, class_prior=None, fit_...</td>\n",
       "      <td>0.772665</td>\n",
       "      <td>mission_prgrm</td>\n",
       "      <td>0.775101</td>\n",
       "      <td>0.777497</td>\n",
       "      <td>lemma_tokenizer</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0.775522</td>\n",
       "      <td>weighted</td>\n",
       "      <td>ComplementNB(alpha=1.0, class_prior=None, fit_...</td>\n",
       "      <td>0.771589</td>\n",
       "      <td>mission_prgrm_spellchk</td>\n",
       "      <td>0.772726</td>\n",
       "      <td>0.775522</td>\n",
       "      <td>porter_tokenizer</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    accuracy average_mtd                                         classifier  \\\n",
       "91  0.778145    weighted  ComplementNB(alpha=1.0, class_prior=None, fit_...   \n",
       "90  0.778145       macro  ComplementNB(alpha=1.0, class_prior=None, fit_...   \n",
       "42  0.777497       macro  ComplementNB(alpha=1.0, class_prior=None, fit_...   \n",
       "43  0.777497    weighted  ComplementNB(alpha=1.0, class_prior=None, fit_...   \n",
       "95  0.775522    weighted  ComplementNB(alpha=1.0, class_prior=None, fit_...   \n",
       "\n",
       "          f1              input_text  precision    recall         tokenizer  \\\n",
       "91  0.773984  mission_prgrm_spellchk   0.774852  0.778145   lemma_tokenizer   \n",
       "90  0.699526  mission_prgrm_spellchk   0.742740  0.685534   lemma_tokenizer   \n",
       "42  0.695233           mission_prgrm   0.750631  0.676116   lemma_tokenizer   \n",
       "43  0.772665           mission_prgrm   0.775101  0.777497   lemma_tokenizer   \n",
       "95  0.771589  mission_prgrm_spellchk   0.772726  0.775522  porter_tokenizer   \n",
       "\n",
       "   vect_type  \n",
       "91     tfidf  \n",
       "90     tfidf  \n",
       "42     tfidf  \n",
       "43     tfidf  \n",
       "95     tfidf  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_performance_nb.sort_values('accuracy', ascending=False)[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Python\n",
    "# Manually check if acc is correct.\n",
    "In :func_naive_bayes(2)\n",
    "    df_performance['accuracy']\n",
    "Out:0    0.340417\n",
    "    Name: accuracy, dtype: float64\n",
    "In :t=pd.DataFrame([classifier.predict(x_valid_vect), y_valid]).T.rename(columns={0:'a', 1:'b'})\n",
    "    len(t[t.a==t.b])/len(t)\n",
    "Out:0.34041666666666665\n",
    "''' Looks correct, scale the computing '''\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a list of parameters.\n",
    "param_llist=[]\n",
    "for input_text in ['mission', 'prgrm_dsc', 'mission_prgrm', 'mission_spellchk', 'prgrm_dsc_spellchk', 'mission_prgrm_spellchk']:\n",
    "    for classifier in [ensemble.RandomForestClassifier()]:\n",
    "        for tokenizer in ['lemma', 'porter']:\n",
    "            for vect_type in ['count', 'tfidf']:\n",
    "                for average_mtd in ['macro', 'weighted']:\n",
    "                    param_llist+=[[input_text, classifier, tokenizer, vect_type, average_mtd]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "p=Pool(24)\n",
    "df_performance_rf=pd.DataFrame(p.map(func_classifier, param_llist))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "### Select model, test on Universal Testing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>average_mtd</th>\n",
       "      <th>classifier</th>\n",
       "      <th>f1</th>\n",
       "      <th>input_text</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>tokenizer</th>\n",
       "      <th>vect_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>0.778145</td>\n",
       "      <td>weighted</td>\n",
       "      <td>ComplementNB(alpha=1.0, class_prior=None, fit_...</td>\n",
       "      <td>0.773984</td>\n",
       "      <td>mission_prgrm_spellchk</td>\n",
       "      <td>0.774852</td>\n",
       "      <td>0.778145</td>\n",
       "      <td>lemma_tokenizer</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>0.778145</td>\n",
       "      <td>macro</td>\n",
       "      <td>ComplementNB(alpha=1.0, class_prior=None, fit_...</td>\n",
       "      <td>0.699526</td>\n",
       "      <td>mission_prgrm_spellchk</td>\n",
       "      <td>0.742740</td>\n",
       "      <td>0.685534</td>\n",
       "      <td>lemma_tokenizer</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>0.777497</td>\n",
       "      <td>weighted</td>\n",
       "      <td>ComplementNB(alpha=1.0, class_prior=None, fit_...</td>\n",
       "      <td>0.772665</td>\n",
       "      <td>mission_prgrm</td>\n",
       "      <td>0.775101</td>\n",
       "      <td>0.777497</td>\n",
       "      <td>lemma_tokenizer</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>0.777497</td>\n",
       "      <td>macro</td>\n",
       "      <td>ComplementNB(alpha=1.0, class_prior=None, fit_...</td>\n",
       "      <td>0.695233</td>\n",
       "      <td>mission_prgrm</td>\n",
       "      <td>0.750631</td>\n",
       "      <td>0.676116</td>\n",
       "      <td>lemma_tokenizer</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>0.775522</td>\n",
       "      <td>weighted</td>\n",
       "      <td>ComplementNB(alpha=1.0, class_prior=None, fit_...</td>\n",
       "      <td>0.771589</td>\n",
       "      <td>mission_prgrm_spellchk</td>\n",
       "      <td>0.772726</td>\n",
       "      <td>0.775522</td>\n",
       "      <td>porter_tokenizer</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>0.775522</td>\n",
       "      <td>macro</td>\n",
       "      <td>ComplementNB(alpha=1.0, class_prior=None, fit_...</td>\n",
       "      <td>0.699870</td>\n",
       "      <td>mission_prgrm_spellchk</td>\n",
       "      <td>0.744213</td>\n",
       "      <td>0.684626</td>\n",
       "      <td>porter_tokenizer</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0.773709</td>\n",
       "      <td>weighted</td>\n",
       "      <td>ComplementNB(alpha=1.0, class_prior=None, fit_...</td>\n",
       "      <td>0.769014</td>\n",
       "      <td>mission_prgrm</td>\n",
       "      <td>0.771355</td>\n",
       "      <td>0.773709</td>\n",
       "      <td>porter_tokenizer</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>0.773709</td>\n",
       "      <td>macro</td>\n",
       "      <td>ComplementNB(alpha=1.0, class_prior=None, fit_...</td>\n",
       "      <td>0.692758</td>\n",
       "      <td>mission_prgrm</td>\n",
       "      <td>0.746558</td>\n",
       "      <td>0.673120</td>\n",
       "      <td>porter_tokenizer</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>0.768626</td>\n",
       "      <td>weighted</td>\n",
       "      <td>ComplementNB(alpha=1.0, class_prior=None, fit_...</td>\n",
       "      <td>0.764820</td>\n",
       "      <td>mission_prgrm</td>\n",
       "      <td>0.766744</td>\n",
       "      <td>0.768626</td>\n",
       "      <td>lemma_tokenizer</td>\n",
       "      <td>count</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>0.768626</td>\n",
       "      <td>macro</td>\n",
       "      <td>ComplementNB(alpha=1.0, class_prior=None, fit_...</td>\n",
       "      <td>0.691887</td>\n",
       "      <td>mission_prgrm</td>\n",
       "      <td>0.729649</td>\n",
       "      <td>0.682032</td>\n",
       "      <td>lemma_tokenizer</td>\n",
       "      <td>count</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     accuracy average_mtd                                         classifier  \\\n",
       "139  0.778145    weighted  ComplementNB(alpha=1.0, class_prior=None, fit_...   \n",
       "138  0.778145       macro  ComplementNB(alpha=1.0, class_prior=None, fit_...   \n",
       "91   0.777497    weighted  ComplementNB(alpha=1.0, class_prior=None, fit_...   \n",
       "90   0.777497       macro  ComplementNB(alpha=1.0, class_prior=None, fit_...   \n",
       "143  0.775522    weighted  ComplementNB(alpha=1.0, class_prior=None, fit_...   \n",
       "142  0.775522       macro  ComplementNB(alpha=1.0, class_prior=None, fit_...   \n",
       "95   0.773709    weighted  ComplementNB(alpha=1.0, class_prior=None, fit_...   \n",
       "94   0.773709       macro  ComplementNB(alpha=1.0, class_prior=None, fit_...   \n",
       "89   0.768626    weighted  ComplementNB(alpha=1.0, class_prior=None, fit_...   \n",
       "88   0.768626       macro  ComplementNB(alpha=1.0, class_prior=None, fit_...   \n",
       "\n",
       "           f1              input_text  precision    recall         tokenizer  \\\n",
       "139  0.773984  mission_prgrm_spellchk   0.774852  0.778145   lemma_tokenizer   \n",
       "138  0.699526  mission_prgrm_spellchk   0.742740  0.685534   lemma_tokenizer   \n",
       "91   0.772665           mission_prgrm   0.775101  0.777497   lemma_tokenizer   \n",
       "90   0.695233           mission_prgrm   0.750631  0.676116   lemma_tokenizer   \n",
       "143  0.771589  mission_prgrm_spellchk   0.772726  0.775522  porter_tokenizer   \n",
       "142  0.699870  mission_prgrm_spellchk   0.744213  0.684626  porter_tokenizer   \n",
       "95   0.769014           mission_prgrm   0.771355  0.773709  porter_tokenizer   \n",
       "94   0.692758           mission_prgrm   0.746558  0.673120  porter_tokenizer   \n",
       "89   0.764820           mission_prgrm   0.766744  0.768626   lemma_tokenizer   \n",
       "88   0.691887           mission_prgrm   0.729649  0.682032   lemma_tokenizer   \n",
       "\n",
       "    vect_type  \n",
       "139     tfidf  \n",
       "138     tfidf  \n",
       "91      tfidf  \n",
       "90      tfidf  \n",
       "143     tfidf  \n",
       "142     tfidf  \n",
       "95      tfidf  \n",
       "94      tfidf  \n",
       "89      count  \n",
       "88      count  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_performance=pd.concat([df_performance_rf, df_performance_nb], ignore_index=True).sort_values(['accuracy', 'f1'], ascending=False)\n",
    "df_performance[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define parameters and datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters.\n",
    "input_text='mission_prgrm_spellchk'\n",
    "classifier=naive_bayes.ComplementNB()\n",
    "tokenizer='lemma'\n",
    "vect_type='tfidf'\n",
    "average_mtd='macro'\n",
    "\n",
    "df_universal_test=pd.read_pickle('../../dataset/df_ntee_universal/test/df_ntee_universal_test.pkl.gz', compression='gzip')\n",
    "df_universal_test['mission_prgrm_spellchk']=df_universal_test['mission_spellchk']+' '+df_universal_test['prgrm_dsc_spellchk'] # Using spell-checked.\n",
    "df_universal_test['broad_cat']=df_universal_test['NTEE1'].apply(ntee2cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ComplementNB(alpha=1.0, class_prior=None, fit_prior=True, norm=False)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##########################################################\n",
    "################ Prepare dataframe for ML ################\n",
    "#### Sample ####\n",
    "# Build training and testing data frame.\n",
    "x_train=trainDF[input_text]\n",
    "y_train=trainDF['broad_cat']\n",
    "################ Prepare dataframe for ML ################\n",
    "##########################################################\n",
    "\n",
    "##########################################################\n",
    "################ Define tokenizer ################\n",
    "\n",
    "def porter_tokenizer(str_input): # '''Pay attention to the input: this is string input, not token!'''\n",
    "    tokens = word_tokenize(str_input)\n",
    "    return [PorterStemmer().stem(token) for token in tokens]\n",
    "\n",
    "# Lemmatize using POS tags, assume to improve accuracy.\n",
    "# Ref: \n",
    "#   - https://stackoverflow.com/questions/15586721/wordnet-lemmatization-and-pos-tagging-in-python\n",
    "#   - https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "def lemma_tokenizer(str_input): # '''Pay attention to the input: this is string input, not token!'''\n",
    "    tokens=word_tokenize(str_input)\n",
    "    return [WordNetLemmatizer().lemmatize(word=word, pos=get_wordnet_pos(pos)) for word, pos in nltk.pos_tag(tokens)]\n",
    "\n",
    "if tokenizer=='lemma':\n",
    "    tokenizer=lemma_tokenizer\n",
    "elif tokenizer=='porter':\n",
    "    tokenizer=porter_tokenizer\n",
    "################ Define tokenizer ################\n",
    "##########################################################\n",
    "\n",
    "##########################################################\n",
    "######### Text Vectorization and Transformation ##########\n",
    "# 1. Use Porter Stemmer.\n",
    "# 2. Use word level, character level does not make sense for current situation.\n",
    "# 3. Use count (freq) and tf-idf vectorizer. see: \n",
    "# Bengfort, B., Bilbro, R., & Ojeda, T. (2018). Applied Text Analysis with Python: Enabling Language-Aware Data Products with Machine Learning (1 edition). Beijing Boston Farnham Sebastopol Tokyo: OReilly Media.\n",
    "# Page: 67.\n",
    "\n",
    "if vect_type=='count':\n",
    "    ##### Token counts #####\n",
    "    # create the transform\n",
    "    vectorizer = CountVectorizer(stop_words='english', \n",
    "                                 tokenizer=tokenizer, \n",
    "                                 analyzer='word'\n",
    "                                )\n",
    "    # tokenize and build vocab.\n",
    "    vectorizer.fit(x_train) # Using training dataset to build vocabulary.\n",
    "    # Encode document: transform the training and validation data using count vectorizer object\n",
    "    x_train_vect =  vectorizer.transform(x_train)\n",
    "elif vect_type=='tfidf':\n",
    "    ##### TF-IDF #####\n",
    "    # create the transform\n",
    "    vectorizer = TfidfVectorizer(stop_words='english', \n",
    "                                 tokenizer=tokenizer, \n",
    "                                 analyzer='word'\n",
    "                                )\n",
    "    # tokenize and build vocab\n",
    "    vectorizer.fit(x_train) # Using training dataset to build vocabulary.\n",
    "    # Encode document: transform the training and validation data using tfidf vectorizer object\n",
    "    x_train_vect =  vectorizer.transform(x_train)\n",
    "######### Text Vectorization and Transformation ##########\n",
    "##########################################################\n",
    "\n",
    "# from imblearn.over_sampling import ADASYN\n",
    "# ada = ADASYN(sampling_strategy='minority', random_state=42)\n",
    "\n",
    "from imblearn.combine import SMOTEENN\n",
    "smote_enn = SMOTEENN(sampling_strategy='auto', random_state=42)\n",
    "\n",
    "x_train_res, y_train_res = smote_enn.fit_resample(x_train_vect, y_train)\n",
    "classifier.fit(x_train_res, y_train_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "#### Test trained model on Universal Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################\n",
    "################ Prepare dataframe for ML ################\n",
    "#### Sample ####\n",
    "# Build training and testing data frame.\n",
    "x_valid=df_universal_test[input_text]\n",
    "y_valid=df_universal_test['broad_cat']\n",
    "################ Prepare dataframe for ML ################\n",
    "##########################################################\n",
    "\n",
    "##########################################################\n",
    "################ Define tokenizer ################\n",
    "\n",
    "def porter_tokenizer(str_input): # '''Pay attention to the input: this is string input, not token!'''\n",
    "    tokens = word_tokenize(str_input)\n",
    "    return [PorterStemmer().stem(token) for token in tokens]\n",
    "\n",
    "# Lemmatize using POS tags, assume to improve accuracy.\n",
    "# Ref: \n",
    "#   - https://stackoverflow.com/questions/15586721/wordnet-lemmatization-and-pos-tagging-in-python\n",
    "#   - https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "def lemma_tokenizer(str_input): # '''Pay attention to the input: this is string input, not token!'''\n",
    "    tokens=word_tokenize(str_input)\n",
    "    return [WordNetLemmatizer().lemmatize(word=word, pos=get_wordnet_pos(pos)) for word, pos in nltk.pos_tag(tokens)]\n",
    "\n",
    "if tokenizer=='lemma':\n",
    "    tokenizer=lemma_tokenizer\n",
    "elif tokenizer=='porter':\n",
    "    tokenizer=porter_tokenizer\n",
    "################ Define tokenizer ################\n",
    "##########################################################\n",
    "\n",
    "##########################################################\n",
    "######### Text Vectorization and Transformation ##########\n",
    "# 1. Use Porter Stemmer.\n",
    "# 2. Use word level, character level does not make sense for current situation.\n",
    "# 3. Use count (freq) and tf-idf vectorizer. see: \n",
    "# Bengfort, B., Bilbro, R., & Ojeda, T. (2018). Applied Text Analysis with Python: Enabling Language-Aware Data Products with Machine Learning (1 edition). Beijing Boston Farnham Sebastopol Tokyo: OReilly Media.\n",
    "# Page: 67.\n",
    "\n",
    "if vect_type=='count':\n",
    "    ##### Token counts #####\n",
    "    # create the transform\n",
    "    vectorizer = CountVectorizer(stop_words='english', \n",
    "                                 tokenizer=tokenizer, \n",
    "                                 analyzer='word'\n",
    "                                )\n",
    "    # tokenize and build vocab.\n",
    "    vectorizer.fit(x_train)\n",
    "    # Encode document: transform the training and validation data using count vectorizer object\n",
    "    x_valid_vect =  vectorizer.transform(x_valid)\n",
    "elif vect_type=='tfidf':\n",
    "    ##### TF-IDF #####\n",
    "    # create the transform\n",
    "    vectorizer = TfidfVectorizer(stop_words='english', \n",
    "                                 tokenizer=tokenizer, \n",
    "                                 analyzer='word'\n",
    "                                )\n",
    "    # tokenize and build vocab\n",
    "    vectorizer.fit(x_train)\n",
    "    # Encode document: transform the training and validation data using tfidf vectorizer object\n",
    "    x_valid_vect =  vectorizer.transform(x_valid)\n",
    "######### Text Vectorization and Transformation ##########\n",
    "##########################################################\n",
    "\n",
    "predictions = classifier.predict(x_valid_vect)\n",
    "performance_dict= {'input_text':input_text,\n",
    "                   'classifier':str(classifier), \n",
    "                   'tokenizer':tokenizer.__name__, \n",
    "                   'vect_type':vect_type, \n",
    "                   'average_mtd':average_mtd,\n",
    "                   'accuracy':metrics.accuracy_score(predictions, y_valid), \n",
    "                   'precision':metrics.precision_score(y_pred=predictions, y_true=y_valid, average=average_mtd),\n",
    "                   'recall':metrics.recall_score(y_pred=predictions, y_true=y_valid, average=average_mtd),\n",
    "                   'f1':metrics.f1_score(y_pred=predictions, y_true=y_valid, average=average_mtd)\n",
    "                  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_text': 'mission_prgrm_spellchk',\n",
       " 'classifier': 'ComplementNB(alpha=1.0, class_prior=None, fit_prior=True, norm=False)',\n",
       " 'tokenizer': 'lemma_tokenizer',\n",
       " 'vect_type': 'tfidf',\n",
       " 'average_mtd': 'macro',\n",
       " 'accuracy': 0.6083093739477297,\n",
       " 'precision': 0.5708095602016783,\n",
       " 'recall': 0.7363826384029141,\n",
       " 'f1': 0.572772059476367}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "performance_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   pre       rec       spe        f1       geo       iba       sup\n",
      "\n",
      "          I       0.74      0.83      0.96      0.78      0.89      0.79      4291\n",
      "         II       0.79      0.70      0.96      0.74      0.82      0.66      6419\n",
      "        III       0.44      0.93      0.94      0.60      0.94      0.88      1861\n",
      "         IV       0.54      0.83      0.91      0.65      0.87      0.75      4329\n",
      "         IX       0.37      0.91      0.93      0.52      0.92      0.84      1701\n",
      "          V       0.94      0.31      0.99      0.47      0.56      0.29     11723\n",
      "         VI       0.13      0.78      0.94      0.23      0.86      0.72       436\n",
      "        VII       0.81      0.54      0.97      0.65      0.73      0.50      6749\n",
      "       VIII       0.37      0.79      0.96      0.50      0.87      0.75      1098\n",
      "\n",
      "avg / total       0.75      0.61      0.96      0.61      0.75      0.57     38607\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from imblearn.metrics import classification_report_imbalanced\n",
    "print(classification_report_imbalanced(y_true=y_valid, y_pred=predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5659455417103673"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from imblearn.metrics import geometric_mean_score as gmean\n",
    "from imblearn.metrics import make_index_balanced_accuracy as iba\n",
    "gmean = iba(alpha=0.1, squared=True)(gmean)\n",
    "gmean(y_true=y_valid, y_pred=predictions, average='weighted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Python\n",
    "print(classification_report_imbalanced(y_true=y_valid, y_pred=predictions)) # SMOTEENN(sampling_strategy='auto', random_state=42)\n",
    "```\n",
    "                       pre       rec       spe        f1       geo       iba       sup\n",
    "\n",
    "              I       0.74      0.83      0.96      0.78      0.89      0.79      4291\n",
    "             II       0.79      0.70      0.96      0.74      0.82      0.66      6419\n",
    "            III       0.44      0.93      0.94      0.60      0.94      0.88      1861\n",
    "             IV       0.54      0.83      0.91      0.65      0.87      0.75      4329\n",
    "             IX       0.37      0.91      0.93      0.52      0.92      0.84      1701\n",
    "              V       0.94      0.31      0.99      0.47      0.56      0.29     11723\n",
    "             VI       0.13      0.78      0.94      0.23      0.86      0.72       436\n",
    "            VII       0.81      0.54      0.97      0.65      0.73      0.50      6749\n",
    "           VIII       0.37      0.79      0.96      0.50      0.87      0.75      1098\n",
    "\n",
    "    avg / total       0.75      0.61      0.96      0.61      0.75      0.57     38607"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Python\n",
    "print(classification_report_imbalanced(y_true=y_valid, y_pred=predictions)) # strategy='minority'\n",
    "```\n",
    "                       pre       rec       spe        f1       geo       iba       sup\n",
    "\n",
    "              I       0.83      0.81      0.98      0.82      0.89      0.78      4291\n",
    "             II       0.82      0.75      0.97      0.78      0.85      0.71      6419\n",
    "            III       0.82      0.79      0.99      0.81      0.89      0.77      1861\n",
    "             IV       0.81      0.66      0.98      0.73      0.80      0.63      4329\n",
    "             IX       0.82      0.69      0.99      0.75      0.83      0.67      1701\n",
    "              V       0.77      0.81      0.90      0.79      0.85      0.72     11723\n",
    "             VI       0.11      0.88      0.92      0.19      0.90      0.80       436\n",
    "            VII       0.79      0.63      0.96      0.70      0.78      0.59      6749\n",
    "           VIII       0.75      0.33      1.00      0.46      0.58      0.31      1098\n",
    "\n",
    "    avg / total       0.79      0.73      0.95      0.75      0.83      0.68     38607"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Python\n",
    "print(classification_report_imbalanced(y_true=y_valid, y_pred=predictions)) # strategy='auto', = 'not majority'\n",
    "```\n",
    "                       pre       rec       spe        f1       geo       iba       sup\n",
    "\n",
    "              I       0.77      0.83      0.97      0.80      0.90      0.80      4291\n",
    "             II       0.82      0.71      0.97      0.76      0.83      0.67      6419\n",
    "            III       0.47      0.94      0.95      0.63      0.94      0.89      1861\n",
    "             IV       0.62      0.82      0.94      0.71      0.87      0.76      4329\n",
    "             IX       0.44      0.92      0.95      0.59      0.93      0.86      1701\n",
    "              V       0.88      0.53      0.97      0.66      0.72      0.49     11723\n",
    "             VI       0.20      0.75      0.97      0.32      0.85      0.71       436\n",
    "            VII       0.81      0.56      0.97      0.66      0.74      0.52      6749\n",
    "           VIII       0.40      0.79      0.97      0.53      0.87      0.75      1098\n",
    "\n",
    "    avg / total       0.76      0.68      0.96      0.69      0.80      0.64     38607"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Python\n",
    "print(classification_report_imbalanced(y_true=y_valid, y_pred=predictions)) # No resampling.\n",
    "```\n",
    "\n",
    "                       pre       rec       spe        f1       geo       iba       sup\n",
    "\n",
    "              I       0.81      0.84      0.98      0.83      0.91      0.81      4291\n",
    "             II       0.79      0.79      0.96      0.79      0.87      0.75      6419\n",
    "            III       0.80      0.82      0.99      0.81      0.90      0.80      1861\n",
    "             IV       0.77      0.78      0.97      0.77      0.87      0.74      4329\n",
    "             IX       0.82      0.68      0.99      0.74      0.82      0.65      1701\n",
    "              V       0.78      0.84      0.89      0.81      0.87      0.74     11723\n",
    "             VI       0.41      0.09      1.00      0.15      0.30      0.08       436\n",
    "            VII       0.76      0.70      0.95      0.73      0.82      0.65      6749\n",
    "           VIII       0.65      0.62      0.99      0.64      0.78      0.59      1098\n",
    "\n",
    "    avg / total       0.78      0.78      0.95      0.77      0.85      0.72     38607"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
