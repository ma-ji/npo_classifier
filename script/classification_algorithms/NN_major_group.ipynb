{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "# Check GPU device.\n",
    "from keras import backend as K\n",
    "print(K.tensorflow_backend._get_available_gpus())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://richliao.github.io/supervised/classification/2016/11/26/textclassifier-convolutional/\n",
    "#https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html\n",
    "#RNN\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from multiprocessing import Pool\n",
    "from spellchecker import SpellChecker\n",
    "import string\n",
    "import math\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "import keras.preprocessing.text as kpt\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Embedding\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk import word_tokenize\n",
    "\n",
    "# For encoding labels.\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read and compile tranining and validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(154424, 38607)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_file_path='../../dataset/UCF/train/'\n",
    "file_list=os.listdir(train_file_path)\n",
    "df_train=pd.DataFrame()\n",
    "for file in file_list:\n",
    "    df_train=pd.concat([df_train, pd.read_pickle(train_file_path+file, compression='gzip')])\n",
    "\n",
    "test_file_path='../../dataset/UCF/test/'\n",
    "file_list=os.listdir(test_file_path)\n",
    "df_test=pd.DataFrame()\n",
    "for file in file_list:\n",
    "    df_test=pd.concat([df_test, pd.read_pickle(test_file_path+file, compression='gzip')])\n",
    "    \n",
    "len(df_train), len(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "154424 25\n",
      "38607 25\n"
     ]
    }
   ],
   "source": [
    "df_train['mission_prgrm_spellchk']=df_train['TAXPAYER_NAME']+' '+df_train['mission_spellchk']+' '+df_train['prgrm_dsc_spellchk'] # Using spell-checked.\n",
    "print(len(df_train['mission_prgrm_spellchk']), len(df_train['NTEE1'].drop_duplicates()))\n",
    "\n",
    "df_test['mission_prgrm_spellchk']=df_test['TAXPAYER_NAME']+' '+df_test['mission_spellchk']+' '+df_test['prgrm_dsc_spellchk'] # Using spell-checked.\n",
    "print(len(df_test['mission_prgrm_spellchk']), len(df_test['NTEE1'].drop_duplicates()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NTEE1\n",
      "A    0.110151\n",
      "B    0.167247\n",
      "C    0.021519\n",
      "D    0.027450\n",
      "E    0.058378\n",
      "F    0.014901\n",
      "G    0.032722\n",
      "H    0.003024\n",
      "I    0.019084\n",
      "J    0.030902\n",
      "K    0.013010\n",
      "L    0.038478\n",
      "M    0.030390\n",
      "N    0.100114\n",
      "O    0.011209\n",
      "P    0.059447\n",
      "Q    0.012867\n",
      "R    0.006890\n",
      "S    0.093632\n",
      "T    0.013159\n",
      "U    0.006476\n",
      "V    0.002266\n",
      "W    0.054117\n",
      "X    0.029568\n",
      "Y    0.042998\n",
      "Name: EIN, dtype: float64 \n",
      "\n",
      " NTEE1\n",
      "A    0.111146\n",
      "B    0.166265\n",
      "C    0.021421\n",
      "D    0.026783\n",
      "E    0.059756\n",
      "F    0.014065\n",
      "G    0.035045\n",
      "H    0.003264\n",
      "I    0.019168\n",
      "J    0.029321\n",
      "K    0.013521\n",
      "L    0.039811\n",
      "M    0.029528\n",
      "N    0.101666\n",
      "O    0.010594\n",
      "P    0.060041\n",
      "Q    0.011293\n",
      "R    0.006657\n",
      "S    0.093325\n",
      "T    0.014013\n",
      "U    0.005828\n",
      "V    0.002202\n",
      "W    0.052788\n",
      "X    0.028440\n",
      "Y    0.044059\n",
      "Name: EIN, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# # Build training and testing data frame.\n",
    "# small_num=0\n",
    "# while small_num<500: # Make sure each category has at least 500 records.\n",
    "#     sampleDF = df_train[df_train.mission.notna() & df_train.NTEE1.notna()].sample(120000)\n",
    "#     trainDF, valDF =train_test_split(sampleDF, test_size=.3)\n",
    "#     small_num=trainDF.groupby('NTEE_M').count().sort_values('EIN').iloc[0]['EIN']\n",
    "\n",
    "# See the composition by NTEE major groups.\n",
    "print(df_train.groupby('NTEE1').count()['EIN']/len(df_train), '\\n'*2, df_test.groupby('NTEE1').count()['EIN']/len(df_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare labels.\n",
    "*One hot encoding.* Prepare after resampling; otherwise, shape of `y_train` will shrink from 25 to 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "lb = preprocessing.LabelBinarizer()\n",
    "lb.fit(list(df_train.NTEE1.unique()))\n",
    "\n",
    "y_train=lb.transform(df_train['NTEE1'])\n",
    "# y_test=lb.transform(df_test['NTEE1']) # No need to transform Y for testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare input text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_list=[s.upper() for s in stopwords.words('english')+list(string.punctuation)]\n",
    "def stopwords_remove(string):\n",
    "    global stop_list\n",
    "    tokens=word_tokenize(string)\n",
    "    return [s for s in tokens if s not in stop_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_token_list_train=df_train['mission_prgrm_spellchk'].apply(stopwords_remove)\n",
    "# text_token_list_test=df_test['mission_prgrm_spellchk'].apply(stopwords_remove)\n",
    "\n",
    "# No difference, stopwords not removed.\n",
    "# https://stackoverflow.com/questions/34721984/stopword-removing-when-using-the-word2vec/40447086#40447086\n",
    "text_token_list_train=df_train['mission_prgrm_spellchk']\n",
    "text_token_list_test=df_test['mission_prgrm_spellchk']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('and', 1), ('the', 2), ('to', 3), ('of', 4), ('in', 5)]\n"
     ]
    }
   ],
   "source": [
    "# Build word index for train and validation texts.\n",
    "tokenizer=Tokenizer()\n",
    "tokenizer.fit_on_texts(text_token_list_train.to_list()+text_token_list_test.to_list())\n",
    "print(list(tokenizer.word_index.items())[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# Save tokenizer class for developing package.\n",
    "with open('../../output/tokenizer.pkl', 'wb') as output:\n",
    "    pickle.dump(tokenizer, output, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text to sequences.\n",
    "seq_encoding_text_train=tokenizer.texts_to_sequences(text_token_list_train)\n",
    "seq_encoding_text_test=tokenizer.texts_to_sequences(text_token_list_test)\n",
    "\n",
    "# Pads sequences to the same length (i.e., prepare matrix).\n",
    "x_test=pad_sequences(sequences=seq_encoding_text_test,\n",
    "                    maxlen=max([len(s) for s in seq_encoding_text_train]), # Max length of the sequence.\n",
    "                    dtype = \"int32\", padding = \"post\", truncating = \"post\", \n",
    "                    value = 0 # Zero is used for representing None or Unknown.\n",
    "                     )\n",
    "x_train=pad_sequences(sequences=seq_encoding_text_train,\n",
    "                      maxlen=max([len(s) for s in seq_encoding_text_train]), # Max length of the sequence.\n",
    "                      dtype = \"int32\", padding = \"post\", truncating = \"post\", \n",
    "                      value = 0 # Zero is used for representing None or Unknown.\n",
    "                     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to Compressed Sparse Row matrix; otherwise, matrix too large, result memory error.\n",
    "# https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html\n",
    "from scipy.sparse import csr_matrix\n",
    "x_train=csr_matrix(x_train)\n",
    "\n",
    "# Define resample strategy.\n",
    "def func_resample(method, sampling_strategy, x_train_vect, y_train):\n",
    "    if method=='ADASYN':\n",
    "        from imblearn.over_sampling import ADASYN\n",
    "        resample = ADASYN(sampling_strategy=sampling_strategy, random_state=42)\n",
    "    elif method=='RandomOverSampler':\n",
    "        from imblearn.over_sampling import RandomOverSampler\n",
    "        resample = RandomOverSampler(sampling_strategy=sampling_strategy, random_state=42)\n",
    "    elif method=='SMOTE':\n",
    "        from imblearn.over_sampling import SMOTE\n",
    "        resample = SMOTE(sampling_strategy=sampling_strategy, random_state=42)\n",
    "    elif method=='SMOTEENN':\n",
    "        from imblearn.combine import SMOTEENN\n",
    "        resample = SMOTEENN(sampling_strategy=sampling_strategy, random_state=42)\n",
    "    elif method=='SMOTETomek':\n",
    "        from imblearn.combine import SMOTETomek\n",
    "        resample = SMOTETomek(sampling_strategy=sampling_strategy, random_state=42)\n",
    "    x_train_vect_res, y_train_res = resample.fit_resample(x_train_vect, y_train)\n",
    "    return [x_train_vect_res, y_train_res]\n",
    "\n",
    "x_train_res, y_train_res = func_resample(method='ADASYN', sampling_strategy='minority', \n",
    "                                         x_train_vect=x_train, y_train=y_train)\n",
    "\n",
    "# x_train_res, y_train_res = [x_train, y_train]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare embedding layer.\n",
    "Use pre-trained GloVe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "EMBEDDING_DIM=100\n",
    "glove_word_vector=api.load('glove-wiki-gigaword-'+str(EMBEDDING_DIM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((len(tokenizer.word_index)+1, EMBEDDING_DIM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 204878/204878 [00:00<00:00, 365671.15it/s]\n"
     ]
    }
   ],
   "source": [
    "for word, index in tqdm(tokenizer.word_index.items()):\n",
    "    try:\n",
    "        embedding_matrix[index] = glove_word_vector.get_vector(word)\n",
    "    except:\n",
    "        pass\n",
    "        # words not found in embedding index will be all-zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/\n",
    "embedding_layer = Embedding(input_dim=len(tokenizer.word_index)+1, # Size of vocabulary.\n",
    "                            input_length=max([len(s) for s in seq_encoding_text_train]), # Length of input, i.e., length of padded sequence.\n",
    "                            output_dim=EMBEDDING_DIM, # Size of the vector space in which words will be embedded.\n",
    "                            weights=[embedding_matrix],\n",
    "                            trainable=False\n",
    "                           )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic tuning of training params."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, GlobalMaxPool1D, Conv1D\n",
    "from keras.layers.advanced_activations import LeakyReLU, PReLU\n",
    "import tensorflow as tf\n",
    "\n",
    "with tf.device('/gpu:1'): # Specify which GPU to use.\n",
    "    # define the model\n",
    "    model = Sequential()\n",
    "    model.add(embedding_layer)\n",
    "    # model.add(Flatten())\n",
    "    model.add(Conv1D(128, 5, activation='softplus'))\n",
    "    model.add(GlobalMaxPool1D())\n",
    "    model.add(Dense(units=32, activation='sigmoid'))\n",
    "    model.add(Dense(units=16, activation='softplus'))\n",
    "    # model.add(PReLU()) # https://medium.com/tinymind/a-practical-guide-to-relu-b83ca804f1f7\n",
    "    model.add(Dense(units=16, activation='tanh'))\n",
    "    model.add(Dense(units=16, activation='softplus'))\n",
    "    model.add(Dense(units=len(y_train[0]), activation='softmax'))\n",
    "    # compile the model\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc', \n",
    "    #                                                                      precision, recall\n",
    "                                                                        ])\n",
    "    # summarize the model\n",
    "    print(model.summary())\n",
    "\n",
    "    # fit the model\n",
    "    history=model.fit(x_train, y_train, validation_split=0.3, epochs=20, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try Grid Search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continue previous work.\n",
    "df_history=pd.read_csv('../../output/grid_search_history_major_group.tsv', sep='\\t', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_list_done=set(map(tuple, \n",
    "                        df_history[['conv_num_filters', 'conv_kernel_size', 'conv_act', 'out_act']].values.tolist()\n",
    "                       )\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, GlobalMaxPool1D, Conv1D\n",
    "import tensorflow as tf\n",
    "from datetime import datetime\n",
    "\n",
    "# df_history=pd.DataFrame()\n",
    "for num_filters in [32, 64, 128]:\n",
    "    for kernel_size in [3,5,7]:\n",
    "        for conv_act in ['sigmoid', 'softplus', 'tanh', 'softmax']:\n",
    "            for out_act in ['sigmoid', 'softplus', 'tanh', 'softmax']:\n",
    "                param=tuple((num_filters, kernel_size, conv_act, out_act))\n",
    "                if param not in param_list_done:\n",
    "                    t1=datetime.now()\n",
    "                    # Run NN on a specified GPU.\n",
    "                    with tf.device('/device:GPU:1'):\n",
    "                        # define the model\n",
    "                        model = Sequential()\n",
    "                        model.add(embedding_layer)\n",
    "                        # model.add(Flatten())\n",
    "                        model.add(Conv1D(num_filters, kernel_size, activation=conv_act))\n",
    "                        model.add(GlobalMaxPool1D())\n",
    "                        model.add(Dense(units=32, activation='sigmoid'))\n",
    "                        model.add(Dense(units=32, activation='softplus'))\n",
    "                        model.add(Dense(units=16, activation='tanh'))\n",
    "                        model.add(Dense(units=16, activation='softplus'))\n",
    "                        model.add(Dense(units=len(y_train[0]), activation=out_act))\n",
    "                        # compile the model\n",
    "                        model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
    "                        # F1, precision, and recall removed. https://github.com/keras-team/keras/issues/5794\n",
    "                        # fit the model\n",
    "                        history=model.fit(x_train, y_train, validation_split=0.2, epochs=50, verbose=0)\n",
    "                        y_prob = model.predict(x_val, verbose=0)\n",
    "                    # Save history.\n",
    "                    acc = history.history['acc']\n",
    "                    val_acc = history.history['val_acc']\n",
    "                    loss = history.history['loss']\n",
    "                    val_loss = history.history['val_loss']\n",
    "                    epochs = range(1, len(acc) + 1)\n",
    "                    # Calculate on validation dataset.\n",
    "                    y_classes = y_prob.argmax(axis=-1)\n",
    "                    y_classes_prob=[s.max() for s in y_prob]\n",
    "                    y_classes_val=y_val.argmax(axis=-1)\n",
    "                    df_val=pd.DataFrame({'pred':y_classes, \n",
    "                                         'true':y_classes_val, \n",
    "                                         'prob':y_classes_prob})\n",
    "                    val_acc_real=len(df_val[df_val.pred==df_val.true])/len(df_val)\n",
    "                    # Save history to datafame.\n",
    "                    df_history_temp=pd.DataFrame()\n",
    "                    df_history_temp['acc']=acc\n",
    "                    df_history_temp['val_acc']=val_acc\n",
    "                    df_history_temp['val_acc_real']=[math.nan]*(len(epochs)-1)+[val_acc_real]\n",
    "                    df_history_temp['loss']=loss\n",
    "                    df_history_temp['val_loss']=val_loss\n",
    "                    df_history_temp['epochs']=epochs\n",
    "                    df_history_temp['conv_num_filters']=[num_filters]*len(epochs)\n",
    "                    df_history_temp['conv_kernel_size']=[kernel_size]*len(epochs)\n",
    "                    df_history_temp['conv_act']=[conv_act]*len(epochs)\n",
    "                    df_history_temp['out_act']=[out_act]*len(epochs)\n",
    "                    df_history_temp['time_stamp']=[str(t1)]+[math.nan]*(len(epochs)-2)+[str(datetime.now())]\n",
    "                    df_history=df_history.append(df_history_temp, ignore_index=True)\n",
    "                    df_history.to_csv('../../output/grid_search_history_major_group.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision making: Optimizing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continue previous work.\n",
    "try:\n",
    "    df_history=pd.read_csv('../../output/grid_search_history_major_group_optimizing.tsv', sep='\\t', index_col=0)\n",
    "except:\n",
    "    df_history=pd.DataFrame(columns=pd.read_csv('../../output/grid_search_history_major_group.tsv', sep='\\t', index_col=0).columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_list_done=set(map(tuple, \n",
    "                        df_history[['conv_num_filters', 'conv_kernel_size', 'conv_act', 'out_act']].values.tolist()\n",
    "                       )\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, GlobalMaxPool1D, Conv1D\n",
    "import tensorflow as tf\n",
    "from datetime import datetime\n",
    "\n",
    "# df_history=pd.DataFrame()\n",
    "for num_filters in [128, 256, 512, 1024]:\n",
    "    for kernel_size in [3]:\n",
    "        for conv_act in ['softplus']:\n",
    "            for out_act in ['sigmoid', 'softplus', 'softmax']:\n",
    "                param=tuple((num_filters, kernel_size, conv_act, out_act))\n",
    "                if param not in param_list_done:\n",
    "                    t1=datetime.now()\n",
    "                    # Run NN on a specified GPU.\n",
    "                    with tf.device('/device:GPU:1'):\n",
    "                        # define the model\n",
    "                        model = Sequential()\n",
    "                        model.add(embedding_layer)\n",
    "                        # model.add(Flatten())\n",
    "                        model.add(Conv1D(num_filters, kernel_size, activation=conv_act))\n",
    "                        model.add(GlobalMaxPool1D())\n",
    "                        model.add(Dense(units=32, activation='sigmoid'))\n",
    "                        model.add(Dense(units=32, activation='softplus'))\n",
    "                        model.add(Dense(units=16, activation='tanh'))\n",
    "                        model.add(Dense(units=16, activation='softplus'))\n",
    "                        model.add(Dense(units=len(y_train[0]), activation=out_act))\n",
    "                        # compile the model\n",
    "                        model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
    "                        # F1, precision, and recall removed. https://github.com/keras-team/keras/issues/5794\n",
    "                        # fit the model\n",
    "                        history=model.fit(x_train, y_train, validation_split=0.2, epochs=50, verbose=0)\n",
    "                        y_prob = model.predict(x_val, verbose=0)\n",
    "                    # Save history.\n",
    "                    acc = history.history['acc']\n",
    "                    val_acc = history.history['val_acc']\n",
    "                    loss = history.history['loss']\n",
    "                    val_loss = history.history['val_loss']\n",
    "                    epochs = range(1, len(acc) + 1)\n",
    "                    # Calculate on validation dataset.\n",
    "                    y_classes = y_prob.argmax(axis=-1)\n",
    "                    y_classes_prob=[s.max() for s in y_prob]\n",
    "                    y_classes_val=y_val.argmax(axis=-1)\n",
    "                    df_val=pd.DataFrame({'pred':y_classes, \n",
    "                                         'true':y_classes_val, \n",
    "                                         'prob':y_classes_prob})\n",
    "                    val_acc_real=len(df_val[df_val.pred==df_val.true])/len(df_val)\n",
    "                    # Save history to datafame.\n",
    "                    df_history_temp=pd.DataFrame()\n",
    "                    df_history_temp['acc']=acc\n",
    "                    df_history_temp['val_acc']=val_acc\n",
    "                    df_history_temp['val_acc_real']=[math.nan]*(len(epochs)-1)+[val_acc_real]\n",
    "                    df_history_temp['loss']=loss\n",
    "                    df_history_temp['val_loss']=val_loss\n",
    "                    df_history_temp['epochs']=epochs\n",
    "                    df_history_temp['conv_num_filters']=[num_filters]*len(epochs)\n",
    "                    df_history_temp['conv_kernel_size']=[kernel_size]*len(epochs)\n",
    "                    df_history_temp['conv_act']=[conv_act]*len(epochs)\n",
    "                    df_history_temp['out_act']=[out_act]*len(epochs)\n",
    "                    df_history_temp['time_stamp']=[str(t1)]+[math.nan]*(len(epochs)-2)+[str(datetime.now())]\n",
    "                    df_history=df_history.append(df_history_temp, ignore_index=True)\n",
    "                    df_history.to_csv('../../output/grid_search_history_major_group_optimizing.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model finalist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Best configuration**\n",
    "\n",
    "_Broad Category_\n",
    "\n",
    "|acc | val_acc | val_acc_real | loss | val_loss | epochs | conv_num_filters | conv_kernel_size | conv_act | out_act|\n",
    "|--|--|--|--|--|--|--|--|--|--|\n",
    "|0.820386905 | 0.776488095 | -- | 0.613613255 | 0.738004138 | 4 | 512 | 3 | softplus | softplus|\n",
    "\n",
    "_Major Group_\n",
    "\n",
    "|acc | val_acc | val_acc_real | loss | val_loss | epochs | conv_num_filters | conv_kernel_size | conv_act | out_act|\n",
    "|--|--|--|--|--|--|--|--|--|--|\n",
    "|0.764369048 | 0.710428571 | -- | 0.888776311 | 1.120441045 | 6 | 256 | 3 | softplus | softmax|\n",
    "|0.779 | 0.71 | -- | 0.834419103 | 1.140895644 | 7 | 256 | 3 | softplus | softplus|\n",
    "\n",
    "Use `softplus`+`softplus`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 143907 samples, validate on 35977 samples\n",
      "Epoch 1/7\n",
      "143907/143907 [==============================] - 1751s 12ms/step - loss: 1.4331 - acc: 0.6057 - val_loss: 4.4017 - val_acc: 0.2135\n",
      "Epoch 2/7\n",
      "143907/143907 [==============================] - 1749s 12ms/step - loss: 0.9492 - acc: 0.7563 - val_loss: 4.4199 - val_acc: 0.2204\n",
      "Epoch 3/7\n",
      "143907/143907 [==============================] - 1749s 12ms/step - loss: 0.8217 - acc: 0.7878 - val_loss: 4.5659 - val_acc: 0.2244\n",
      "Epoch 4/7\n",
      "143907/143907 [==============================] - 1749s 12ms/step - loss: 0.7377 - acc: 0.8088 - val_loss: 4.5111 - val_acc: 0.2278\n",
      "Epoch 5/7\n",
      "143907/143907 [==============================] - 1749s 12ms/step - loss: 0.6701 - acc: 0.8254 - val_loss: 5.0968 - val_acc: 0.2269\n",
      "Epoch 6/7\n",
      "143907/143907 [==============================] - 1749s 12ms/step - loss: 0.6146 - acc: 0.8383 - val_loss: 4.5156 - val_acc: 0.2284\n",
      "Epoch 7/7\n",
      "143907/143907 [==============================] - 1748s 12ms/step - loss: 0.5709 - acc: 0.8478 - val_loss: 5.0666 - val_acc: 0.2300\n",
      "Test on UCF-Testing\n",
      "38607/38607 [==============================] - 141s 4ms/step\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, GlobalMaxPool1D, Conv1D\n",
    "import tensorflow as tf\n",
    "from datetime import datetime\n",
    "\n",
    "with tf.device('/device:GPU:1'):\n",
    "    # define the model\n",
    "    model = Sequential()\n",
    "    model.add(embedding_layer)\n",
    "    # model.add(Flatten())\n",
    "    model.add(Conv1D(512, 3, activation='softplus'))\n",
    "    model.add(GlobalMaxPool1D())\n",
    "    model.add(Dense(units=32, activation='sigmoid'))\n",
    "    model.add(Dense(units=32, activation='softplus'))\n",
    "    model.add(Dense(units=16, activation='tanh'))\n",
    "    model.add(Dense(units=16, activation='softplus'))\n",
    "    model.add(Dense(units=len(y_train[0]), activation='softplus'))\n",
    "    # compile the model\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
    "    # F1, precision, and recall removed. https://github.com/keras-team/keras/issues/5794\n",
    "    # fit the model\n",
    "    history=model.fit(x_train_res, y_train_res, validation_split=0.2, epochs=7, verbose=1)\n",
    "    print('Test on UCF-Testing')\n",
    "    y_prob = model.predict(x_test, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall ACC: 0.7879659129173466\n"
     ]
    }
   ],
   "source": [
    "# Calculate on validation dataset.\n",
    "# From probability --> serial coding, e.g., [4, 3, 2, 55] --> categorical, e.g., [[00010...], [001000..]...]\n",
    "y_pred = lb.inverse_transform(np_utils.to_categorical(y_prob.argmax(axis=-1)))\n",
    "y_pred_prob=[s.max() for s in y_prob]\n",
    "df_val=pd.DataFrame({'pred':y_pred, \n",
    "                     'true':df_test['NTEE1'], \n",
    "                     'prob':y_pred_prob})\n",
    "print('Overall ACC:', len(df_val[df_val.pred==df_val.true])/len(df_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   pre       rec       spe        f1       geo       iba       sup\n",
      "\n",
      "          A       0.84      0.86      0.98      0.85      0.92      0.83      4291\n",
      "          B       0.87      0.85      0.97      0.86      0.91      0.82      6419\n",
      "          C       0.67      0.73      0.99      0.70      0.85      0.70       827\n",
      "          D       0.87      0.86      1.00      0.87      0.93      0.85      1034\n",
      "          E       0.76      0.79      0.98      0.78      0.88      0.76      2307\n",
      "          F       0.67      0.50      1.00      0.58      0.71      0.48       543\n",
      "          G       0.68      0.65      0.99      0.67      0.80      0.62      1353\n",
      "          H       0.67      0.02      1.00      0.03      0.13      0.01       126\n",
      "          I       0.71      0.70      0.99      0.71      0.84      0.68       740\n",
      "          J       0.84      0.69      1.00      0.76      0.83      0.67      1132\n",
      "          K       0.70      0.74      1.00      0.72      0.86      0.72       522\n",
      "          L       0.74      0.77      0.99      0.76      0.87      0.75      1537\n",
      "          M       0.85      0.90      0.99      0.87      0.95      0.89      1140\n",
      "          N       0.88      0.91      0.99      0.89      0.95      0.89      3925\n",
      "          O       0.53      0.70      0.99      0.60      0.83      0.67       409\n",
      "          P       0.59      0.66      0.97      0.62      0.80      0.62      2318\n",
      "          Q       0.52      0.31      1.00      0.39      0.55      0.29       436\n",
      "          R       0.51      0.36      1.00      0.42      0.60      0.34       257\n",
      "          S       0.78      0.82      0.98      0.80      0.89      0.79      3603\n",
      "          T       0.56      0.45      0.99      0.50      0.67      0.42       541\n",
      "          U       0.42      0.40      1.00      0.41      0.63      0.37       225\n",
      "          V       0.00      0.00      1.00      0.00      0.00      0.00        85\n",
      "          W       0.87      0.84      0.99      0.85      0.91      0.82      2038\n",
      "          X       0.71      0.71      0.99      0.71      0.84      0.69      1098\n",
      "          Y       0.86      0.88      0.99      0.87      0.93      0.86      1701\n",
      "\n",
      "avg / total       0.79      0.79      0.98      0.78      0.88      0.76     38607\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from imblearn.metrics import classification_report_imbalanced\n",
    "print(classification_report_imbalanced(y_true=df_test['NTEE1'], y_pred=y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model for developing package.\n",
    "model.save('../../output/major_group_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Python\n",
    "from imblearn.metrics import classification_report_imbalanced\n",
    "print(classification_report_imbalanced(y_true=df_test['broad_cat'], y_pred=y_train))\n",
    "```\n",
    "\n",
    "*epochs=4, resampling: method='ADASYN', sampling_strategy='not majority'.*\n",
    "    \n",
    "    Overall ACC: 0.7046390550936359\n",
    "                       pre       rec       spe        f1       geo       iba       sup\n",
    "\n",
    "              A       0.79      0.83      0.97      0.81      0.90      0.80      4291\n",
    "              B       0.86      0.81      0.97      0.83      0.89      0.77      6419\n",
    "              C       0.69      0.52      0.99      0.59      0.72      0.49       827\n",
    "              D       0.77      0.86      0.99      0.81      0.92      0.84      1034\n",
    "              E       0.79      0.64      0.99      0.71      0.80      0.62      2307\n",
    "              F       0.00      0.00      1.00      0.00      0.00      0.00       543\n",
    "              G       0.36      0.74      0.95      0.49      0.84      0.69      1353\n",
    "              H       0.02      0.02      1.00      0.02      0.15      0.02       126\n",
    "              I       0.24      0.31      0.98      0.27      0.55      0.29       740\n",
    "              J       0.49      0.71      0.98      0.58      0.83      0.67      1132\n",
    "              K       0.49      0.57      0.99      0.53      0.75      0.54       522\n",
    "              L       0.67      0.72      0.99      0.69      0.84      0.69      1537\n",
    "              M       0.85      0.89      1.00      0.87      0.94      0.88      1140\n",
    "              N       0.87      0.84      0.99      0.86      0.91      0.81      3925\n",
    "              O       0.41      0.47      0.99      0.44      0.69      0.45       409\n",
    "              P       0.44      0.65      0.95      0.52      0.78      0.60      2318\n",
    "              Q       0.08      0.00      1.00      0.00      0.05      0.00       436\n",
    "              R       0.03      0.00      1.00      0.01      0.06      0.00       257\n",
    "              S       0.83      0.71      0.99      0.77      0.84      0.68      3603\n",
    "              T       0.50      0.19      1.00      0.27      0.43      0.17       541\n",
    "              U       0.00      0.00      1.00      0.00      0.00      0.00       225\n",
    "              V       0.00      0.00      1.00      0.00      0.00      0.00        85\n",
    "              W       0.90      0.75      1.00      0.82      0.86      0.73      2038\n",
    "              X       0.60      0.62      0.99      0.61      0.79      0.59      1098\n",
    "              Y       0.89      0.79      1.00      0.83      0.89      0.77      1701\n",
    "\n",
    "    avg / total       0.71      0.70      0.98      0.70      0.81      0.68     38607\n",
    "\n",
    "\n",
    "*epochs=7, no resampling*\n",
    "\n",
    "    Overall ACC: 0.7776309995596653\n",
    "                       pre       rec       spe        f1       geo       iba       sup\n",
    "\n",
    "              A       0.85      0.85      0.98      0.85      0.91      0.82      4291\n",
    "              B       0.85      0.86      0.97      0.85      0.91      0.82      6419\n",
    "              C       0.57      0.76      0.99      0.65      0.86      0.73       827\n",
    "              D       0.78      0.90      0.99      0.83      0.94      0.88      1034\n",
    "              E       0.77      0.76      0.99      0.76      0.87      0.73      2307\n",
    "              F       0.70      0.37      1.00      0.48      0.61      0.35       543\n",
    "              G       0.56      0.73      0.98      0.64      0.85      0.70      1353\n",
    "              H       0.00      0.00      1.00      0.00      0.00      0.00       126\n",
    "              I       0.78      0.63      1.00      0.70      0.79      0.61       740\n",
    "              J       0.78      0.73      0.99      0.75      0.85      0.71      1132\n",
    "              K       0.67      0.69      1.00      0.68      0.83      0.66       522\n",
    "              L       0.78      0.71      0.99      0.75      0.84      0.69      1537\n",
    "              M       0.86      0.89      1.00      0.87      0.94      0.88      1140\n",
    "              N       0.89      0.90      0.99      0.89      0.94      0.88      3925\n",
    "              O       0.75      0.56      1.00      0.65      0.75      0.54       409\n",
    "              P       0.56      0.69      0.96      0.61      0.81      0.65      2318\n",
    "              Q       0.37      0.39      0.99      0.38      0.63      0.37       436\n",
    "              R       0.60      0.30      1.00      0.40      0.55      0.28       257\n",
    "              S       0.78      0.81      0.98      0.79      0.89      0.78      3603\n",
    "              T       0.55      0.43      0.99      0.48      0.65      0.40       541\n",
    "              U       0.35      0.19      1.00      0.24      0.43      0.17       225\n",
    "              V       0.00      0.00      1.00      0.00      0.00      0.00        85\n",
    "              W       0.89      0.82      0.99      0.85      0.90      0.81      2038\n",
    "              X       0.77      0.66      0.99      0.71      0.81      0.63      1098\n",
    "              Y       0.91      0.83      1.00      0.87      0.91      0.82      1701\n",
    "\n",
    "    avg / total       0.78      0.78      0.98      0.77      0.87      0.75     38607\n",
    "\n",
    "\n",
    "*epochs=7, resampling: method='ADASYN', sampling_strategy='minority'.*\n",
    "\n",
    "    ***Overall ACC: 0.7821897583339809 # This is chosen.***\n",
    "                       pre       rec       spe        f1       geo       iba       sup\n",
    "\n",
    "              A       0.80      0.87      0.97      0.83      0.92      0.83      4291\n",
    "              B       0.85      0.85      0.97      0.85      0.91      0.82      6419\n",
    "              C       0.65      0.74      0.99      0.69      0.86      0.72       827\n",
    "              D       0.80      0.90      0.99      0.85      0.94      0.88      1034\n",
    "              E       0.77      0.78      0.98      0.78      0.88      0.76      2307\n",
    "              F       0.51      0.60      0.99      0.55      0.77      0.57       543\n",
    "              G       0.68      0.68      0.99      0.68      0.82      0.65      1353\n",
    "              H       0.55      0.19      1.00      0.28      0.44      0.17       126\n",
    "              I       0.71      0.71      0.99      0.71      0.84      0.68       740\n",
    "              J       0.86      0.67      1.00      0.75      0.82      0.65      1132\n",
    "              K       0.63      0.68      0.99      0.66      0.82      0.66       522\n",
    "              L       0.70      0.76      0.99      0.73      0.87      0.73      1537\n",
    "              M       0.87      0.90      1.00      0.88      0.95      0.89      1140\n",
    "              N       0.83      0.93      0.98      0.88      0.95      0.90      3925\n",
    "              O       0.65      0.61      1.00      0.63      0.78      0.59       409\n",
    "              P       0.64      0.57      0.98      0.60      0.75      0.53      2318\n",
    "              Q       0.43      0.36      0.99      0.39      0.60      0.33       436\n",
    "              R       0.46      0.21      1.00      0.28      0.45      0.19       257\n",
    "              S       0.84      0.79      0.98      0.81      0.88      0.76      3603\n",
    "              T       0.66      0.32      1.00      0.43      0.56      0.30       541\n",
    "              U       0.52      0.22      1.00      0.31      0.47      0.20       225\n",
    "              V       0.00      0.00      1.00      0.00      0.00      0.00        85\n",
    "              W       0.87      0.86      0.99      0.86      0.92      0.84      2038\n",
    "              X       0.68      0.71      0.99      0.70      0.84      0.69      1098\n",
    "              Y       0.84      0.91      0.99      0.88      0.95      0.90      1701\n",
    "\n",
    "    avg / total       0.78      0.78      0.98      0.78      0.87      0.76     38607\n",
    "\n",
    "*epochs=4, resampling: method='ADASYN', sampling_strategy='minority'.*\n",
    "\n",
    "    Overall ACC: 0.7663895148548191\n",
    "                       pre       rec       spe        f1       geo       iba       sup\n",
    "\n",
    "              A       0.86      0.83      0.98      0.84      0.90      0.80      4291\n",
    "              B       0.86      0.85      0.97      0.86      0.91      0.82      6419\n",
    "              C       0.56      0.81      0.99      0.66      0.89      0.79       827\n",
    "              D       0.86      0.88      1.00      0.87      0.94      0.87      1034\n",
    "              E       0.84      0.71      0.99      0.77      0.84      0.68      2307\n",
    "              F       0.45      0.62      0.99      0.52      0.78      0.59       543\n",
    "              G       0.60      0.64      0.98      0.62      0.79      0.61      1353\n",
    "              H       0.00      0.00      1.00      0.00      0.00      0.00       126\n",
    "              I       0.60      0.73      0.99      0.66      0.85      0.70       740\n",
    "              J       0.79      0.71      0.99      0.75      0.84      0.69      1132\n",
    "              K       0.66      0.75      0.99      0.70      0.87      0.73       522\n",
    "              L       0.74      0.76      0.99      0.75      0.86      0.73      1537\n",
    "              M       0.93      0.85      1.00      0.89      0.92      0.83      1140\n",
    "              N       0.93      0.86      0.99      0.90      0.92      0.84      3925\n",
    "              O       0.51      0.59      0.99      0.55      0.77      0.57       409\n",
    "              P       0.49      0.72      0.95      0.58      0.83      0.67      2318\n",
    "              Q       0.27      0.18      0.99      0.22      0.42      0.17       436\n",
    "              R       0.34      0.21      1.00      0.26      0.46      0.19       257\n",
    "              S       0.88      0.75      0.99      0.81      0.86      0.72      3603\n",
    "              T       0.41      0.06      1.00      0.10      0.24      0.05       541\n",
    "              U       0.35      0.03      1.00      0.05      0.16      0.02       225\n",
    "              V       0.00      0.00      1.00      0.00      0.00      0.00        85\n",
    "              W       0.85      0.84      0.99      0.85      0.91      0.82      2038\n",
    "              X       0.52      0.83      0.98      0.64      0.90      0.80      1098\n",
    "              Y       0.89      0.87      1.00      0.88      0.93      0.86      1701\n",
    "\n",
    "    avg / total       0.77      0.77      0.98      0.76      0.86      0.74     38607\n",
    "\n",
    "    ???    \n",
    "                       pre       rec       spe        f1       geo       iba       sup\n",
    "\n",
    "              A       0.91      0.79      0.99      0.85      0.89      0.77      4291\n",
    "              B       0.82      0.89      0.96      0.86      0.93      0.85      6419\n",
    "              C       0.63      0.72      0.99      0.67      0.84      0.69       827\n",
    "              D       0.77      0.91      0.99      0.83      0.95      0.90      1034\n",
    "              E       0.71      0.83      0.98      0.77      0.90      0.80      2307\n",
    "              F       0.51      0.62      0.99      0.56      0.78      0.59       543\n",
    "              G       0.66      0.69      0.99      0.68      0.82      0.66      1353\n",
    "              H       0.38      0.06      1.00      0.11      0.25      0.06       126\n",
    "              I       0.68      0.73      0.99      0.70      0.85      0.70       740\n",
    "              J       0.81      0.72      0.99      0.76      0.84      0.69      1132\n",
    "              K       0.73      0.62      1.00      0.67      0.79      0.60       522\n",
    "              L       0.70      0.76      0.99      0.73      0.87      0.73      1537\n",
    "              M       0.87      0.89      1.00      0.88      0.94      0.88      1140\n",
    "              N       0.89      0.88      0.99      0.89      0.93      0.86      3925\n",
    "              O       0.62      0.64      1.00      0.63      0.80      0.61       409\n",
    "              P       0.59      0.62      0.97      0.61      0.78      0.58      2318\n",
    "              Q       0.43      0.26      1.00      0.33      0.51      0.24       436\n",
    "              R       0.49      0.29      1.00      0.36      0.54      0.27       257\n",
    "              S       0.83      0.79      0.98      0.81      0.88      0.76      3603\n",
    "              T       0.54      0.44      0.99      0.49      0.66      0.42       541\n",
    "              U       0.34      0.12      1.00      0.18      0.35      0.11       225\n",
    "              V       0.00      0.00      1.00      0.00      0.00      0.00        85\n",
    "              W       0.89      0.83      0.99      0.86      0.91      0.81      2038\n",
    "              X       0.68      0.71      0.99      0.70      0.84      0.69      1098\n",
    "              Y       0.84      0.89      0.99      0.86      0.94      0.87      1701\n",
    "\n",
    "    avg / total       0.78      0.78      0.98      0.78      0.87      0.75     38607\n",
    "    \n",
    "*epochs=4, no resampling*\n",
    "\n",
    "    Overall ACC: 0.7731499469008211\n",
    "                       pre       rec       spe        f1       geo       iba       sup\n",
    "\n",
    "              A       0.84      0.85      0.98      0.85      0.91      0.83      4291\n",
    "              B       0.87      0.85      0.97      0.86      0.91      0.82      6419\n",
    "              C       0.60      0.77      0.99      0.67      0.87      0.75       827\n",
    "              D       0.89      0.89      1.00      0.89      0.94      0.88      1034\n",
    "              E       0.77      0.79      0.99      0.78      0.88      0.76      2307\n",
    "              F       0.50      0.35      1.00      0.41      0.59      0.32       543\n",
    "              G       0.66      0.66      0.99      0.66      0.81      0.63      1353\n",
    "              H       0.00      0.00      1.00      0.00      0.00      0.00       126\n",
    "              I       0.59      0.69      0.99      0.64      0.83      0.66       740\n",
    "              J       0.78      0.71      0.99      0.74      0.84      0.68      1132\n",
    "              K       0.47      0.78      0.99      0.59      0.88      0.76       522\n",
    "              L       0.81      0.70      0.99      0.75      0.83      0.67      1537\n",
    "              M       0.92      0.86      1.00      0.89      0.93      0.85      1140\n",
    "              N       0.90      0.88      0.99      0.89      0.94      0.87      3925\n",
    "              O       0.57      0.47      1.00      0.52      0.69      0.45       409\n",
    "              P       0.53      0.67      0.96      0.59      0.80      0.62      2318\n",
    "              Q       0.40      0.16      1.00      0.23      0.40      0.15       436\n",
    "              R       0.33      0.10      1.00      0.16      0.32      0.09       257\n",
    "              S       0.75      0.83      0.97      0.79      0.90      0.79      3603\n",
    "              T       0.51      0.37      0.99      0.43      0.61      0.35       541\n",
    "              U       0.83      0.02      1.00      0.04      0.15      0.02       225\n",
    "              V       0.00      0.00      1.00      0.00      0.00      0.00        85\n",
    "              W       0.90      0.80      1.00      0.85      0.89      0.78      2038\n",
    "              X       0.62      0.78      0.99      0.69      0.87      0.75      1098\n",
    "              Y       0.88      0.89      0.99      0.89      0.94      0.88      1701\n",
    "\n",
    "    avg / total       0.77      0.77      0.98      0.77      0.86      0.75     38607"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Draft codes saved in `NN_broad_cat.ipynb`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
