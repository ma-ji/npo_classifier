{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection, preprocessing, naive_bayes, metrics\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn import decomposition, ensemble\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from multiprocessing import Pool\n",
    "\n",
    "from imblearn.metrics import geometric_mean_score as gmean\n",
    "from imblearn.metrics import make_index_balanced_accuracy as iba\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le=LabelEncoder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in and prepare data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "154424"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_file_path='../../dataset/df_ntee_universal/train/'\n",
    "file_list=os.listdir(train_file_path)\n",
    "df_train=pd.DataFrame()\n",
    "for file in file_list:\n",
    "    df_train=pd.concat([df_train, pd.read_pickle(train_file_path+file, compression='gzip')])\n",
    "len(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(154424, 25)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['mission_prgrm']=df_train['mission']+' '+df_train['prgrm_dsc']\n",
    "df_train['mission_prgrm_spellchk']=df_train['mission_spellchk']+' '+df_train['prgrm_dsc_spellchk'] # Using spell-checked.\n",
    "len(df_train['mission_prgrm_spellchk']), len(df_train['NTEE1'].drop_duplicates())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NTEE1\n",
      "A    13586\n",
      "B    20618\n",
      "C     2640\n",
      "D     3430\n",
      "E     7153\n",
      "F     1828\n",
      "G     4032\n",
      "H      378\n",
      "I     2349\n",
      "J     3808\n",
      "K     1606\n",
      "L     4824\n",
      "M     3734\n",
      "N    12379\n",
      "O     1379\n",
      "P     7329\n",
      "Q     1572\n",
      "R      842\n",
      "S    11640\n",
      "T     1602\n",
      "U      804\n",
      "V      290\n",
      "W     6686\n",
      "X     3708\n",
      "Y     5322\n",
      "Name: EIN, dtype: int64 \n",
      "\n",
      " NTEE1\n",
      "A    3424\n",
      "B    5209\n",
      "C     683\n",
      "D     809\n",
      "E    1862\n",
      "F     473\n",
      "G    1021\n",
      "H      89\n",
      "I     598\n",
      "J     964\n",
      "K     403\n",
      "L    1118\n",
      "M     959\n",
      "N    3081\n",
      "O     352\n",
      "P    1851\n",
      "Q     415\n",
      "R     222\n",
      "S    2819\n",
      "T     430\n",
      "U     196\n",
      "V      60\n",
      "W    1671\n",
      "X     858\n",
      "Y    1318\n",
      "Name: EIN, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# # Check if the sampling criteria can be satisfied.\n",
    "# small_num=0\n",
    "# while small_num<500: # Make sure each category in training dataset has at least 500 records.\n",
    "#     trainDF, valDF = model_selection.train_test_split(df_train, test_size=.2)\n",
    "#     small_num=trainDF.groupby('broad_cat').count().sort_values('EIN').iloc[0]['EIN']\n",
    "\n",
    "trainDF, testDF = model_selection.train_test_split(df_train, test_size=.2)\n",
    "# See the composition by broad category.\n",
    "print(trainDF.groupby('NTEE1').count()['EIN'], '\\n'*2, valDF.groupby('NTEE1').count()['EIN'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "### Prepare classifier function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redefine vectorizer.\n",
    "def text_vectorizer(tokenizer_type=None, vectorizer_type=None):\n",
    "    ########################################################\n",
    "    ######### Define and choose tokenizers #################\n",
    "    def porter_tokenizer(str_input): # '''Pay attention to the input: this is string input, not token!'''\n",
    "        tokens = word_tokenize(str_input)\n",
    "        return [PorterStemmer().stem(token) for token in tokens]\n",
    "    # Lemmatize using POS tags, assume improving accuracy.\n",
    "    # Ref: \n",
    "    #   - https://stackoverflow.com/questions/15586721/wordnet-lemmatization-and-pos-tagging-in-python\n",
    "    #   - https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\n",
    "    def get_wordnet_pos(treebank_tag):\n",
    "        if treebank_tag.startswith('J'):\n",
    "            return wordnet.ADJ\n",
    "        elif treebank_tag.startswith('V'):\n",
    "            return wordnet.VERB\n",
    "        elif treebank_tag.startswith('N'):\n",
    "            return wordnet.NOUN\n",
    "        elif treebank_tag.startswith('R'):\n",
    "            return wordnet.ADV\n",
    "        else:\n",
    "            return wordnet.NOUN\n",
    "    def lemma_tokenizer(str_input): # '''Pay attention to the input: this is string input, not token!'''\n",
    "        tokens=word_tokenize(str_input)\n",
    "        return [WordNetLemmatizer().lemmatize(word=word, pos=get_wordnet_pos(pos)) for word, pos in nltk.pos_tag(tokens)]\n",
    "    # Choose tokenizer using parameter passed.\n",
    "    if tokenizer_type=='lemma':\n",
    "        tokenizer=lemma_tokenizer\n",
    "    elif tokenizer_type=='porter':\n",
    "        tokenizer=porter_tokenizer\n",
    "    ########################################################\n",
    "    ######### Define and choose vectorizer #################\n",
    "    # 1. Use word level, character level does not make sense for current situation.\n",
    "    # 2. Use count (freq) and tf-idf vectorizer. see: \n",
    "    # Bengfort, B., Bilbro, R., & Ojeda, T. (2018). Applied Text Analysis with Python: Enabling Language-Aware Data Products with Machine Learning (1 edition). Beijing Boston Farnham Sebastopol Tokyo: O’Reilly Media.\n",
    "    # Page: 67.\n",
    "    if vectorizer_type=='count':\n",
    "        ##### Token counts #####\n",
    "        vectorizer = CountVectorizer(stop_words='english', \n",
    "                                     tokenizer=tokenizer, \n",
    "                                     analyzer='word'\n",
    "                                    )\n",
    "        return vectorizer\n",
    "    elif vectorizer_type=='tfidf':\n",
    "        ##### TF-IDF #####\n",
    "        vectorizer = TfidfVectorizer(stop_words='english', \n",
    "                                     tokenizer=tokenizer, \n",
    "                                     analyzer='word'\n",
    "                                    )\n",
    "        return vectorizer\n",
    "    ########################################################\n",
    "\n",
    "# Define resample strategy.\n",
    "def func_resample(method, sampling_strategy, x_train_vect, y_train):\n",
    "    if method=='ADASYN':\n",
    "        from imblearn.over_sampling import ADASYN\n",
    "        resample = ADASYN(sampling_strategy=sampling_strategy, random_state=42)\n",
    "    elif method=='RandomOverSampler':\n",
    "        from imblearn.over_sampling import RandomOverSampler\n",
    "        resample = RandomOverSampler(sampling_strategy=sampling_strategy, random_state=42)\n",
    "    elif method=='SMOTE':\n",
    "        from imblearn.over_sampling import SMOTE\n",
    "        resample = SMOTE(sampling_strategy=sampling_strategy, random_state=42)\n",
    "    elif method=='SMOTENC':\n",
    "        from imblearn.over_sampling import SMOTENC\n",
    "        resample = SMOTENC(sampling_strategy=sampling_strategy, random_state=42)\n",
    "    elif method=='SMOTEENN':\n",
    "        from imblearn.combine import SMOTEENN\n",
    "        resample = SMOTEENN(sampling_strategy=sampling_strategy, random_state=42)\n",
    "    elif method=='SMOTETomek':\n",
    "        from imblearn.combine import SMOTETomek\n",
    "        resample = SMOTETomek(sampling_strategy=sampling_strategy, random_state=42)\n",
    "    x_train_vect_res, y_train_res = resample.fit_resample(x_train_vect, y_train)\n",
    "    return [x_train_vect_res, y_train_res]\n",
    "\n",
    "# Compile the workflow as a function.\n",
    "def func_classifier(param_list):\n",
    "    global gmean, iba, lb\n",
    "    # Pass parameters.\n",
    "    x_train, y_train, x_test, y_test, resample_method, sampling_strategy, classifier, tokenizer, vect_type = param_list\n",
    "    # Encode text input.\n",
    "    vectorizer=text_vectorizer(tokenizer_type=tokenizer, vectorizer_type=vect_type)\n",
    "    vectorizer.fit(x_train.append(x_test)) # Fit on all texts.\n",
    "    x_train_vect=vectorizer.transform(x_train)\n",
    "    x_test_vect=vectorizer.transform(x_test)\n",
    "    # Encode class labels -- Not necessary for NB/RF algorithms: https://stats.stackexchange.com/questions/288095/what-algorithms-require-one-hot-encoding\n",
    "    # See test results below.\n",
    "#     y_train_vect=le.fit_transform(y_train)\n",
    "#     y_test_vect=le.fit_transform(y_test)\n",
    "    # Resample imbalanced dataset.\n",
    "    resample_x_y=func_resample(method=resample_method, sampling_strategy=sampling_strategy,\n",
    "                               x_train_vect=x_train_vect, y_train=y_train)\n",
    "    classifier.fit(resample_x_y[0], resample_x_y[1])\n",
    "    predictions = classifier.predict(x_test_vect)\n",
    "    gmean = iba(alpha=0.1, squared=True)(gmean)\n",
    "\n",
    "    return {'resample_method':resample_method,\n",
    "            'sampling_strategy':sampling_strategy,\n",
    "            'classifier':str(classifier), \n",
    "            'tokenizer':tokenizer, \n",
    "            'vect_type':vect_type, \n",
    "            'weighted_acc': gmean(y_true=y_test, y_pred=predictions, average='weighted')\n",
    "           }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_llist=[]\n",
    "x_train, y_train, x_test, y_test = [trainDF['mission_prgrm_spellchk'], trainDF['NTEE1'],\n",
    "                                    testDF['mission_prgrm_spellchk'], testDF['NTEE1']]\n",
    "for resample_method in ['ADASYN', 'RandomOverSampler', 'SMOTE', 'SMOTENC', 'SMOTEENN', 'SMOTETomek']:\n",
    "    for sampling_strategy in ['minority','not minority','not majority','all']:\n",
    "        for classifier in [naive_bayes.MultinomialNB(), naive_bayes.ComplementNB(), ensemble.RandomForestClassifier()]:\n",
    "            for tokenizer in ['lemma', 'porter']:\n",
    "                for vect_type in ['count', 'tfidf']:\n",
    "                    param_llist+=[[x_train, y_train, x_test, y_test, resample_method, sampling_strategy, classifier, tokenizer, vect_type]]\n",
    "print('total parameter combinations:', len(param_llist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_llist_0, param_llist_1, param_llist_2, param_llist_3, param_llist_4, param_llist_5 = [param_llist[0:48], param_llist[48:48*2], param_llist[48*2:48*3], \n",
    "                                                                                            param_llist[48*3:48*4], param_llist[48*4:48*5], param_llist[48*5:48*6]\n",
    "                                                                                           ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "p=Pool(48)\n",
    "result_dicts=p.map(func_classifier, param_llist_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a list of parameters.\n",
    "param_llist=[]\n",
    "for input_text in ['mission', 'prgrm_dsc', 'mission_prgrm', 'mission_spellchk', 'prgrm_dsc_spellchk', 'mission_prgrm_spellchk']:\n",
    "    for classifier in [ensemble.RandomForestClassifier()]:\n",
    "        for tokenizer in ['lemma', 'porter']:\n",
    "            for vect_type in ['count', 'tfidf']:\n",
    "                for average_mtd in ['macro', 'weighted']:\n",
    "                    param_llist+=[[input_text, classifier, tokenizer, vect_type, average_mtd]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "p=Pool(24)\n",
    "df_performance_rf=pd.DataFrame(p.map(func_classifier, param_llist))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select model, test on Universal Testing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>average_mtd</th>\n",
       "      <th>classifier</th>\n",
       "      <th>f1</th>\n",
       "      <th>input_text</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>tokenizer</th>\n",
       "      <th>vect_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>0.713097</td>\n",
       "      <td>weighted</td>\n",
       "      <td>ComplementNB(alpha=1.0, class_prior=None, fit_...</td>\n",
       "      <td>0.697706</td>\n",
       "      <td>mission_prgrm_spellchk</td>\n",
       "      <td>0.710380</td>\n",
       "      <td>0.713097</td>\n",
       "      <td>lemma_tokenizer</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>0.713097</td>\n",
       "      <td>weighted</td>\n",
       "      <td>ComplementNB(alpha=1.0, class_prior=None, fit_...</td>\n",
       "      <td>0.697509</td>\n",
       "      <td>mission_prgrm</td>\n",
       "      <td>0.709199</td>\n",
       "      <td>0.713097</td>\n",
       "      <td>lemma_tokenizer</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>0.713097</td>\n",
       "      <td>macro</td>\n",
       "      <td>ComplementNB(alpha=1.0, class_prior=None, fit_...</td>\n",
       "      <td>0.548333</td>\n",
       "      <td>mission_prgrm_spellchk</td>\n",
       "      <td>0.664668</td>\n",
       "      <td>0.527116</td>\n",
       "      <td>lemma_tokenizer</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>0.713097</td>\n",
       "      <td>macro</td>\n",
       "      <td>ComplementNB(alpha=1.0, class_prior=None, fit_...</td>\n",
       "      <td>0.548009</td>\n",
       "      <td>mission_prgrm</td>\n",
       "      <td>0.654455</td>\n",
       "      <td>0.524998</td>\n",
       "      <td>lemma_tokenizer</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0.710960</td>\n",
       "      <td>weighted</td>\n",
       "      <td>ComplementNB(alpha=1.0, class_prior=None, fit_...</td>\n",
       "      <td>0.695338</td>\n",
       "      <td>mission_prgrm</td>\n",
       "      <td>0.707266</td>\n",
       "      <td>0.710960</td>\n",
       "      <td>porter_tokenizer</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>0.710960</td>\n",
       "      <td>macro</td>\n",
       "      <td>ComplementNB(alpha=1.0, class_prior=None, fit_...</td>\n",
       "      <td>0.545275</td>\n",
       "      <td>mission_prgrm</td>\n",
       "      <td>0.655212</td>\n",
       "      <td>0.522680</td>\n",
       "      <td>porter_tokenizer</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>0.710733</td>\n",
       "      <td>weighted</td>\n",
       "      <td>ComplementNB(alpha=1.0, class_prior=None, fit_...</td>\n",
       "      <td>0.695284</td>\n",
       "      <td>mission_prgrm_spellchk</td>\n",
       "      <td>0.707968</td>\n",
       "      <td>0.710733</td>\n",
       "      <td>porter_tokenizer</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>0.710733</td>\n",
       "      <td>macro</td>\n",
       "      <td>ComplementNB(alpha=1.0, class_prior=None, fit_...</td>\n",
       "      <td>0.545764</td>\n",
       "      <td>mission_prgrm_spellchk</td>\n",
       "      <td>0.664868</td>\n",
       "      <td>0.525096</td>\n",
       "      <td>porter_tokenizer</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>0.700372</td>\n",
       "      <td>weighted</td>\n",
       "      <td>ComplementNB(alpha=1.0, class_prior=None, fit_...</td>\n",
       "      <td>0.685512</td>\n",
       "      <td>mission_prgrm</td>\n",
       "      <td>0.705112</td>\n",
       "      <td>0.700372</td>\n",
       "      <td>lemma_tokenizer</td>\n",
       "      <td>count</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>0.700372</td>\n",
       "      <td>macro</td>\n",
       "      <td>ComplementNB(alpha=1.0, class_prior=None, fit_...</td>\n",
       "      <td>0.537670</td>\n",
       "      <td>mission_prgrm</td>\n",
       "      <td>0.644781</td>\n",
       "      <td>0.516050</td>\n",
       "      <td>lemma_tokenizer</td>\n",
       "      <td>count</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     accuracy average_mtd                                         classifier  \\\n",
       "139  0.713097    weighted  ComplementNB(alpha=1.0, class_prior=None, fit_...   \n",
       "91   0.713097    weighted  ComplementNB(alpha=1.0, class_prior=None, fit_...   \n",
       "138  0.713097       macro  ComplementNB(alpha=1.0, class_prior=None, fit_...   \n",
       "90   0.713097       macro  ComplementNB(alpha=1.0, class_prior=None, fit_...   \n",
       "95   0.710960    weighted  ComplementNB(alpha=1.0, class_prior=None, fit_...   \n",
       "94   0.710960       macro  ComplementNB(alpha=1.0, class_prior=None, fit_...   \n",
       "143  0.710733    weighted  ComplementNB(alpha=1.0, class_prior=None, fit_...   \n",
       "142  0.710733       macro  ComplementNB(alpha=1.0, class_prior=None, fit_...   \n",
       "89   0.700372    weighted  ComplementNB(alpha=1.0, class_prior=None, fit_...   \n",
       "88   0.700372       macro  ComplementNB(alpha=1.0, class_prior=None, fit_...   \n",
       "\n",
       "           f1              input_text  precision    recall         tokenizer  \\\n",
       "139  0.697706  mission_prgrm_spellchk   0.710380  0.713097   lemma_tokenizer   \n",
       "91   0.697509           mission_prgrm   0.709199  0.713097   lemma_tokenizer   \n",
       "138  0.548333  mission_prgrm_spellchk   0.664668  0.527116   lemma_tokenizer   \n",
       "90   0.548009           mission_prgrm   0.654455  0.524998   lemma_tokenizer   \n",
       "95   0.695338           mission_prgrm   0.707266  0.710960  porter_tokenizer   \n",
       "94   0.545275           mission_prgrm   0.655212  0.522680  porter_tokenizer   \n",
       "143  0.695284  mission_prgrm_spellchk   0.707968  0.710733  porter_tokenizer   \n",
       "142  0.545764  mission_prgrm_spellchk   0.664868  0.525096  porter_tokenizer   \n",
       "89   0.685512           mission_prgrm   0.705112  0.700372   lemma_tokenizer   \n",
       "88   0.537670           mission_prgrm   0.644781  0.516050   lemma_tokenizer   \n",
       "\n",
       "    vect_type  \n",
       "139     tfidf  \n",
       "91      tfidf  \n",
       "138     tfidf  \n",
       "90      tfidf  \n",
       "95      tfidf  \n",
       "94      tfidf  \n",
       "143     tfidf  \n",
       "142     tfidf  \n",
       "89      count  \n",
       "88      count  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_performance=pd.concat([df_performance_rf, df_performance_nb], ignore_index=True).sort_values(['accuracy', 'f1'], ascending=False)\n",
    "df_performance[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define parameters and datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters.\n",
    "input_text='mission_prgrm_spellchk'\n",
    "classifier=naive_bayes.ComplementNB()\n",
    "tokenizer='lemma'\n",
    "vect_type='tfidf'\n",
    "average_mtd='macro'\n",
    "\n",
    "df_universal_test=pd.read_pickle('../../dataset/df_ntee_universal/test/df_ntee_universal_test.pkl.gz', compression='gzip')\n",
    "df_universal_test['mission_prgrm_spellchk']=df_universal_test['mission_spellchk']+' '+df_universal_test['prgrm_dsc_spellchk'] # Using spell-checked."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ComplementNB(alpha=1.0, class_prior=None, fit_prior=True, norm=False)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##########################################################\n",
    "################ Prepare dataframe for ML ################\n",
    "#### Sample ####\n",
    "# Build training and testing data frame.\n",
    "x_train=trainDF[input_text]\n",
    "y_train=trainDF['NTEE1']\n",
    "################ Prepare dataframe for ML ################\n",
    "##########################################################\n",
    "\n",
    "##########################################################\n",
    "################ Define tokenizer ################\n",
    "\n",
    "def porter_tokenizer(str_input): # '''Pay attention to the input: this is string input, not token!'''\n",
    "    tokens = word_tokenize(str_input)\n",
    "    return [PorterStemmer().stem(token) for token in tokens]\n",
    "\n",
    "# Lemmatize using POS tags, assume to improve accuracy.\n",
    "# Ref: \n",
    "#   - https://stackoverflow.com/questions/15586721/wordnet-lemmatization-and-pos-tagging-in-python\n",
    "#   - https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "def lemma_tokenizer(str_input): # '''Pay attention to the input: this is string input, not token!'''\n",
    "    tokens=word_tokenize(str_input)\n",
    "    return [WordNetLemmatizer().lemmatize(word=word, pos=get_wordnet_pos(pos)) for word, pos in nltk.pos_tag(tokens)]\n",
    "\n",
    "if tokenizer=='lemma':\n",
    "    tokenizer=lemma_tokenizer\n",
    "elif tokenizer=='porter':\n",
    "    tokenizer=porter_tokenizer\n",
    "################ Define tokenizer ################\n",
    "##########################################################\n",
    "\n",
    "##########################################################\n",
    "######### Text Vectorization and Transformation ##########\n",
    "# 1. Use Porter Stemmer.\n",
    "# 2. Use word level, character level does not make sense for current situation.\n",
    "# 3. Use count (freq) and tf-idf vectorizer. see: \n",
    "# Bengfort, B., Bilbro, R., & Ojeda, T. (2018). Applied Text Analysis with Python: Enabling Language-Aware Data Products with Machine Learning (1 edition). Beijing Boston Farnham Sebastopol Tokyo: O’Reilly Media.\n",
    "# Page: 67.\n",
    "\n",
    "if vect_type=='count':\n",
    "    ##### Token counts #####\n",
    "    # create the transform\n",
    "    vectorizer = CountVectorizer(stop_words='english', \n",
    "                                 tokenizer=tokenizer, \n",
    "                                 analyzer='word'\n",
    "                                )\n",
    "    # tokenize and build vocab.\n",
    "    vectorizer.fit(x_train) # Using training dataset to build vocabulary.\n",
    "    # Encode document: transform the training and validation data using count vectorizer object\n",
    "    x_train_vect =  vectorizer.transform(x_train)\n",
    "elif vect_type=='tfidf':\n",
    "    ##### TF-IDF #####\n",
    "    # create the transform\n",
    "    vectorizer = TfidfVectorizer(stop_words='english', \n",
    "                                 tokenizer=tokenizer, \n",
    "                                 analyzer='word'\n",
    "                                )\n",
    "    # tokenize and build vocab\n",
    "    vectorizer.fit(x_train) # Using training dataset to build vocabulary.\n",
    "    # Encode document: transform the training and validation data using tfidf vectorizer object\n",
    "    x_train_vect =  vectorizer.transform(x_train)\n",
    "######### Text Vectorization and Transformation ##########\n",
    "##########################################################\n",
    "\n",
    "classifier.fit(x_train_vect, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test trained model on Universal Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################\n",
    "################ Prepare dataframe for ML ################\n",
    "#### Sample ####\n",
    "# Build training and testing data frame.\n",
    "x_valid=df_universal_test[input_text]\n",
    "y_valid=df_universal_test['NTEE1']\n",
    "################ Prepare dataframe for ML ################\n",
    "##########################################################\n",
    "\n",
    "##########################################################\n",
    "################ Define tokenizer ################\n",
    "\n",
    "def porter_tokenizer(str_input): # '''Pay attention to the input: this is string input, not token!'''\n",
    "    tokens = word_tokenize(str_input)\n",
    "    return [PorterStemmer().stem(token) for token in tokens]\n",
    "\n",
    "# Lemmatize using POS tags, assume to improve accuracy.\n",
    "# Ref: \n",
    "#   - https://stackoverflow.com/questions/15586721/wordnet-lemmatization-and-pos-tagging-in-python\n",
    "#   - https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "def lemma_tokenizer(str_input): # '''Pay attention to the input: this is string input, not token!'''\n",
    "    tokens=word_tokenize(str_input)\n",
    "    return [WordNetLemmatizer().lemmatize(word=word, pos=get_wordnet_pos(pos)) for word, pos in nltk.pos_tag(tokens)]\n",
    "\n",
    "if tokenizer=='lemma':\n",
    "    tokenizer=lemma_tokenizer\n",
    "elif tokenizer=='porter':\n",
    "    tokenizer=porter_tokenizer\n",
    "################ Define tokenizer ################\n",
    "##########################################################\n",
    "\n",
    "##########################################################\n",
    "######### Text Vectorization and Transformation ##########\n",
    "# 1. Use Porter Stemmer.\n",
    "# 2. Use word level, character level does not make sense for current situation.\n",
    "# 3. Use count (freq) and tf-idf vectorizer. see: \n",
    "# Bengfort, B., Bilbro, R., & Ojeda, T. (2018). Applied Text Analysis with Python: Enabling Language-Aware Data Products with Machine Learning (1 edition). Beijing Boston Farnham Sebastopol Tokyo: O’Reilly Media.\n",
    "# Page: 67.\n",
    "\n",
    "if vect_type=='count':\n",
    "    ##### Token counts #####\n",
    "    # create the transform\n",
    "    vectorizer = CountVectorizer(stop_words='english', \n",
    "                                 tokenizer=tokenizer, \n",
    "                                 analyzer='word'\n",
    "                                )\n",
    "    # tokenize and build vocab.\n",
    "    vectorizer.fit(x_train)\n",
    "    # Encode document: transform the training and validation data using count vectorizer object\n",
    "    x_valid_vect =  vectorizer.transform(x_valid)\n",
    "elif vect_type=='tfidf':\n",
    "    ##### TF-IDF #####\n",
    "    # create the transform\n",
    "    vectorizer = TfidfVectorizer(stop_words='english', \n",
    "                                 tokenizer=tokenizer, \n",
    "                                 analyzer='word'\n",
    "                                )\n",
    "    # tokenize and build vocab\n",
    "    vectorizer.fit(x_train)\n",
    "    # Encode document: transform the training and validation data using tfidf vectorizer object\n",
    "    x_valid_vect =  vectorizer.transform(x_valid)\n",
    "######### Text Vectorization and Transformation ##########\n",
    "##########################################################\n",
    "\n",
    "predictions = classifier.predict(x_valid_vect)\n",
    "performance_dict= {'input_text':input_text,\n",
    "                   'classifier':str(classifier), \n",
    "                   'tokenizer':tokenizer.__name__, \n",
    "                   'vect_type':vect_type, \n",
    "                   'average_mtd':average_mtd,\n",
    "                   'accuracy':metrics.accuracy_score(predictions, y_valid), \n",
    "                   'precision':metrics.precision_score(y_pred=predictions, y_true=y_valid, average=average_mtd),\n",
    "                   'recall':metrics.recall_score(y_pred=predictions, y_true=y_valid, average=average_mtd),\n",
    "                   'f1':metrics.f1_score(y_pred=predictions, y_true=y_valid, average=average_mtd)\n",
    "                  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_text': 'mission_prgrm_spellchk',\n",
       " 'classifier': 'ComplementNB(alpha=1.0, class_prior=None, fit_prior=True, norm=False)',\n",
       " 'tokenizer': 'lemma_tokenizer',\n",
       " 'vect_type': 'tfidf',\n",
       " 'average_mtd': 'macro',\n",
       " 'accuracy': 0.716113658144896,\n",
       " 'precision': 0.659588192865402,\n",
       " 'recall': 0.5311606825339955,\n",
       " 'f1': 0.5511202901330786}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "performance_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>L</td>\n",
       "      <td>P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>S</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>G</td>\n",
       "      <td>G</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>E</td>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>A</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>S</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>I</td>\n",
       "      <td>I</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>X</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>A</td>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>S</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>S</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>S</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>S</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>D</td>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>L</td>\n",
       "      <td>L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38577</th>\n",
       "      <td>B</td>\n",
       "      <td>V</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38578</th>\n",
       "      <td>N</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38579</th>\n",
       "      <td>X</td>\n",
       "      <td>X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38580</th>\n",
       "      <td>S</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38581</th>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38582</th>\n",
       "      <td>S</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38583</th>\n",
       "      <td>E</td>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38584</th>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38585</th>\n",
       "      <td>E</td>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38586</th>\n",
       "      <td>S</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38587</th>\n",
       "      <td>S</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38588</th>\n",
       "      <td>C</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38589</th>\n",
       "      <td>W</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38590</th>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38591</th>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38592</th>\n",
       "      <td>S</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38593</th>\n",
       "      <td>P</td>\n",
       "      <td>L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38594</th>\n",
       "      <td>B</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38595</th>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38596</th>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38597</th>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38598</th>\n",
       "      <td>D</td>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38599</th>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38600</th>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38601</th>\n",
       "      <td>P</td>\n",
       "      <td>P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38602</th>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38603</th>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38604</th>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38605</th>\n",
       "      <td>L</td>\n",
       "      <td>P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38606</th>\n",
       "      <td>E</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>38607 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0  1\n",
       "0      L  P\n",
       "1      A  A\n",
       "2      N  N\n",
       "3      A  A\n",
       "4      S  Y\n",
       "5      A  A\n",
       "6      G  G\n",
       "7      E  E\n",
       "8      A  C\n",
       "9      S  S\n",
       "10     N  N\n",
       "11     I  I\n",
       "12     X  A\n",
       "13     A  E\n",
       "14     Y  Y\n",
       "15     S  S\n",
       "16     A  A\n",
       "17     B  B\n",
       "18     S  S\n",
       "19     S  N\n",
       "20     S  S\n",
       "21     A  B\n",
       "22     A  A\n",
       "23     A  A\n",
       "24     B  B\n",
       "25     Y  Y\n",
       "26     D  D\n",
       "27     A  A\n",
       "28     C  C\n",
       "29     L  L\n",
       "...   .. ..\n",
       "38577  B  V\n",
       "38578  N  S\n",
       "38579  X  X\n",
       "38580  S  S\n",
       "38581  B  B\n",
       "38582  S  S\n",
       "38583  E  E\n",
       "38584  B  B\n",
       "38585  E  E\n",
       "38586  S  S\n",
       "38587  S  S\n",
       "38588  C  N\n",
       "38589  W  C\n",
       "38590  B  B\n",
       "38591  A  A\n",
       "38592  S  S\n",
       "38593  P  L\n",
       "38594  B  S\n",
       "38595  A  A\n",
       "38596  B  B\n",
       "38597  N  N\n",
       "38598  D  D\n",
       "38599  M  M\n",
       "38600  N  N\n",
       "38601  P  P\n",
       "38602  N  N\n",
       "38603  A  B\n",
       "38604  A  B\n",
       "38605  L  P\n",
       "38606  E  F\n",
       "\n",
       "[38607 rows x 2 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame([predictions, y_valid]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'PUBLIC EDUCATION AND ENTERTAINMENT PREPARING FOR AN ANNUAL OPERA PERFORMANCE AND SCHOOL EDUCATIONAL PROGRAMS THROUGHOUT THE YEAR .'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_valid.iloc[85]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x125816 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 12 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_valid_vect[85]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "t=x_valid_vect[32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.18100557, 0.22802972, 0.29788442, 0.23999539, 0.19132063,\n",
       "       0.15383652, 0.05427941, 0.11782835, 0.19442201, 0.16970152,\n",
       "       0.20433292, 0.34350801, 0.17996997, 0.33130379, 0.15262076,\n",
       "       0.14241593, 0.13472251, 0.2148029 , 0.25939262, 0.18852346,\n",
       "       0.1310445 , 0.17414271, 0.10713031, 0.19326427, 0.08294151,\n",
       "       0.09700927])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['E'], dtype='<U1')"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.predict(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'E'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_valid.iloc[32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38607"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_universal_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size=round(0.2*len(df_universal_test))\n",
    "df_universal_test_sample=df_universal_test.sample(sample_size, weights=random.choices(range(1, 10000), k=len(df_universal_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7721"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(random.choices(range(1, 100), k=sample_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NTEE1\n",
       "A    0.109830\n",
       "B    0.164616\n",
       "C    0.020334\n",
       "D    0.030307\n",
       "E    0.060355\n",
       "F    0.014765\n",
       "G    0.033156\n",
       "H    0.003108\n",
       "I    0.018003\n",
       "J    0.031084\n",
       "K    0.010750\n",
       "L    0.041186\n",
       "M    0.030696\n",
       "N    0.102707\n",
       "O    0.011786\n",
       "P    0.059060\n",
       "Q    0.011009\n",
       "R    0.008548\n",
       "S    0.089237\n",
       "T    0.015024\n",
       "U    0.006994\n",
       "V    0.001684\n",
       "W    0.053879\n",
       "X    0.028105\n",
       "Y    0.043777\n",
       "Name: EIN, dtype: float64"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_universal_test_sample.groupby('NTEE1').count()['EIN']/len(df_universal_test_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NTEE1\n",
       "A    0.111146\n",
       "B    0.166265\n",
       "C    0.021421\n",
       "D    0.026783\n",
       "E    0.059756\n",
       "F    0.014065\n",
       "G    0.035045\n",
       "H    0.003264\n",
       "I    0.019168\n",
       "J    0.029321\n",
       "K    0.013521\n",
       "L    0.039811\n",
       "M    0.029528\n",
       "N    0.101666\n",
       "O    0.010594\n",
       "P    0.060041\n",
       "Q    0.011293\n",
       "R    0.006657\n",
       "S    0.093325\n",
       "T    0.014013\n",
       "U    0.005828\n",
       "V    0.002202\n",
       "W    0.052788\n",
       "X    0.028440\n",
       "Y    0.044059\n",
       "Name: EIN, dtype: float64"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_universal_test.groupby('NTEE1').count()['EIN']/len(df_universal_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc-autonumbering": false,
  "toc-showcode": true,
  "toc-showmarkdowntxt": true,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
