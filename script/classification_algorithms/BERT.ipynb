{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertConfig\n",
    "from pytorch_pretrained_bert import BertAdam, BertForSequenceClassification\n",
    "from tqdm import tqdm, trange\n",
    "import pandas as pd\n",
    "import io\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quadro RTX 6000\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# n_gpu = torch.cuda.device_count()\n",
    "print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file_path='../../dataset/UCF/test/'\n",
    "df_UCF_train=pd.concat([pd.read_pickle(train_file_path+file, compression='gzip') for file in os.listdir(train_file_path)], ignore_index=True)\n",
    "df_UCF_train['input']= df_UCF_train['TAXPAYER_NAME']+' '+df_UCF_train['mission_spellchk']+' '+df_UCF_train['prgrm_dsc_spellchk']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sentence and label lists\n",
    "sentences = df_UCF_train.input.values\n",
    "\n",
    "# We need to add special tokens at the beginning and end of each sentence for BERT to work properly\n",
    "sentences = [\"[CLS] \" + sentence + \" [SEP]\" for sentence in sentences]\n",
    "labels = preprocessing.LabelEncoder().fit_transform(df_UCF_train.NTEE1.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenize the first sentence:\n",
      "['[CLS]', 'green', '##croft', 'go', '##sh', '##en', 'inc', 'green', '##croft', 'go', '##sden', 'provides', 'active', ',', 'affordable', 'retirement', 'living', 'with', 'supportive', 'services', ',', 'assisted', 'living', ',', 'and', 'skilled', 'nursing', 'care', '.', 'charity', 'care', 'for', 'eye', 'june', '30', ',', '2014', 'amounted', 'to', '$', '4', '##e', '##27', '##9', '##39', '##22', '.', 'nursing', 'services', '-', 'green', '##croft', 'go', '##sden', 'is', 'health', 'facilities', 'provide', 'state', '-', 'of', '-', 'the', '-', 'art', 'health', 'care', 'techniques', 'and', 'technology', 'in', 'an', 'inviting', 'and', 'supportive', 'residential', 'environment', '.', 'our', 'setting', 'is', 'appropriate', 'for', 'individuals', 'who', 'are', 'recovering', 'from', 'surgery', 'or', 'illness', ',', 'need', 'rehabilitation', ',', 'or', 'require', 'ongoing', 'long', '-', 'term', 'nursing', 'care', '.', 'our', 'rest', '##ora', '##tive', 'care', 'philosophy', 'reinforce', '##s', 'the', 'healing', 'and', 'rehabilitation', 'process', 'and', 'promotes', 'independence', '.', 'a', 'full', 'range', 'of', 'activities', ',', 'chaplain', '##cy', ',', 'and', 'social', 'services', 'are', 'provided', '.', 'we', 'provide', 'out', '##patient', 'physical', ',', 'occupational', ',', 'and', 'speech', '/', 'language', 'the', '##ra', '##pies', 'for', 'all', 'ages', '.', 'we', 'emphasize', 'education', 'and', 'communication', 'with', 'you', ',', 'your', 'family', ',', 'and', 'referring', 'physicians', '.', 'residents', 'who', 'do', 'not', 'mis', '##use', 'their', 'assets', 'may', 'be', 'eligible', 'for', 'support', 'from', 'green', '##croft', 'communities', 'foundation', 'if', 'their', 'assets', 'run', 'out', '.', 'during', 'the', 'year', 'ended', 'june', '30', ',', '2014', ',', 'over', '21', '%', 'of', 'total', 'revenues', 'were', 'used', 'to', 'provide', 'community', 'benefits', ',', 'which', 'includes', 'the', 'following', 'amounts', ':', 'ind', '##igen', '##t', 'care', '/', 'charity', 'care', '-', '$', '4', '##e', '##27', '##9', '##39', '##22', ',', 'reduced', '/', 'free', 'care', '/', 'use', 'of', 'facilities', '-', '$', '48', '##9', '##35', '##6', '##9', ',', 'medicare', 'write', '-', 'offs', '-', '$', '1200', '##3', '.', '490', '.', ';', 'dietary', 'and', 'other', 'services', '-', 'the', 'organization', 'provides', 'dietary', 'and', 'other', 'miscellaneous', 'program', 'services', 'to', 'its', 'residents', '.', ';', 'resident', 'services', '-', 'green', '##croft', 'go', '##sden', '(', 'g', '##g', ')', 'is', 'a', 'non', '-', 'profit', 'community', 'benefit', 'corporation', 'that', 'provides', 'continuing', 'care', 'retirement', 'community', '(', 'ic', '##rc', ')', 'services', 'to', '800', 'residents', 'on', 'a', '167', '-', 'acre', 'campus', '.', 'g', '##g', 'provides', 'ic', '##rc', 'services', 'including', 'independent', 'living', ',', 'assisted', 'living', ',', 'skilled', 'nursing', ',', 'adult', 'day', 'programs', ',', 'senior', 'center', 'programs', ',', 'and', 'home', 'care', 'services', '.', 'evergreen', 'place', 'provides', 'g', '##g', 'residents', 'with', 'a', 'vital', 'option', 'in', 'the', 'continuum', 'of', 'care', 'if', 'and', 'when', 'they', 'should', 'need', 'assistance', 'services', '.', 'assisted', 'living', 'va', '##can', '##cies', 'not', 'filled', 'by', 'g', '##g', 'are', 'available', 'to', 'older', 'adults', 'in', 'the', 'wider', 'community', '.', 'we', 'partner', 'with', 'residents', ',', 'their', 'families', ',', 'and', 'others', 'to', 'create', 'a', 'community', 'where', 'older', 'adults', 'enjoy', 'life', '.', 'such', 'partnership', 'is', 'essential', 'to', 'appropriately', 'meet', 'the', 'needs', 'of', 'a', 'rapidly', 'growing', 'older', 'adult', 'population', '.', 'residents', 'who', 'do', 'not', 'mis', '##use', 'their', 'assets', 'are', 'eligible', 'for', 'support', 'from', 'the', 'green', '##croft', 'community', 'foundation', 'if', 'their', 'assets', 'run', 'out', '.', 'g', '##g', 'is', 'in', 'an', 'active', 'partnership', 'with', 'go', '##sden', 'college', 'to', 'provide', 'lifelong', 'learning', 'opportunities', 'to', 'people', 'over', 'the', 'age', 'of', '55', 'in', 'elk', '##hart', 'county', '.', 'additionally', ',', 'g', '##g', 'provides', 'numerous', 'programs', 'and', 'activities', 'for', 'seniors', 'on', 'and', 'off', 'our', 'campus', '.', 'g', '##g', 'is', 'affiliated', 'with', 'three', 'hu', '##d', 'housing', 'organizations', 'on', 'its', 'campus', 'that', 'are', 'integrated', 'into', 'g', '##g', 'is', 'services', '.', 'these', 'organizations', 'serve', '325', 'residents', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "\n",
    "tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n",
    "print (\"Tokenize the first sentence:\")\n",
    "print (tokenized_texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (533 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (572 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (752 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (998 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (566 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (817 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (760 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (618 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1673 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1428 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (679 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1014 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (564 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1947 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (614 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (643 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1177 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (742 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (792 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (515 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (791 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (554 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (962 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (610 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (634 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1642 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (668 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (654 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (593 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1164 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1378 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1221 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (528 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (666 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (566 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (573 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (630 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (587 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (874 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1762 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (579 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1069 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (939 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (538 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1441 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1498 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (593 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1314 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (906 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (532 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1563 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1237 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (612 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1689 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (636 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (561 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (898 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (749 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (581 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (803 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (895 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1321 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (593 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (517 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1052 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1621 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1455 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1756 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (803 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (515 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (592 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1193 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (535 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (515 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1258 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1005 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (725 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (4019 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (581 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1501 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (4403 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1722 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (876 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (678 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (2897 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1398 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1323 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (654 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (881 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (581 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (638 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1393 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (813 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (798 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (604 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (717 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (708 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (2796 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (813 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (948 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (835 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (794 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (661 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (638 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (5834 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (779 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (551 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1637 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (704 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (554 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (751 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (541 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1293 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (514 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (763 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1325 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (795 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (665 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (614 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (650 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (558 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (5006 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1022 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (988 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (788 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1310 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (601 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (755 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1161 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (758 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (555 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (526 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (647 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1019 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (514 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (606 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (535 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1037 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (593 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (521 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (668 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1768 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (540 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1297 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1598 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (856 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1373 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (604 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (731 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (667 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1463 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (930 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (564 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (648 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (513 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1307 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1363 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (720 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (780 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1334 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (518 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (727 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (796 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (526 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (877 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (614 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (932 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (762 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (903 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (691 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (3380 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (2090 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1675 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (786 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1052 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (676 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (650 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (555 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (568 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (846 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1227 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (820 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1201 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1370 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1000 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (2279 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1709 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (641 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (700 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (555 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (517 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (533 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (710 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (773 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (2258 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (602 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (562 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1497 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (602 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (554 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (596 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1394 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (544 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (848 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (710 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1095 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1138 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1183 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (922 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (639 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (582 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (573 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (3150 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1505 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1247 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (895 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (944 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (714 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (529 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (722 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (681 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (555 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (636 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (641 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (636 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (521 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (2820 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (557 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (539 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (653 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1707 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (582 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (614 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (644 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (531 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (536 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (849 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (545 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (688 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (534 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1416 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (3132 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (581 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (617 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (991 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (3150 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (581 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1284 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (965 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (756 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (690 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1472 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (614 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (981 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (756 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (776 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (513 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (546 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (911 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (992 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1836 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (695 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1631 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (527 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (610 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (519 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1044 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (596 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (867 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (993 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (3546 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (2896 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1463 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (714 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (813 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (627 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (2584 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1867 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (776 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (877 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (768 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (554 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (980 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (693 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (815 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1363 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (541 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (3110 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (839 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (607 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (660 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (2811 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (555 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (641 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (706 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (657 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1292 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (770 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (514 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (2585 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (788 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (619 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (3935 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (589 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (925 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (6187 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (647 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (721 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (546 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (670 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (540 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (2239 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (905 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (627 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1767 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (656 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (581 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (785 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (927 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (552 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (30921 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (605 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (628 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (700 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (737 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (4468 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (537 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (799 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1066 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (563 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (619 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (813 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (594 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (538 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (57254 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (535 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (519 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (705 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (570 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (978 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (716 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (978 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1541 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (650 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1027 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (2956 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (906 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1648 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (3515 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (670 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1401 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1081 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (555 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (759 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1647 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (2585 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1366 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (2998 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1166 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (690 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (725 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (818 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (957 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (515 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (869 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (528 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1803 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1750 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (630 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (781 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (772 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (628 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1932 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1303 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1201 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (555 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (816 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1813 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (549 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (674 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (570 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1262 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (2576 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1187 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (957 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (601 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (701 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (529 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (546 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (572 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (654 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (637 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (572 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (949 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (606 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (533 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (620 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (553 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (610 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (625 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (606 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (614 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1697 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (525 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (535 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (840 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1047 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1504 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (850 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (714 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (768 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (551 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1140 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (905 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (655 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (518 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1095 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (518 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (690 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (745 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (577 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (987 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1694 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (762 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (658 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (982 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (712 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1292 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (561 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (896 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1603 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (710 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (532 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1917 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (583 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (555 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (672 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (696 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (590 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (531 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (798 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (892 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1818 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1000 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1501 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1248 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (769 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (3679 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (613 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (537 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (722 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (2114 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (3823 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (568 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (545 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (5565 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (584 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (863 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (633 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (664 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (629 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1186 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (629 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (815 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (735 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1073 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (649 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (552 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (797 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1215 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1392 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (740 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (565 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (518 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1359 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (683 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (588 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (762 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (566 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (6444 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (538 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (522 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (577 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (905 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (670 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (624 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (552 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (7982 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (2640 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1926 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1644 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1620 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1651 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1004 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (663 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (580 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1252 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (533 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (745 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (577 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (610 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (710 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (4734 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1043 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (660 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (5318 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (677 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (527 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (585 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (734 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1735 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (530 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (613 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (821 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (591 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (884 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (5638 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (706 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (788 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (679 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (610 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1754 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1164 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (640 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1014 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (660 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (2072 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (961 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (594 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (844 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (742 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (518 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (532 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (617 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (975 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (750 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (759 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (630 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1153 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (869 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (559 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (524 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (550 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1737 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (863 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1470 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1341 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (4163 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (590 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1453 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (672 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (620 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1779 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1055 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (656 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1064 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (542 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (826 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1405 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (611 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (519 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (608 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (538 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (917 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (516 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1149 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1173 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (887 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (584 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1559 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (881 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (584 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (842 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1314 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (626 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (513 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1319 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1443 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1634 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (518 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (911 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (531 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (558 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (881 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (853 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (647 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (612 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (784 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1168 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (552 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (979 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (3352 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (660 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (822 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (611 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1352 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (2576 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1230 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (2921 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1051 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (562 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (2001 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1352 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (761 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (670 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (878 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (636 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (993 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1669 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (514 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (585 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (567 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (2021 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (9200 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (572 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1369 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (583 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (519 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (2244 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (551 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (739 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1601 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (597 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (614 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (955 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (612 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (568 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (10361 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (614 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1516 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1276 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (600 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (2605 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1659 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (634 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1808 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (815 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (660 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (2253 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (597 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (761 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (543 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (714 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (608 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (594 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (593 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (537 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1816 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (746 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1233 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (632 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (650 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1985 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (534 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1974 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1133 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (666 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1101 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (983 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (754 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (3043 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (624 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1772 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (778 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (622 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (600 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1148 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (646 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (951 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (580 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (527 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (3715 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (663 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (826 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (810 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (518 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (711 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (519 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1213 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (528 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (2186 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1096 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1437 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (561 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (549 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1304 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (867 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (691 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (790 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1220 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (542 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (519 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1929 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (4132 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (968 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (576 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (583 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (667 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (2311 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (562 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1010 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (582 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1265 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (776 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (2189 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (561 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1025 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (904 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (708 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (901 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (704 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (516 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (702 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (657 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1042 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (662 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (808 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (557 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1530 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1549 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (716 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (2479 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (617 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1555 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1047 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (538 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1574 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (554 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (2867 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (2445 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (629 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (523 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1537 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (947 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1780 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (683 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (883 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1157 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (671 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (986 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (789 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1365 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1134 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1613 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (950 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (2262 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (719 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1043 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1967 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1394 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (709 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (521 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (578 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (695 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (521 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (674 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (973 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1572 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (666 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (727 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1301 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (932 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (584 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1088 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (2393 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (719 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (709 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1556 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (614 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (2183 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (2407 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (521 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (879 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1138 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (602 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (663 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (601 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (880 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (746 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (3516 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (535 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (629 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (632 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (6872 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (3361 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (590 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (534 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (690 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (556 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (565 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (761 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1183 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (562 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (671 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (517 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1055 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (2036 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (557 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (628 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (762 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (530 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1764 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (3185 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1635 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (581 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1052 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (711 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (893 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (796 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (611 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (587 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (609 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (709 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (593 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (915 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (753 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (728 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (743 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (616 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (805 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (741 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (575 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (847 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (960 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (999 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (678 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (575 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (520 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (539 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1786 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1820 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (540 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (566 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (679 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1307 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (569 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (631 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (586 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (5280 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (861 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (886 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (535 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (657 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (2464 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1246 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (639 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1828 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (519 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (740 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (588 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (739 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (2316 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1333 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (527 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (742 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1390 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1532 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1559 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (625 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1127 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1589 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1498 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1201 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (615 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (533 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (756 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1724 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (617 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (550 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (736 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (522 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (646 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (969 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (661 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (699 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (704 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (663 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1096 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1034 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (587 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (568 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1884 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (651 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (619 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1164 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (2364 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (517 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (628 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (4003 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (650 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (548 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (704 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1604 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1673 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (601 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (803 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (2123 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (607 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (587 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (690 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (554 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (2381 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (788 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (947 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1543 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (3065 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (879 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (700 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (797 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (721 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (632 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (2138 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (782 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (599 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (719 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (563 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1675 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (5617 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1016 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1652 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (689 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (575 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (697 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (928 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (733 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (759 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (2032 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (647 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1720 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (783 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (513 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (798 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1056 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1018 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (2315 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (561 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (582 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (957 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1500 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (905 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (868 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (696 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1305 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (768 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1055 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (794 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (967 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1051 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (815 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (669 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1244 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (543 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (699 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (955 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (994 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (580 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (525 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (738 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (636 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (970 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (573 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1065 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1093 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1108 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (702 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (600 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1824 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (514 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (638 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1002 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (855 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1107 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (854 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (929 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1525 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (522 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (563 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (741 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (768 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1866 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (590 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1494 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (622 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (773 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (869 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (3264 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (850 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (532 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (591 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (718 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (7068 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (518 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (916 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (588 > 512). Running this sequence through BERT will result in indexing errors\n",
      "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (599 > 512). Running this sequence through BERT will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "# Set the maximum sequence length. The longest sequence in our training set is 47, but we'll leave room on the end anyway. \n",
    "# In the original paper, the authors used a length of 512.\n",
    "# MAX_LEN = np.max([len(tokens) for tokens in tokenized_texts])\n",
    "MAX_LEN = 128\n",
    "\n",
    "# Use the BERT tokenizer to convert the tokens to their index numbers in the BERT vocabulary\n",
    "input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
    "\n",
    "# Pad our input tokens\n",
    "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'fairbanks', 'concert', 'association', 'to', 'present', ',', 'promote', ',', 'and', 'sponsor', 'artistic', '##ally', 'and', 'culturally', 'diverse', 'performing', 'arts', 'events', 'and', 'educational', 'opportunities', 'of', 'the', 'highest', 'quality', 'from', 'stages', 'around', 'the', 'world', '.', 'the', 'fairbanks', 'concert', 'association', 'proudly', 'presented', '9', 'guest', 'artists', ',', '10', 'concerts', ',', '4', 'school', 'performances', ',', '2', 'workshops', ',', '5', 'rural', 'outreach', 'performances', ',', 'reside', '##ncies', 'at', 'west', 'valley', 'high', 'school', 'and', 'the', 'fairbanks', 'youth', 'facility', ',', 'performances', 'for', 'the', 'pioneer', 'home', 'and', 'the', 'den', '##ali', 'center', ',', '5', 'performances', 'on', 'ku', '##an', 'is', 'alaska', 'live', '-', 'in', 'all', 'more', 'than', '30', 'events', 'this', 'season', '.', 'it', 'was', 'another', 'wonderful', 'season', 'from', 'an', 'artistic', 'stand', '##point', 'and', 'very', 'well', 'received', ',', 'exceeding', 'our', 'budget', 'projections', 'for', 'both', 'subscription', '##s', 'and', 'membership', '.', 'in', 'all', ',', 'our', 'guest', 'artists', 'entertained', 'a', 'total', 'of', '76', '##6', '##24', 'performance', 'attendees', ',', 'with', 'another', '113', '##6', '##8', 'benefit', '##ing', 'from', 'our', 'outreach', 'activities', ',', '49', '##40', '##2', 'through', 'education', '.', 'a', 'total', 'of', '139', '##39', '##4', 'people', 'benefit', '##ing', 'from', 'our', 'activities', 'this', 'year', '.', '[SEP]'] 38607\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_texts[1], len(tokenized_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  101, 24502,  4164,  2523,  2000,  2556,  1010,  5326,  1010,\n",
       "        1998, 10460,  6018,  3973,  1998, 20547,  7578,  4488,  2840,\n",
       "        2824,  1998,  4547,  6695,  1997,  1996,  3284,  3737,  2013,\n",
       "        5711,  2105,  1996,  2088,  1012,  1996, 24502,  4164,  2523,\n",
       "       18067,  3591,  1023,  4113,  3324,  1010,  2184,  6759,  1010,\n",
       "        1018,  2082,  4616,  1010,  1016,  9656,  1010,  1019,  3541,\n",
       "       15641,  4616,  1010, 13960, 14767,  2012,  2225,  3028,  2152,\n",
       "        2082,  1998,  1996, 24502,  3360,  4322,  1010,  4616,  2005,\n",
       "        1996,  7156,  2188,  1998,  1996,  7939, 11475,  2415,  1010,\n",
       "        1019,  4616,  2006, 13970,  2319,  2003,  7397,  2444,  1011,\n",
       "        1999,  2035,  2062,  2084,  2382,  2824,  2023,  2161,  1012,\n",
       "        2009,  2001,  2178,  6919,  2161,  2013,  2019,  6018,  3233,\n",
       "        8400,  1998,  2200,  2092,  2363,  1010, 17003,  2256,  5166,\n",
       "       21796,  2005,  2119, 15002,  2015,  1998,  5779,  1012,  1999,\n",
       "        2035,  1010])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create attention masks\n",
    "attention_masks = []\n",
    "\n",
    "# Create a mask of 1s for each token followed by 0s for padding\n",
    "for seq in input_ids:\n",
    "    seq_mask = [float(i>0) for i in seq]\n",
    "    attention_masks.append(seq_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Use train_test_split to split our data into train and validation sets for training\n",
    "# train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels, random_state=2018, test_size=0.2)\n",
    "# train_masks, validation_masks, _, _ = train_test_split(attention_masks, input_ids, random_state=2018, test_size=0.2)\n",
    "\n",
    "# Use train_test_split to split our data into train and validation sets for training\n",
    "validation_inputs, validation_labels = [input_ids, labels]\n",
    "validation_masks = attention_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(38607, 38607, 38607)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(validation_inputs), len(validation_labels), len(validation_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all of our data into torch tensors, the required datatype for our model\n",
    "# train_inputs = torch.tensor(train_inputs)\n",
    "# train_labels = torch.tensor(train_labels)\n",
    "# train_masks = torch.tensor(train_masks)\n",
    "\n",
    "validation_inputs = torch.tensor(validation_inputs)\n",
    "validation_labels = torch.tensor(validation_labels)\n",
    "validation_masks = torch.tensor(validation_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a batch size for training. For fine-tuning BERT on a specific task, the authors recommend a batch size of 16 or 32\n",
    "batch_size = 32\n",
    "\n",
    "# Create an iterator of our data with torch DataLoader. This helps save on memory during training because, unlike a for loop, \n",
    "# with an iterator the entire dataset does not need to be loaded into memory\n",
    "\n",
    "# train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "# train_sampler = RandomSampler(train_data)\n",
    "# train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
    "validation_sampler = SequentialSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((tensor([  101, 15775, 24860,  4212,  4547,  3192,  2000,  5770,  1996, 15775,\n",
       "          24860,  4212, 11819,  2098,  2082,  2212,  2000,  3073,  2490,  2005,\n",
       "           3454,  1998,  3450,  1999,  1996, 15775, 24860,  4212, 11819,  2098,\n",
       "           2082,  2212,  3946,  2000,  1996, 15775, 24860,  4212,  3502,  2604,\n",
       "           1002,  3486, 17914,  1998,  1002,  1015,  2063,  8889,  2692,  2000,\n",
       "          15775, 24860,  4212,  5981,  2252,  1025,  5857, 11095,  2000, 15775,\n",
       "          24860,  4212, 11819,  2098,  2082,  2212,  2005,  1996,  2311,  1997,\n",
       "           1996,  8144,  2415,  2006,  1996, 15775, 24860,  4212,  2152,  2082,\n",
       "           3200,  1025,  3946,  2000, 12785,  2495,  3836,  6566,  2005, 12785,\n",
       "           2493,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0]),\n",
       "  tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0.]),\n",
       "  tensor(1)),\n",
       " 30885)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[5], len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 1024, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 1024)\n",
       "      (token_type_embeddings): Embedding(2, 1024)\n",
       "      (LayerNorm): BertLayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (12): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (13): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (14): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (15): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (16): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (17): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (18): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (19): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (20): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (21): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (22): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (23): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=1024, out_features=25, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load BertForSequenceClassification, the pretrained BERT model with a single linear classification layer on top. \n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-large-uncased\", num_labels=len(df_UCF_train.NTEE1.unique()))\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = ['bias', 'gamma', 'beta']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.01},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.0}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t_total value of -1 results in schedule not being applied\n"
     ]
    }
   ],
   "source": [
    "# This variable contains all of the hyperparemeter information our training loop needs\n",
    "optimizer = BertAdam(optimizer_grouped_parameters,\n",
    "                     lr=2e-5,\n",
    "                     warmup=.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the accuracy of our predictions vs labels\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.6066325898500528\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  25%|       | 1/4 [1:06:23<3:19:10, 3983.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.8648744824016563\n",
      "Train loss: 0.4018097531385247\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  50%|     | 2/4 [2:13:01<2:12:55, 3987.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.8520639233954451\n",
      "Train loss: 0.3226928493942476\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  75%|  | 3/4 [3:19:55<1:06:35, 3995.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.8716679606625258\n",
      "Train loss: 0.24706817368357578\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|| 4/4 [4:26:12<00:00, 3993.19s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.8677536231884058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "t = [] \n",
    "# Store our loss and accuracy for plotting\n",
    "train_loss_set = []\n",
    "\n",
    "# Number of training epochs (authors recommend between 2 and 4)\n",
    "epochs = 4\n",
    "\n",
    "# trange is a tqdm wrapper around the normal python range\n",
    "for _ in trange(epochs, desc=\"Epoch\"):\n",
    "    \n",
    "    \n",
    "    # Training\n",
    "    # Set our model to training mode (as opposed to evaluation mode)\n",
    "    model.train()\n",
    "    # Tracking variables\n",
    "    tr_loss = 0\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "    # Train the data for one epoch\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        # Add batch to GPU\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        # Unpack the inputs from our dataloader\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        # Clear out the gradients (by default they accumulate)\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass\n",
    "        loss = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
    "        train_loss_set.append(loss.item())    \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        # Update parameters and take a step using the computed gradient\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update tracking variables\n",
    "        tr_loss += loss.item()\n",
    "        nb_tr_examples += b_input_ids.size(0)\n",
    "        nb_tr_steps += 1\n",
    "\n",
    "    print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))\n",
    "    \n",
    "    \n",
    "    # Validation\n",
    "\n",
    "    # Put model in evaluation mode to evaluate loss on the validation set\n",
    "    model.eval()\n",
    "\n",
    "    # Tracking variables \n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_steps, nb_eval_examples = 0, 0\n",
    "\n",
    "    # Evaluate data for one epoch\n",
    "    for batch in validation_dataloader:\n",
    "        # Add batch to GPU\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        # Unpack the inputs from our dataloader\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        # Telling the model not to compute or store gradients, saving memory and speeding up validation\n",
    "        with torch.no_grad():\n",
    "            # Forward pass, calculate logit predictions\n",
    "            logits = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "\n",
    "        # Move logits and labels to CPU\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "\n",
    "        eval_accuracy += tmp_eval_accuracy\n",
    "        nb_eval_steps += 1\n",
    "\n",
    "    print(\"Validation Accuracy: {}\".format(eval_accuracy/nb_eval_steps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.8693264982049158\n"
     ]
    }
   ],
   "source": [
    "# Validation\n",
    "\n",
    "# Put model in evaluation mode to evaluate loss on the validation set\n",
    "model.eval()\n",
    "\n",
    "# Tracking variables \n",
    "eval_loss, eval_accuracy = 0, 0\n",
    "nb_eval_steps, nb_eval_examples = 0, 0\n",
    "logits_all=[]\n",
    "label_ids_all=[]\n",
    "\n",
    "# Evaluate data for one epoch\n",
    "for batch in validation_dataloader:\n",
    "    # Add batch to GPU\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    # Unpack the inputs from our dataloader\n",
    "    b_input_ids, b_input_mask, b_labels = batch\n",
    "    # Telling the model not to compute or store gradients, saving memory and speeding up validation\n",
    "    with torch.no_grad():\n",
    "        # Forward pass, calculate logit predictions\n",
    "        logits = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "\n",
    "    # Move logits and labels to CPU\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    logits_all+=list(np.argmax(logits, axis=1))\n",
    "    label_ids = b_labels.to('cpu').numpy()\n",
    "    label_ids_all+=list(label_ids)\n",
    "\n",
    "    tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "\n",
    "    eval_accuracy += tmp_eval_accuracy\n",
    "    nb_eval_steps += 1\n",
    "\n",
    "print(\"Validation Accuracy: {}\".format(eval_accuracy/nb_eval_steps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits_all_letter=preprocessing.LabelEncoder().fit(df_UCF_train.NTEE1.values).inverse_transform(logits_all)\n",
    "label_ids_all_letter=preprocessing.LabelEncoder().fit(df_UCF_train.NTEE1.values).inverse_transform(label_ids_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   pre       rec       spe        f1       geo       iba       sup\n",
      "\n",
      "          A     0.9468    0.8835    0.9938    0.9140    0.9370    0.8683      4291\n",
      "          B     0.9114    0.9057    0.9824    0.9086    0.9433    0.8830      6419\n",
      "          C     0.7935    0.8597    0.9951    0.8253    0.9249    0.8439       827\n",
      "          D     0.8873    0.9516    0.9967    0.9183    0.9739    0.9442      1034\n",
      "          E     0.8858    0.8240    0.9933    0.8538    0.9047    0.8046      2307\n",
      "          F     0.7423    0.7532    0.9963    0.7477    0.8663    0.7322       543\n",
      "          G     0.8259    0.8381    0.9936    0.8320    0.9126    0.8198      1353\n",
      "          H     0.4775    0.6746    0.9976    0.5592    0.8203    0.6512       126\n",
      "          I     0.8268    0.8514    0.9965    0.8389    0.9211    0.8361       740\n",
      "          J     0.8495    0.8224    0.9956    0.8357    0.9049    0.8046      1132\n",
      "          K     0.8340    0.8276    0.9977    0.8308    0.9087    0.8117       522\n",
      "          L     0.8123    0.8276    0.9921    0.8199    0.9061    0.8075      1537\n",
      "          M     0.9004    0.9596    0.9968    0.9291    0.9780    0.9530      1140\n",
      "          N     0.9333    0.9483    0.9923    0.9407    0.9701    0.9369      3925\n",
      "          O     0.8859    0.8166    0.9989    0.8499    0.9032    0.8008       409\n",
      "          P     0.7777    0.7062    0.9871    0.7402    0.8349    0.6775      2318\n",
      "          Q     0.5755    0.7339    0.9938    0.6452    0.8541    0.7105       436\n",
      "          R     0.7768    0.6770    0.9987    0.7235    0.8223    0.6544       257\n",
      "          S     0.8562    0.8973    0.9845    0.8763    0.9399    0.8757      3603\n",
      "          T     0.7568    0.6728    0.9969    0.7123    0.8190    0.6490       541\n",
      "          U     0.6343    0.7556    0.9974    0.6897    0.8681    0.7354       225\n",
      "          V     0.4494    0.4706    0.9987    0.4598    0.6856    0.4452        85\n",
      "          W     0.9042    0.9406    0.9944    0.9221    0.9672    0.9304      2038\n",
      "          X     0.7910    0.8342    0.9935    0.8121    0.9104    0.8157      1098\n",
      "          Y     0.9246    0.9089    0.9966    0.9167    0.9517    0.8978      1701\n",
      "\n",
      "avg / total     0.8712    0.8693    0.9911    0.8695    0.9273    0.8515     38607\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from imblearn.metrics import classification_report_imbalanced\n",
    "print(classification_report_imbalanced(y_true=label_ids_all_letter, y_pred=logits_all_letter, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'P': 2105,\n",
       "         'A': 4004,\n",
       "         'N': 3988,\n",
       "         'Y': 1672,\n",
       "         'G': 1373,\n",
       "         'E': 2146,\n",
       "         'C': 896,\n",
       "         'S': 3776,\n",
       "         'I': 762,\n",
       "         'B': 6379,\n",
       "         'M': 1215,\n",
       "         'D': 1109,\n",
       "         'L': 1566,\n",
       "         'X': 1158,\n",
       "         'Q': 556,\n",
       "         'K': 518,\n",
       "         'R': 224,\n",
       "         'O': 377,\n",
       "         'U': 268,\n",
       "         'W': 2120,\n",
       "         'H': 178,\n",
       "         'J': 1096,\n",
       "         'F': 551,\n",
       "         'T': 481,\n",
       "         'V': 89})"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(logits_all_letter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "t=pd.DataFrame([logits_all_letter, label_ids_all_letter]).T.rename(columns={0:'pred', 1:'true'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8692983137772943"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(t[t.pred==t.true])/len(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred</th>\n",
       "      <th>true</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1937</th>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37047</th>\n",
       "      <td>G</td>\n",
       "      <td>G</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10310</th>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3989</th>\n",
       "      <td>S</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4727</th>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13050</th>\n",
       "      <td>L</td>\n",
       "      <td>L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20474</th>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21304</th>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21964</th>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3206</th>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7279</th>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17480</th>\n",
       "      <td>J</td>\n",
       "      <td>J</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4016</th>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17735</th>\n",
       "      <td>W</td>\n",
       "      <td>W</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35544</th>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29179</th>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1102</th>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274</th>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19492</th>\n",
       "      <td>W</td>\n",
       "      <td>W</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33841</th>\n",
       "      <td>E</td>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5758</th>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38518</th>\n",
       "      <td>T</td>\n",
       "      <td>T</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18778</th>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31446</th>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3040</th>\n",
       "      <td>W</td>\n",
       "      <td>W</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>778</th>\n",
       "      <td>S</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1362</th>\n",
       "      <td>S</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13880</th>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12724</th>\n",
       "      <td>D</td>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37721</th>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13247</th>\n",
       "      <td>W</td>\n",
       "      <td>W</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27738</th>\n",
       "      <td>D</td>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4447</th>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4665</th>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12235</th>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9099</th>\n",
       "      <td>I</td>\n",
       "      <td>I</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13603</th>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21394</th>\n",
       "      <td>W</td>\n",
       "      <td>W</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24125</th>\n",
       "      <td>X</td>\n",
       "      <td>X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27765</th>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2236</th>\n",
       "      <td>P</td>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28270</th>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22324</th>\n",
       "      <td>U</td>\n",
       "      <td>U</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2413</th>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19475</th>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3059</th>\n",
       "      <td>X</td>\n",
       "      <td>X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34695</th>\n",
       "      <td>W</td>\n",
       "      <td>W</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31093</th>\n",
       "      <td>W</td>\n",
       "      <td>W</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13654</th>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3597</th>\n",
       "      <td>W</td>\n",
       "      <td>W</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      pred true\n",
       "1937     N    N\n",
       "37047    G    G\n",
       "10310    N    N\n",
       "3989     S    S\n",
       "4727     N    N\n",
       "13050    L    L\n",
       "20474    A    A\n",
       "21304    B    B\n",
       "21964    B    B\n",
       "3206     Y    Y\n",
       "7279     N    N\n",
       "17480    J    J\n",
       "4016     C    C\n",
       "17735    W    W\n",
       "35544    A    A\n",
       "29179    B    B\n",
       "1102     B    B\n",
       "274      N    N\n",
       "19492    W    W\n",
       "33841    E    E\n",
       "5758     N    N\n",
       "38518    T    T\n",
       "18778    B    B\n",
       "31446    B    B\n",
       "3040     W    W\n",
       "778      S    S\n",
       "1362     S    S\n",
       "13880    Y    Y\n",
       "12724    D    D\n",
       "37721    M    M\n",
       "13247    W    W\n",
       "27738    D    D\n",
       "4447     M    M\n",
       "4665     N    N\n",
       "12235    A    A\n",
       "9099     I    I\n",
       "13603    A    A\n",
       "21394    W    W\n",
       "24125    X    X\n",
       "27765    A    A\n",
       "2236     P    E\n",
       "28270    B    B\n",
       "22324    U    U\n",
       "2413     B    B\n",
       "19475    A    A\n",
       "3059     X    X\n",
       "34695    W    W\n",
       "31093    W    W\n",
       "13654    B    B\n",
       "3597     W    W"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.sample(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
