{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Draft codes saved in `NN_broad_cat.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1']\n"
     ]
    }
   ],
   "source": [
    "# obtain reproducible results\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random as rn\n",
    "\n",
    "# The below is necessary for starting Numpy generated random numbers\n",
    "# in a well-defined initial state.\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# The below is necessary for starting core Python generated random numbers\n",
    "# in a well-defined state.\n",
    "\n",
    "rn.seed(12345)\n",
    "\n",
    "# Force TensorFlow to use single thread.\n",
    "# Multiple threads are a potential source of non-reproducible results.\n",
    "# For further details, see: https://stackoverflow.com/questions/42022950/\n",
    "\n",
    "session_conf = tf.ConfigProto(intra_op_parallelism_threads=1,\n",
    "                              inter_op_parallelism_threads=1)\n",
    "\n",
    "from keras import backend as K\n",
    "\n",
    "# The below tf.set_random_seed() will make random number generation\n",
    "# in the TensorFlow backend have a well-defined initial state.\n",
    "# For further details, see:\n",
    "# https://www.tensorflow.org/api_docs/python/tf/set_random_seed\n",
    "\n",
    "tf.set_random_seed(1234)\n",
    "\n",
    "sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
    "K.set_session(sess)\n",
    "\n",
    "# Rest of code follows ...\n",
    "\n",
    "# Check GPU device.\n",
    "print(K.tensorflow_backend._get_available_gpus())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://richliao.github.io/supervised/classification/2016/11/26/textclassifier-convolutional/\n",
    "#https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html\n",
    "#RNN\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from multiprocessing import Pool\n",
    "from spellchecker import SpellChecker\n",
    "import string\n",
    "import math\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "import keras.preprocessing.text as kpt\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Embedding\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk import word_tokenize\n",
    "\n",
    "# For encoding labels.\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read and compile tranining and validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(154424, 38607)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_file_path='../../dataset/UCF/train/'\n",
    "file_list=os.listdir(train_file_path)\n",
    "df_train=pd.DataFrame()\n",
    "for file in file_list:\n",
    "    df_train=pd.concat([df_train, pd.read_pickle(train_file_path+file, compression='gzip')])\n",
    "\n",
    "test_file_path='../../dataset/UCF/test/'\n",
    "file_list=os.listdir(test_file_path)\n",
    "df_test=pd.DataFrame()\n",
    "for file in file_list:\n",
    "    df_test=pd.concat([df_test, pd.read_pickle(test_file_path+file, compression='gzip')])\n",
    "    \n",
    "len(df_train), len(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "154424 25\n",
      "38607 25\n"
     ]
    }
   ],
   "source": [
    "df_train['mission_prgrm_spellchk']=df_train['TAXPAYER_NAME']+' '+df_train['mission_spellchk']+' '+df_train['prgrm_dsc_spellchk'] # Using spell-checked.\n",
    "print(len(df_train['mission_prgrm_spellchk']), len(df_train['NTEE1'].drop_duplicates()))\n",
    "\n",
    "df_test['mission_prgrm_spellchk']=df_test['TAXPAYER_NAME']+' '+df_test['mission_spellchk']+' '+df_test['prgrm_dsc_spellchk'] # Using spell-checked.\n",
    "print(len(df_test['mission_prgrm_spellchk']), len(df_test['NTEE1'].drop_duplicates()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NTEE1\n",
      "A    0.110151\n",
      "B    0.167247\n",
      "C    0.021519\n",
      "D    0.027450\n",
      "E    0.058378\n",
      "F    0.014901\n",
      "G    0.032722\n",
      "H    0.003024\n",
      "I    0.019084\n",
      "J    0.030902\n",
      "K    0.013010\n",
      "L    0.038478\n",
      "M    0.030390\n",
      "N    0.100114\n",
      "O    0.011209\n",
      "P    0.059447\n",
      "Q    0.012867\n",
      "R    0.006890\n",
      "S    0.093632\n",
      "T    0.013159\n",
      "U    0.006476\n",
      "V    0.002266\n",
      "W    0.054117\n",
      "X    0.029568\n",
      "Y    0.042998\n",
      "Name: EIN, dtype: float64 \n",
      "\n",
      " NTEE1\n",
      "A    0.111146\n",
      "B    0.166265\n",
      "C    0.021421\n",
      "D    0.026783\n",
      "E    0.059756\n",
      "F    0.014065\n",
      "G    0.035045\n",
      "H    0.003264\n",
      "I    0.019168\n",
      "J    0.029321\n",
      "K    0.013521\n",
      "L    0.039811\n",
      "M    0.029528\n",
      "N    0.101666\n",
      "O    0.010594\n",
      "P    0.060041\n",
      "Q    0.011293\n",
      "R    0.006657\n",
      "S    0.093325\n",
      "T    0.014013\n",
      "U    0.005828\n",
      "V    0.002202\n",
      "W    0.052788\n",
      "X    0.028440\n",
      "Y    0.044059\n",
      "Name: EIN, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# # Build training and testing data frame.\n",
    "# small_num=0\n",
    "# while small_num<500: # Make sure each category has at least 500 records.\n",
    "#     sampleDF = df_train[df_train.mission.notna() & df_train.NTEE1.notna()].sample(120000)\n",
    "#     trainDF, valDF =train_test_split(sampleDF, test_size=.3)\n",
    "#     small_num=trainDF.groupby('NTEE_M').count().sort_values('EIN').iloc[0]['EIN']\n",
    "\n",
    "# See the composition by NTEE major groups.\n",
    "print(df_train.groupby('NTEE1').count()['EIN']/len(df_train), '\\n'*2, df_test.groupby('NTEE1').count()['EIN']/len(df_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare labels.\n",
    "*One hot encoding.* Prepare after resampling; otherwise, shape of `y_train` will shrink from 25 to 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "lb = preprocessing.LabelBinarizer()\n",
    "lb.fit(list(df_train.NTEE1.unique()))\n",
    "\n",
    "y_train=lb.transform(df_train['NTEE1'])\n",
    "# y_test=lb.transform(df_test['NTEE1']) # No need to transform Y for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# Save LabelBinarizer class for developing package.\n",
    "with open('../../output/lb_major_group.pkl', 'wb') as output:\n",
    "    pickle.dump(lb, output, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare input text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_token_list_train=df_train['mission_prgrm_spellchk']\n",
    "text_token_list_test=df_test['mission_prgrm_spellchk']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('and', 1), ('the', 2), ('to', 3), ('of', 4), ('in', 5)]\n"
     ]
    }
   ],
   "source": [
    "# Build word index for train and validation texts.\n",
    "tokenizer=Tokenizer()\n",
    "tokenizer.fit_on_texts(text_token_list_train.to_list()+text_token_list_test.to_list())\n",
    "print(list(tokenizer.word_index.items())[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text to sequences.\n",
    "seq_encoding_text_train=tokenizer.texts_to_sequences(text_token_list_train)\n",
    "seq_encoding_text_test=tokenizer.texts_to_sequences(text_token_list_test)\n",
    "\n",
    "# Pads sequences to the same length (i.e., prepare matrix).\n",
    "x_train=pad_sequences(sequences=seq_encoding_text_train,\n",
    "                      maxlen=max([len(s) for s in seq_encoding_text_train]), # Max length of the sequence.\n",
    "                      dtype = \"int32\", padding = \"post\", truncating = \"post\", \n",
    "                      value = 0 # Zero is used for representing None or Unknown.\n",
    "                     )\n",
    "\n",
    "x_test=pad_sequences(sequences=seq_encoding_text_test,\n",
    "                    maxlen=max([len(s) for s in seq_encoding_text_train]), # Max length of the sequence.\n",
    "                    dtype = \"int32\", padding = \"post\", truncating = \"post\", \n",
    "                    value = 0 # Zero is used for representing None or Unknown.\n",
    "                     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to Compressed Sparse Row matrix; otherwise, matrix too large, result memory error.\n",
    "# https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html\n",
    "from scipy.sparse import csr_matrix\n",
    "x_train=csr_matrix(x_train)\n",
    "\n",
    "# Define resample strategy.\n",
    "def func_resample(method, sampling_strategy, x_train_vect, y_train):\n",
    "    if method=='ADASYN':\n",
    "        from imblearn.over_sampling import ADASYN\n",
    "        resample = ADASYN(sampling_strategy=sampling_strategy, random_state=42)\n",
    "    elif method=='RandomOverSampler':\n",
    "        from imblearn.over_sampling import RandomOverSampler\n",
    "        resample = RandomOverSampler(sampling_strategy=sampling_strategy, random_state=42)\n",
    "    elif method=='SMOTE':\n",
    "        from imblearn.over_sampling import SMOTE\n",
    "        resample = SMOTE(sampling_strategy=sampling_strategy, random_state=42)\n",
    "    elif method=='SMOTEENN':\n",
    "        from imblearn.combine import SMOTEENN\n",
    "        resample = SMOTEENN(sampling_strategy=sampling_strategy, random_state=42)\n",
    "    elif method=='SMOTETomek':\n",
    "        from imblearn.combine import SMOTETomek\n",
    "        resample = SMOTETomek(sampling_strategy=sampling_strategy, random_state=42)\n",
    "    x_train_vect_res, y_train_res = resample.fit_resample(x_train_vect, y_train)\n",
    "    return [x_train_vect_res, y_train_res]\n",
    "\n",
    "x_train_res, y_train_res = func_resample(method='ADASYN', sampling_strategy='minority', \n",
    "                                         x_train_vect=x_train, y_train=y_train)\n",
    "\n",
    "# x_train_res, y_train_res = [x_train, y_train]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare embedding layer.\n",
    "Use pre-trained GloVe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "EMBEDDING_DIM=100\n",
    "glove_word_vector=api.load('glove-wiki-gigaword-'+str(EMBEDDING_DIM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((len(tokenizer.word_index)+1, EMBEDDING_DIM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 140908/140908 [00:00<00:00, 393936.68it/s]\n"
     ]
    }
   ],
   "source": [
    "for word, index in tqdm(tokenizer.word_index.items()):\n",
    "    try:\n",
    "        embedding_matrix[index] = glove_word_vector.get_vector(word)\n",
    "    except:\n",
    "        pass\n",
    "        # words not found in embedding index will be all-zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/\n",
    "embedding_layer = Embedding(input_dim=len(tokenizer.word_index)+1, # Size of vocabulary.\n",
    "                            input_length=max([len(s) for s in seq_encoding_text_train]), # Length of input, i.e., length of padded sequence.\n",
    "                            output_dim=EMBEDDING_DIM, # Size of the vector space in which words will be embedded.\n",
    "                            weights=[embedding_matrix],\n",
    "                            trainable=False\n",
    "                           )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic tuning of training params."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, GlobalMaxPool1D, Conv1D\n",
    "from keras.layers.advanced_activations import LeakyReLU, PReLU\n",
    "\n",
    "with tf.device('/gpu:1'): # Specify which GPU to use.\n",
    "    # define the model\n",
    "    model = Sequential()\n",
    "    model.add(embedding_layer)\n",
    "    # model.add(Flatten())\n",
    "    model.add(Conv1D(128, 5, activation='softplus'))\n",
    "    model.add(GlobalMaxPool1D())\n",
    "    model.add(Dense(units=32, activation='sigmoid'))\n",
    "    model.add(Dense(units=16, activation='softplus'))\n",
    "    # model.add(PReLU()) # https://medium.com/tinymind/a-practical-guide-to-relu-b83ca804f1f7\n",
    "    model.add(Dense(units=16, activation='tanh'))\n",
    "    model.add(Dense(units=16, activation='softplus'))\n",
    "    model.add(Dense(units=len(y_train[0]), activation='softmax'))\n",
    "    # compile the model\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc', \n",
    "    #                                                                      precision, recall\n",
    "                                                                        ])\n",
    "    # summarize the model\n",
    "    print(model.summary())\n",
    "\n",
    "    # fit the model\n",
    "    history=model.fit(x_train, y_train, validation_split=0.3, epochs=20, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try Grid Search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continue previous work.\n",
    "df_history=pd.read_csv('../../output/grid_search_history_major_group.tsv', sep='\\t', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_list_done=set(map(tuple, \n",
    "                        df_history[['conv_num_filters', 'conv_kernel_size', 'conv_act', 'out_act']].values.tolist()\n",
    "                       )\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, GlobalMaxPool1D, Conv1D\n",
    "from datetime import datetime\n",
    "\n",
    "# df_history=pd.DataFrame()\n",
    "for num_filters in [32, 64, 128]:\n",
    "    for kernel_size in [3,5,7]:\n",
    "        for conv_act in ['sigmoid', 'softplus', 'tanh', 'softmax']:\n",
    "            for out_act in ['sigmoid', 'softplus', 'tanh', 'softmax']:\n",
    "                param=tuple((num_filters, kernel_size, conv_act, out_act))\n",
    "                if param not in param_list_done:\n",
    "                    t1=datetime.now()\n",
    "                    # Run NN on a specified GPU.\n",
    "                    with tf.device('/device:GPU:1'):\n",
    "                        # define the model\n",
    "                        model = Sequential()\n",
    "                        model.add(embedding_layer)\n",
    "                        # model.add(Flatten())\n",
    "                        model.add(Conv1D(num_filters, kernel_size, activation=conv_act))\n",
    "                        model.add(GlobalMaxPool1D())\n",
    "                        model.add(Dense(units=32, activation='sigmoid'))\n",
    "                        model.add(Dense(units=32, activation='softplus'))\n",
    "                        model.add(Dense(units=16, activation='tanh'))\n",
    "                        model.add(Dense(units=16, activation='softplus'))\n",
    "                        model.add(Dense(units=len(y_train[0]), activation=out_act))\n",
    "                        # compile the model\n",
    "                        model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
    "                        # F1, precision, and recall removed. https://github.com/keras-team/keras/issues/5794\n",
    "                        # fit the model\n",
    "                        history=model.fit(x_train, y_train, validation_split=0.2, epochs=50, verbose=0)\n",
    "                        y_prob = model.predict(x_val, verbose=0)\n",
    "                    # Save history.\n",
    "                    acc = history.history['acc']\n",
    "                    val_acc = history.history['val_acc']\n",
    "                    loss = history.history['loss']\n",
    "                    val_loss = history.history['val_loss']\n",
    "                    epochs = range(1, len(acc) + 1)\n",
    "                    # Calculate on validation dataset.\n",
    "                    y_classes = y_prob.argmax(axis=-1)\n",
    "                    y_classes_prob=[s.max() for s in y_prob]\n",
    "                    y_classes_val=y_val.argmax(axis=-1)\n",
    "                    df_val=pd.DataFrame({'pred':y_classes, \n",
    "                                         'true':y_classes_val, \n",
    "                                         'prob':y_classes_prob})\n",
    "                    val_acc_real=len(df_val[df_val.pred==df_val.true])/len(df_val)\n",
    "                    # Save history to datafame.\n",
    "                    df_history_temp=pd.DataFrame()\n",
    "                    df_history_temp['acc']=acc\n",
    "                    df_history_temp['val_acc']=val_acc\n",
    "                    df_history_temp['val_acc_real']=[math.nan]*(len(epochs)-1)+[val_acc_real]\n",
    "                    df_history_temp['loss']=loss\n",
    "                    df_history_temp['val_loss']=val_loss\n",
    "                    df_history_temp['epochs']=epochs\n",
    "                    df_history_temp['conv_num_filters']=[num_filters]*len(epochs)\n",
    "                    df_history_temp['conv_kernel_size']=[kernel_size]*len(epochs)\n",
    "                    df_history_temp['conv_act']=[conv_act]*len(epochs)\n",
    "                    df_history_temp['out_act']=[out_act]*len(epochs)\n",
    "                    df_history_temp['time_stamp']=[str(t1)]+[math.nan]*(len(epochs)-2)+[str(datetime.now())]\n",
    "                    df_history=df_history.append(df_history_temp, ignore_index=True)\n",
    "                    df_history.to_csv('../../output/grid_search_history_major_group.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision making: Optimizing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continue previous work.\n",
    "try:\n",
    "    df_history=pd.read_csv('../../output/grid_search_history_major_group_optimizing.tsv', sep='\\t', index_col=0)\n",
    "except:\n",
    "    df_history=pd.DataFrame(columns=pd.read_csv('../../output/grid_search_history_major_group.tsv', sep='\\t', index_col=0).columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_list_done=set(map(tuple, \n",
    "                        df_history[['conv_num_filters', 'conv_kernel_size', 'conv_act', 'out_act']].values.tolist()\n",
    "                       )\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, GlobalMaxPool1D, Conv1D\n",
    "from datetime import datetime\n",
    "\n",
    "# df_history=pd.DataFrame()\n",
    "for num_filters in [128, 256, 512, 1024]:\n",
    "    for kernel_size in [3]:\n",
    "        for conv_act in ['softplus']:\n",
    "            for out_act in ['sigmoid', 'softplus', 'softmax']:\n",
    "                param=tuple((num_filters, kernel_size, conv_act, out_act))\n",
    "                if param not in param_list_done:\n",
    "                    t1=datetime.now()\n",
    "                    # Run NN on a specified GPU.\n",
    "                    with tf.device('/device:GPU:1'):\n",
    "                        # define the model\n",
    "                        model = Sequential()\n",
    "                        model.add(embedding_layer)\n",
    "                        # model.add(Flatten())\n",
    "                        model.add(Conv1D(num_filters, kernel_size, activation=conv_act))\n",
    "                        model.add(GlobalMaxPool1D())\n",
    "                        model.add(Dense(units=32, activation='sigmoid'))\n",
    "                        model.add(Dense(units=32, activation='softplus'))\n",
    "                        model.add(Dense(units=16, activation='tanh'))\n",
    "                        model.add(Dense(units=16, activation='softplus'))\n",
    "                        model.add(Dense(units=len(y_train[0]), activation=out_act))\n",
    "                        # compile the model\n",
    "                        model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
    "                        # F1, precision, and recall removed. https://github.com/keras-team/keras/issues/5794\n",
    "                        # fit the model\n",
    "                        history=model.fit(x_train, y_train, validation_split=0.2, epochs=50, verbose=0)\n",
    "                        y_prob = model.predict(x_val, verbose=0)\n",
    "                    # Save history.\n",
    "                    acc = history.history['acc']\n",
    "                    val_acc = history.history['val_acc']\n",
    "                    loss = history.history['loss']\n",
    "                    val_loss = history.history['val_loss']\n",
    "                    epochs = range(1, len(acc) + 1)\n",
    "                    # Calculate on validation dataset.\n",
    "                    y_classes = y_prob.argmax(axis=-1)\n",
    "                    y_classes_prob=[s.max() for s in y_prob]\n",
    "                    y_classes_val=y_val.argmax(axis=-1)\n",
    "                    df_val=pd.DataFrame({'pred':y_classes, \n",
    "                                         'true':y_classes_val, \n",
    "                                         'prob':y_classes_prob})\n",
    "                    val_acc_real=len(df_val[df_val.pred==df_val.true])/len(df_val)\n",
    "                    # Save history to datafame.\n",
    "                    df_history_temp=pd.DataFrame()\n",
    "                    df_history_temp['acc']=acc\n",
    "                    df_history_temp['val_acc']=val_acc\n",
    "                    df_history_temp['val_acc_real']=[math.nan]*(len(epochs)-1)+[val_acc_real]\n",
    "                    df_history_temp['loss']=loss\n",
    "                    df_history_temp['val_loss']=val_loss\n",
    "                    df_history_temp['epochs']=epochs\n",
    "                    df_history_temp['conv_num_filters']=[num_filters]*len(epochs)\n",
    "                    df_history_temp['conv_kernel_size']=[kernel_size]*len(epochs)\n",
    "                    df_history_temp['conv_act']=[conv_act]*len(epochs)\n",
    "                    df_history_temp['out_act']=[out_act]*len(epochs)\n",
    "                    df_history_temp['time_stamp']=[str(t1)]+[math.nan]*(len(epochs)-2)+[str(datetime.now())]\n",
    "                    df_history=df_history.append(df_history_temp, ignore_index=True)\n",
    "                    df_history.to_csv('../../output/grid_search_history_major_group_optimizing.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model finalist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Best configuration**\n",
    "\n",
    "_Broad Category_\n",
    "\n",
    "|acc | val_acc | val_acc_real | loss | val_loss | epochs | conv_num_filters | conv_kernel_size | conv_act | out_act|\n",
    "|--|--|--|--|--|--|--|--|--|--|\n",
    "|0.820386905 | 0.776488095 | -- | 0.613613255 | 0.738004138 | 4 | 512 | 3 | softplus | softplus|\n",
    "\n",
    "_Major Group_\n",
    "\n",
    "|acc | val_acc | val_acc_real | loss | val_loss | epochs | conv_num_filters | conv_kernel_size | conv_act | out_act|\n",
    "|--|--|--|--|--|--|--|--|--|--|\n",
    "|0.764369048 | 0.710428571 | -- | 0.888776311 | 1.120441045 | 6 | 256 | 3 | softplus | softmax|\n",
    "|0.779 | 0.71 | -- | 0.834419103 | 1.140895644 | 7 | 256 | 3 | softplus | softplus|\n",
    "\n",
    "Use `softplus`+`softplus`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 143931 samples, validate on 35983 samples\n",
      "Epoch 1/7\n",
      "143931/143931 [==============================] - 1796s 12ms/step - loss: 1.5458 - acc: 0.5732 - val_loss: 4.3219 - val_acc: 0.2098\n",
      "Epoch 2/7\n",
      "143931/143931 [==============================] - 1794s 12ms/step - loss: 0.9679 - acc: 0.7517 - val_loss: 4.2726 - val_acc: 0.2163\n",
      "Epoch 3/7\n",
      "143931/143931 [==============================] - 1793s 12ms/step - loss: 0.8444 - acc: 0.7828 - val_loss: 4.6133 - val_acc: 0.2205\n",
      "Epoch 4/7\n",
      "143931/143931 [==============================] - 1794s 12ms/step - loss: 0.7608 - acc: 0.8030 - val_loss: 4.6981 - val_acc: 0.2253\n",
      "Epoch 5/7\n",
      "143931/143931 [==============================] - 1794s 12ms/step - loss: 0.6972 - acc: 0.8183 - val_loss: 4.8746 - val_acc: 0.2222\n",
      "Epoch 6/7\n",
      "143931/143931 [==============================] - 1794s 12ms/step - loss: 0.6397 - acc: 0.8325 - val_loss: 4.7663 - val_acc: 0.2249\n",
      "Epoch 7/7\n",
      "143931/143931 [==============================] - 1794s 12ms/step - loss: 0.5926 - acc: 0.8442 - val_loss: 5.8283 - val_acc: 0.2254\n",
      "Test on UCF-Testing\n",
      "38607/38607 [==============================] - 144s 4ms/step\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, GlobalMaxPool1D, Conv1D\n",
    "from datetime import datetime\n",
    "\n",
    "with tf.device('/device:GPU:1'):\n",
    "    # define the model\n",
    "    model = Sequential()\n",
    "    model.add(embedding_layer)\n",
    "    # model.add(Flatten())\n",
    "    model.add(Conv1D(512, 3, activation='softplus'))\n",
    "    model.add(GlobalMaxPool1D())\n",
    "    model.add(Dense(units=32, activation='sigmoid'))\n",
    "    model.add(Dense(units=32, activation='softplus'))\n",
    "    model.add(Dense(units=16, activation='tanh'))\n",
    "    model.add(Dense(units=16, activation='softplus'))\n",
    "    model.add(Dense(units=len(y_train[0]), activation='softplus'))\n",
    "    # compile the model\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
    "    # F1, precision, and recall removed. https://github.com/keras-team/keras/issues/5794\n",
    "    # fit the model\n",
    "    history=model.fit(x_train_res, y_train_res, validation_split=0.2, epochs=7, verbose=1)\n",
    "    print('Test on UCF-Testing')\n",
    "    y_prob = model.predict(x_test, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall ACC: 0.7773719791747611\n"
     ]
    }
   ],
   "source": [
    "# Calculate on validation dataset.\n",
    "# From probability --> serial coding, e.g., [4, 3, 2, 55] --> categorical, e.g., [[00010...], [001000..]...]\n",
    "y_pred = lb.inverse_transform(np_utils.to_categorical(y_prob.argmax(axis=-1)))\n",
    "y_pred_prob=[s.max() for s in y_prob]\n",
    "df_val=pd.DataFrame({'pred':y_pred, \n",
    "                     'true':df_test['NTEE1'], \n",
    "                     'prob':y_pred_prob})\n",
    "print('Overall ACC:', len(df_val[df_val.pred==df_val.true])/len(df_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   pre       rec       spe        f1       geo       iba       sup\n",
      "\n",
      "          A       0.86      0.84      0.98      0.85      0.91      0.81      4291\n",
      "          B       0.85      0.86      0.97      0.85      0.91      0.82      6419\n",
      "          C       0.71      0.65      0.99      0.68      0.80      0.63       827\n",
      "          D       0.86      0.89      1.00      0.87      0.94      0.87      1034\n",
      "          E       0.76      0.78      0.98      0.77      0.88      0.75      2307\n",
      "          F       0.48      0.60      0.99      0.53      0.77      0.57       543\n",
      "          G       0.68      0.60      0.99      0.64      0.77      0.57      1353\n",
      "          H       0.45      0.04      1.00      0.07      0.20      0.04       126\n",
      "          I       0.62      0.72      0.99      0.66      0.85      0.70       740\n",
      "          J       0.73      0.75      0.99      0.74      0.86      0.73      1132\n",
      "          K       0.75      0.67      1.00      0.71      0.82      0.64       522\n",
      "          L       0.80      0.70      0.99      0.75      0.83      0.67      1537\n",
      "          M       0.83      0.91      0.99      0.87      0.95      0.89      1140\n",
      "          N       0.84      0.92      0.98      0.88      0.95      0.90      3925\n",
      "          O       0.78      0.51      1.00      0.61      0.71      0.48       409\n",
      "          P       0.58      0.68      0.97      0.62      0.81      0.64      2318\n",
      "          Q       0.36      0.34      0.99      0.35      0.58      0.31       436\n",
      "          R       0.37      0.26      1.00      0.30      0.51      0.24       257\n",
      "          S       0.83      0.78      0.98      0.80      0.87      0.75      3603\n",
      "          T       0.51      0.25      1.00      0.34      0.50      0.23       541\n",
      "          U       0.45      0.20      1.00      0.28      0.45      0.19       225\n",
      "          V       0.00      0.00      1.00      0.00      0.00      0.00        85\n",
      "          W       0.77      0.88      0.99      0.83      0.93      0.86      2038\n",
      "          X       0.73      0.70      0.99      0.71      0.83      0.68      1098\n",
      "          Y       0.84      0.89      0.99      0.86      0.94      0.87      1701\n",
      "\n",
      "avg / total       0.77      0.78      0.98      0.77      0.87      0.75     38607\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from imblearn.metrics import classification_report_imbalanced\n",
    "print(classification_report_imbalanced(y_true=df_test['NTEE1'], y_pred=y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Python\n",
    "from imblearn.metrics import classification_report_imbalanced\n",
    "print(classification_report_imbalanced(y_true=df_test['broad_cat'], y_pred=y_train))\n",
    "```\n",
    "**Chosen:** *epochs=7, resampling: method='ADASYN', sampling_strategy='minority'.*\n",
    "\n",
    "    ***Overall ACC: 0.7821897583339809 # This is chosen.***\n",
    "                       pre       rec       spe        f1       geo       iba       sup\n",
    "\n",
    "              A       0.80      0.87      0.97      0.83      0.92      0.83      4291\n",
    "              B       0.85      0.85      0.97      0.85      0.91      0.82      6419\n",
    "              C       0.65      0.74      0.99      0.69      0.86      0.72       827\n",
    "              D       0.80      0.90      0.99      0.85      0.94      0.88      1034\n",
    "              E       0.77      0.78      0.98      0.78      0.88      0.76      2307\n",
    "              F       0.51      0.60      0.99      0.55      0.77      0.57       543\n",
    "              G       0.68      0.68      0.99      0.68      0.82      0.65      1353\n",
    "              H       0.55      0.19      1.00      0.28      0.44      0.17       126\n",
    "              I       0.71      0.71      0.99      0.71      0.84      0.68       740\n",
    "              J       0.86      0.67      1.00      0.75      0.82      0.65      1132\n",
    "              K       0.63      0.68      0.99      0.66      0.82      0.66       522\n",
    "              L       0.70      0.76      0.99      0.73      0.87      0.73      1537\n",
    "              M       0.87      0.90      1.00      0.88      0.95      0.89      1140\n",
    "              N       0.83      0.93      0.98      0.88      0.95      0.90      3925\n",
    "              O       0.65      0.61      1.00      0.63      0.78      0.59       409\n",
    "              P       0.64      0.57      0.98      0.60      0.75      0.53      2318\n",
    "              Q       0.43      0.36      0.99      0.39      0.60      0.33       436\n",
    "              R       0.46      0.21      1.00      0.28      0.45      0.19       257\n",
    "              S       0.84      0.79      0.98      0.81      0.88      0.76      3603\n",
    "              T       0.66      0.32      1.00      0.43      0.56      0.30       541\n",
    "              U       0.52      0.22      1.00      0.31      0.47      0.20       225\n",
    "              V       0.00      0.00      1.00      0.00      0.00      0.00        85\n",
    "              W       0.87      0.86      0.99      0.86      0.92      0.84      2038\n",
    "              X       0.68      0.71      0.99      0.70      0.84      0.69      1098\n",
    "              Y       0.84      0.91      0.99      0.88      0.95      0.90      1701\n",
    "\n",
    "    avg / total       0.78      0.78      0.98      0.78      0.87      0.76     38607\n",
    "\n",
    "*epochs=4, resampling: method='ADASYN', sampling_strategy='not majority'.*\n",
    "    \n",
    "    Overall ACC: 0.7046390550936359\n",
    "                       pre       rec       spe        f1       geo       iba       sup\n",
    "\n",
    "              A       0.79      0.83      0.97      0.81      0.90      0.80      4291\n",
    "              B       0.86      0.81      0.97      0.83      0.89      0.77      6419\n",
    "              C       0.69      0.52      0.99      0.59      0.72      0.49       827\n",
    "              D       0.77      0.86      0.99      0.81      0.92      0.84      1034\n",
    "              E       0.79      0.64      0.99      0.71      0.80      0.62      2307\n",
    "              F       0.00      0.00      1.00      0.00      0.00      0.00       543\n",
    "              G       0.36      0.74      0.95      0.49      0.84      0.69      1353\n",
    "              H       0.02      0.02      1.00      0.02      0.15      0.02       126\n",
    "              I       0.24      0.31      0.98      0.27      0.55      0.29       740\n",
    "              J       0.49      0.71      0.98      0.58      0.83      0.67      1132\n",
    "              K       0.49      0.57      0.99      0.53      0.75      0.54       522\n",
    "              L       0.67      0.72      0.99      0.69      0.84      0.69      1537\n",
    "              M       0.85      0.89      1.00      0.87      0.94      0.88      1140\n",
    "              N       0.87      0.84      0.99      0.86      0.91      0.81      3925\n",
    "              O       0.41      0.47      0.99      0.44      0.69      0.45       409\n",
    "              P       0.44      0.65      0.95      0.52      0.78      0.60      2318\n",
    "              Q       0.08      0.00      1.00      0.00      0.05      0.00       436\n",
    "              R       0.03      0.00      1.00      0.01      0.06      0.00       257\n",
    "              S       0.83      0.71      0.99      0.77      0.84      0.68      3603\n",
    "              T       0.50      0.19      1.00      0.27      0.43      0.17       541\n",
    "              U       0.00      0.00      1.00      0.00      0.00      0.00       225\n",
    "              V       0.00      0.00      1.00      0.00      0.00      0.00        85\n",
    "              W       0.90      0.75      1.00      0.82      0.86      0.73      2038\n",
    "              X       0.60      0.62      0.99      0.61      0.79      0.59      1098\n",
    "              Y       0.89      0.79      1.00      0.83      0.89      0.77      1701\n",
    "\n",
    "    avg / total       0.71      0.70      0.98      0.70      0.81      0.68     38607\n",
    "\n",
    "\n",
    "*epochs=7, no resampling*\n",
    "\n",
    "    Overall ACC: 0.7776309995596653\n",
    "                       pre       rec       spe        f1       geo       iba       sup\n",
    "\n",
    "              A       0.85      0.85      0.98      0.85      0.91      0.82      4291\n",
    "              B       0.85      0.86      0.97      0.85      0.91      0.82      6419\n",
    "              C       0.57      0.76      0.99      0.65      0.86      0.73       827\n",
    "              D       0.78      0.90      0.99      0.83      0.94      0.88      1034\n",
    "              E       0.77      0.76      0.99      0.76      0.87      0.73      2307\n",
    "              F       0.70      0.37      1.00      0.48      0.61      0.35       543\n",
    "              G       0.56      0.73      0.98      0.64      0.85      0.70      1353\n",
    "              H       0.00      0.00      1.00      0.00      0.00      0.00       126\n",
    "              I       0.78      0.63      1.00      0.70      0.79      0.61       740\n",
    "              J       0.78      0.73      0.99      0.75      0.85      0.71      1132\n",
    "              K       0.67      0.69      1.00      0.68      0.83      0.66       522\n",
    "              L       0.78      0.71      0.99      0.75      0.84      0.69      1537\n",
    "              M       0.86      0.89      1.00      0.87      0.94      0.88      1140\n",
    "              N       0.89      0.90      0.99      0.89      0.94      0.88      3925\n",
    "              O       0.75      0.56      1.00      0.65      0.75      0.54       409\n",
    "              P       0.56      0.69      0.96      0.61      0.81      0.65      2318\n",
    "              Q       0.37      0.39      0.99      0.38      0.63      0.37       436\n",
    "              R       0.60      0.30      1.00      0.40      0.55      0.28       257\n",
    "              S       0.78      0.81      0.98      0.79      0.89      0.78      3603\n",
    "              T       0.55      0.43      0.99      0.48      0.65      0.40       541\n",
    "              U       0.35      0.19      1.00      0.24      0.43      0.17       225\n",
    "              V       0.00      0.00      1.00      0.00      0.00      0.00        85\n",
    "              W       0.89      0.82      0.99      0.85      0.90      0.81      2038\n",
    "              X       0.77      0.66      0.99      0.71      0.81      0.63      1098\n",
    "              Y       0.91      0.83      1.00      0.87      0.91      0.82      1701\n",
    "\n",
    "    avg / total       0.78      0.78      0.98      0.77      0.87      0.75     38607\n",
    "\n",
    "*epochs=4, resampling: method='ADASYN', sampling_strategy='minority'.*\n",
    "\n",
    "    Overall ACC: 0.7663895148548191\n",
    "                       pre       rec       spe        f1       geo       iba       sup\n",
    "\n",
    "              A       0.86      0.83      0.98      0.84      0.90      0.80      4291\n",
    "              B       0.86      0.85      0.97      0.86      0.91      0.82      6419\n",
    "              C       0.56      0.81      0.99      0.66      0.89      0.79       827\n",
    "              D       0.86      0.88      1.00      0.87      0.94      0.87      1034\n",
    "              E       0.84      0.71      0.99      0.77      0.84      0.68      2307\n",
    "              F       0.45      0.62      0.99      0.52      0.78      0.59       543\n",
    "              G       0.60      0.64      0.98      0.62      0.79      0.61      1353\n",
    "              H       0.00      0.00      1.00      0.00      0.00      0.00       126\n",
    "              I       0.60      0.73      0.99      0.66      0.85      0.70       740\n",
    "              J       0.79      0.71      0.99      0.75      0.84      0.69      1132\n",
    "              K       0.66      0.75      0.99      0.70      0.87      0.73       522\n",
    "              L       0.74      0.76      0.99      0.75      0.86      0.73      1537\n",
    "              M       0.93      0.85      1.00      0.89      0.92      0.83      1140\n",
    "              N       0.93      0.86      0.99      0.90      0.92      0.84      3925\n",
    "              O       0.51      0.59      0.99      0.55      0.77      0.57       409\n",
    "              P       0.49      0.72      0.95      0.58      0.83      0.67      2318\n",
    "              Q       0.27      0.18      0.99      0.22      0.42      0.17       436\n",
    "              R       0.34      0.21      1.00      0.26      0.46      0.19       257\n",
    "              S       0.88      0.75      0.99      0.81      0.86      0.72      3603\n",
    "              T       0.41      0.06      1.00      0.10      0.24      0.05       541\n",
    "              U       0.35      0.03      1.00      0.05      0.16      0.02       225\n",
    "              V       0.00      0.00      1.00      0.00      0.00      0.00        85\n",
    "              W       0.85      0.84      0.99      0.85      0.91      0.82      2038\n",
    "              X       0.52      0.83      0.98      0.64      0.90      0.80      1098\n",
    "              Y       0.89      0.87      1.00      0.88      0.93      0.86      1701\n",
    "\n",
    "    avg / total       0.77      0.77      0.98      0.76      0.86      0.74     38607\n",
    "\n",
    "    ???    \n",
    "                       pre       rec       spe        f1       geo       iba       sup\n",
    "\n",
    "              A       0.91      0.79      0.99      0.85      0.89      0.77      4291\n",
    "              B       0.82      0.89      0.96      0.86      0.93      0.85      6419\n",
    "              C       0.63      0.72      0.99      0.67      0.84      0.69       827\n",
    "              D       0.77      0.91      0.99      0.83      0.95      0.90      1034\n",
    "              E       0.71      0.83      0.98      0.77      0.90      0.80      2307\n",
    "              F       0.51      0.62      0.99      0.56      0.78      0.59       543\n",
    "              G       0.66      0.69      0.99      0.68      0.82      0.66      1353\n",
    "              H       0.38      0.06      1.00      0.11      0.25      0.06       126\n",
    "              I       0.68      0.73      0.99      0.70      0.85      0.70       740\n",
    "              J       0.81      0.72      0.99      0.76      0.84      0.69      1132\n",
    "              K       0.73      0.62      1.00      0.67      0.79      0.60       522\n",
    "              L       0.70      0.76      0.99      0.73      0.87      0.73      1537\n",
    "              M       0.87      0.89      1.00      0.88      0.94      0.88      1140\n",
    "              N       0.89      0.88      0.99      0.89      0.93      0.86      3925\n",
    "              O       0.62      0.64      1.00      0.63      0.80      0.61       409\n",
    "              P       0.59      0.62      0.97      0.61      0.78      0.58      2318\n",
    "              Q       0.43      0.26      1.00      0.33      0.51      0.24       436\n",
    "              R       0.49      0.29      1.00      0.36      0.54      0.27       257\n",
    "              S       0.83      0.79      0.98      0.81      0.88      0.76      3603\n",
    "              T       0.54      0.44      0.99      0.49      0.66      0.42       541\n",
    "              U       0.34      0.12      1.00      0.18      0.35      0.11       225\n",
    "              V       0.00      0.00      1.00      0.00      0.00      0.00        85\n",
    "              W       0.89      0.83      0.99      0.86      0.91      0.81      2038\n",
    "              X       0.68      0.71      0.99      0.70      0.84      0.69      1098\n",
    "              Y       0.84      0.89      0.99      0.86      0.94      0.87      1701\n",
    "\n",
    "    avg / total       0.78      0.78      0.98      0.78      0.87      0.75     38607\n",
    "    \n",
    "*epochs=4, no resampling*\n",
    "\n",
    "    Overall ACC: 0.7731499469008211\n",
    "                       pre       rec       spe        f1       geo       iba       sup\n",
    "\n",
    "              A       0.84      0.85      0.98      0.85      0.91      0.83      4291\n",
    "              B       0.87      0.85      0.97      0.86      0.91      0.82      6419\n",
    "              C       0.60      0.77      0.99      0.67      0.87      0.75       827\n",
    "              D       0.89      0.89      1.00      0.89      0.94      0.88      1034\n",
    "              E       0.77      0.79      0.99      0.78      0.88      0.76      2307\n",
    "              F       0.50      0.35      1.00      0.41      0.59      0.32       543\n",
    "              G       0.66      0.66      0.99      0.66      0.81      0.63      1353\n",
    "              H       0.00      0.00      1.00      0.00      0.00      0.00       126\n",
    "              I       0.59      0.69      0.99      0.64      0.83      0.66       740\n",
    "              J       0.78      0.71      0.99      0.74      0.84      0.68      1132\n",
    "              K       0.47      0.78      0.99      0.59      0.88      0.76       522\n",
    "              L       0.81      0.70      0.99      0.75      0.83      0.67      1537\n",
    "              M       0.92      0.86      1.00      0.89      0.93      0.85      1140\n",
    "              N       0.90      0.88      0.99      0.89      0.94      0.87      3925\n",
    "              O       0.57      0.47      1.00      0.52      0.69      0.45       409\n",
    "              P       0.53      0.67      0.96      0.59      0.80      0.62      2318\n",
    "              Q       0.40      0.16      1.00      0.23      0.40      0.15       436\n",
    "              R       0.33      0.10      1.00      0.16      0.32      0.09       257\n",
    "              S       0.75      0.83      0.97      0.79      0.90      0.79      3603\n",
    "              T       0.51      0.37      0.99      0.43      0.61      0.35       541\n",
    "              U       0.83      0.02      1.00      0.04      0.15      0.02       225\n",
    "              V       0.00      0.00      1.00      0.00      0.00      0.00        85\n",
    "              W       0.90      0.80      1.00      0.85      0.89      0.78      2038\n",
    "              X       0.62      0.78      0.99      0.69      0.87      0.75      1098\n",
    "              Y       0.88      0.89      0.99      0.89      0.94      0.88      1701\n",
    "\n",
    "    avg / total       0.77      0.77      0.98      0.77      0.86      0.75     38607"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save model for developing package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('../../output/major_group_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 46612, 100)        14090900  \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 46610, 512)        154112    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                16416     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 25)                425       \n",
      "=================================================================\n",
      "Total params: 14,263,709\n",
      "Trainable params: 172,809\n",
      "Non-trainable params: 14,090,900\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 46612, 100)        14090900  \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 46610, 512)        154112    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                16416     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 25)                425       \n",
      "=================================================================\n",
      "Total params: 14,263,709\n",
      "Trainable params: 172,809\n",
      "Non-trainable params: 14,090,900\n",
      "_________________________________________________________________\n",
      "None None\n"
     ]
    }
   ],
   "source": [
    "# Check the saved models.\n",
    "from keras.models import load_model\n",
    "with tf.device('/device:GPU:1'): # Specify the GPU# to run if OOM errors.\n",
    "    model_major_group=load_model('../../output/major_group_model.h5')\n",
    "print(model_major_group.summary(), model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
