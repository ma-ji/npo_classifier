{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:**\n",
    "- [x] Build NTEE-10 major groups.\n",
    "- [x] Vectorize output labels.\n",
    "- [x] Vectorize input texts.\n",
    "- [x] Spell check.\n",
    "- [x] Resampling - Add more records of 'VI' (International, Foreign Affairs - Q).\n",
    "- [x] Grid search. - Not feasible, need too much resources (i.e., memory, CPU).\n",
    "- [x] Spellcheck.\n",
    "- [x] Train/test dataset complete split.\n",
    "- [x] Solve reproducibility problem, [Ref1](https://github.com/keras-team/keras/issues/7676) [Ref2](https://github.com/keras-team/keras/issues/4875) [Ref3 (solution)](https://keras.io/getting-started/faq/#how-can-i-obtain-reproducible-results-using-keras-during-development)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /root/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /root/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# obtain reproducible results\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random as rn\n",
    "\n",
    "# The below is necessary for starting Numpy generated random numbers\n",
    "# in a well-defined initial state.\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# The below is necessary for starting core Python generated random numbers\n",
    "# in a well-defined state.\n",
    "\n",
    "rn.seed(12345)\n",
    "\n",
    "# Force TensorFlow to use single thread.\n",
    "# Multiple threads are a potential source of non-reproducible results.\n",
    "# For further details, see: https://stackoverflow.com/questions/42022950/\n",
    "\n",
    "session_conf = tf.ConfigProto(intra_op_parallelism_threads=1,\n",
    "                              inter_op_parallelism_threads=1)\n",
    "\n",
    "from keras import backend as K\n",
    "\n",
    "# The below tf.set_random_seed() will make random number generation\n",
    "# in the TensorFlow backend have a well-defined initial state.\n",
    "# For further details, see:\n",
    "# https://www.tensorflow.org/api_docs/python/tf/set_random_seed\n",
    "\n",
    "tf.set_random_seed(1234)\n",
    "\n",
    "sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
    "K.set_session(sess)\n",
    "\n",
    "# Rest of code follows ...\n",
    "\n",
    "# Check GPU device.\n",
    "print(K.tensorflow_backend._get_available_gpus())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://richliao.github.io/supervised/classification/2016/11/26/textclassifier-convolutional/\n",
    "#https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html\n",
    "#RNN\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from multiprocessing import Pool\n",
    "from spellchecker import SpellChecker\n",
    "import string\n",
    "import math\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "import keras.preprocessing.text as kpt\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Embedding\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk import word_tokenize\n",
    "\n",
    "# For encoding labels.\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read and compile tranining and validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(154424, 38607)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_file_path='../../dataset/UCF/train/'\n",
    "file_list=os.listdir(train_file_path)\n",
    "df_train=pd.DataFrame()\n",
    "for file in file_list:\n",
    "    df_train=pd.concat([df_train, pd.read_pickle(train_file_path+file, compression='gzip')])\n",
    "\n",
    "test_file_path='../../dataset/UCF/test/'\n",
    "file_list=os.listdir(test_file_path)\n",
    "df_test=pd.DataFrame()\n",
    "for file in file_list:\n",
    "    df_test=pd.concat([df_test, pd.read_pickle(test_file_path+file, compression='gzip')])\n",
    "    \n",
    "len(df_train), len(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "154424 25 9\n",
      "38607 25 9\n"
     ]
    }
   ],
   "source": [
    "# Code as 10 broad categories.\n",
    "broad_cat_dict={'I': ['A'],\n",
    "                'II': ['B'],\n",
    "                'III': ['C', 'D'],\n",
    "                'IV': ['E', 'F', 'G', 'H'],\n",
    "                'V': ['I', 'J', 'K', 'L', 'M', 'N', 'O', 'P'],\n",
    "                'VI': ['Q'],\n",
    "                'VII': ['R', 'S', 'T', 'U', 'V', 'W'],\n",
    "                'VIII': ['X'],\n",
    "                'IX': ['Y'],\n",
    "                'X': ['Z'],\n",
    "               }\n",
    "def ntee2cat(string):\n",
    "    global broad_cat_dict\n",
    "    return [s for s in broad_cat_dict.keys() if string in broad_cat_dict[s]][0]\n",
    "\n",
    "df_train['mission_prgrm_spellchk']=df_train['TAXPAYER_NAME']+' '+df_train['mission_spellchk']+' '+df_train['prgrm_dsc_spellchk'] # Using spell-checked.\n",
    "df_train['broad_cat']=df_train['NTEE1'].apply(ntee2cat)\n",
    "print(len(df_train['mission_prgrm_spellchk']), len(df_train['NTEE1'].drop_duplicates()), len(df_train['broad_cat'].drop_duplicates()))\n",
    "\n",
    "df_test['mission_prgrm_spellchk']=df_test['TAXPAYER_NAME']+' '+df_test['mission_spellchk']+' '+df_test['prgrm_dsc_spellchk'] # Using spell-checked.\n",
    "df_test['broad_cat']=df_test['NTEE1'].apply(ntee2cat)\n",
    "print(len(df_test['mission_prgrm_spellchk']), len(df_test['NTEE1'].drop_duplicates()), len(df_test['broad_cat'].drop_duplicates()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<s>\n",
    "\n",
    "```Python\n",
    "# Build training and testing data frame.\n",
    "small_num=0\n",
    "while small_num<500: # Make sure each category has at least 500 records.\n",
    "    sampleDF = df_train[df_train.mission.notna() & df_train.NTEE1.notna()].sample(120000)\n",
    "    trainDF, valDF =train_test_split(sampleDF, test_size=.3)\n",
    "    small_num=trainDF.groupby('NTEE_M').count().sort_values('EIN').iloc[0]['EIN']\n",
    "```\n",
    "\n",
    "</s>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "broad_cat\n",
      "I       0.110151\n",
      "II      0.167247\n",
      "III     0.048969\n",
      "IV      0.109025\n",
      "IX      0.042998\n",
      "V       0.302634\n",
      "VI      0.012867\n",
      "VII     0.176540\n",
      "VIII    0.029568\n",
      "Name: EIN, dtype: float64 \n",
      "\n",
      " broad_cat\n",
      "I       0.111146\n",
      "II      0.166265\n",
      "III     0.048204\n",
      "IV      0.112130\n",
      "IX      0.044059\n",
      "V       0.303650\n",
      "VI      0.011293\n",
      "VII     0.174813\n",
      "VIII    0.028440\n",
      "Name: EIN, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# See the composition by NTEE major groups.\n",
    "print(df_train.groupby('broad_cat').count()['EIN']/len(df_train), '\\n'*2, df_test.groupby('broad_cat').count()['EIN']/len(df_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare labels.\n",
    "*One hot encoding.* Prepare after resampling; otherwise, shape of `y_train` will shrink from 25 to 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "lb = preprocessing.LabelBinarizer()\n",
    "lb.fit(list(df_train.broad_cat.unique()))\n",
    "\n",
    "y_train=lb.transform(df_train['broad_cat'])\n",
    "# y_test=lb.transform(df_test['broad_cat']) # No need to transform Y for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# Save LabelBinarizer class for developing package.\n",
    "with open('../../output/lb_broad_cat.pkl', 'wb') as output:\n",
    "    pickle.dump(lb, output, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Not efficient codes, ignore._\n",
    "\n",
    "<s>\n",
    "    \n",
    "``` Python\n",
    "def one_hot(label_list, class_list):\n",
    "    int_encoder=LabelEncoder().fit(class_list) # Build the encoder.\n",
    "    label_int_encoded=int_encoder.transform(label_list) # One-dimensional integer encoded.\n",
    "    return np_utils.to_categorical(label_int_encoded) # Multi-dimensional binary/one-hot encoded.\n",
    "\n",
    "y_train=one_hot(label_list=df_train['NTEE1'], class_list=list(df_train.NTEE1.unique()))\n",
    "y_test=one_hot(label_list=df_test['NTEE1'], class_list=list(df_train.NTEE1.unique()))\n",
    "```\n",
    "\n",
    "### One-dimensional integer encoding for grid search.\n",
    "\n",
    "```Python\n",
    "def int_encode(label_list, class_list):\n",
    "    int_encoder=LabelEncoder().fit(class_list) # Build the encoder.\n",
    "    label_int_encoded=int_encoder.transform(label_list) # One-dimensional integer encoded.\n",
    "    return label_int_encoded # Multi-dimensional binary/one-hot encoded.\n",
    "\n",
    "y_train=int_encode(label_list=trainDF['NTEE_M'], class_list=list(major_group_dict.keys()))\n",
    "y_val=int_encode(label_list=valDF['NTEE_M'], class_list=list(major_group_dict.keys()))\n",
    "\n",
    "# Use integer encoding: https://github.com/keras-team/keras/issues/9331\n",
    "# Use sparse_categorical_crossentropy: https://keras.io/losses/\n",
    "```\n",
    "    \n",
    "</s>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare input text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Wrong codes, stop words not removed because capitalizations are not consistent**\n",
    "- If choose word embedding, no difference if not removed. \n",
    "- See reference: https://stackoverflow.com/questions/34721984/stopword-removing-when-using-the-word2vec/40447086#40447086\n",
    "\n",
    "<s>\n",
    "    \n",
    "```Python\n",
    "stop_list=stopwords.words('english')+list(string.punctuation)\n",
    "def stopwords_remove(string):\n",
    "    global stop_list\n",
    "    tokens=word_tokenize(string)\n",
    "    return [s for s in tokens if s not in stop_list]\n",
    "\n",
    "text_token_list_train=df_train['mission_prgrm_spellchk'].apply(stopwords_remove)\n",
    "text_token_list_test=df_test['mission_prgrm_spellchk'].apply(stopwords_remove)\n",
    "```\n",
    "</s>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_token_list_train=df_train['mission_prgrm_spellchk']\n",
    "text_token_list_test=df_test['mission_prgrm_spellchk']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Moved to preprocessing pipeline.**\n",
    "\n",
    "<s>\n",
    "\n",
    "```Python\n",
    "# Spell check function. Return corrected word if unknown; return original word if known.\n",
    "def spellcheck(word_string_list):\n",
    "    return [SpellChecker().correction(word=s).upper() for s in word_string_list]\n",
    "\n",
    "# Parallel computing\n",
    "p = Pool(48)\n",
    "text_token_list_train=p.map(spellcheck, text_token_list_train)\n",
    "text_token_list_val=p.map(spellcheck, text_token_list_val)\n",
    "# Pool.map keep the original order of data passed to map.\n",
    "# https://stackoverflow.com/questions/41273960/python-3-does-pool-keep-the-original-order-of-data-passed-to-map\n",
    "```\n",
    "\n",
    "</s>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('and', 1), ('the', 2), ('to', 3), ('of', 4), ('in', 5)]\n"
     ]
    }
   ],
   "source": [
    "# Build word index for train and validation texts.\n",
    "tokenizer=Tokenizer()\n",
    "tokenizer.fit_on_texts(text_token_list_train.to_list()+text_token_list_test.to_list())\n",
    "print(list(tokenizer.word_index.items())[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# Save tokenizer class for developing package.\n",
    "with open('../../output/tokenizer.pkl', 'wb') as output:\n",
    "    pickle.dump(tokenizer, output, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text to sequences.\n",
    "seq_encoding_text_train=tokenizer.texts_to_sequences(text_token_list_train)\n",
    "seq_encoding_text_test=tokenizer.texts_to_sequences(text_token_list_test)\n",
    "\n",
    "# Pads sequences to the same length (i.e., prepare matrix).\n",
    "x_train=pad_sequences(sequences=seq_encoding_text_train,\n",
    "                      maxlen=max([len(s) for s in seq_encoding_text_train]), # Max length of the sequence.\n",
    "                      dtype = \"int32\", padding = \"post\", truncating = \"post\", \n",
    "                      value = 0 # Zero is used for representing None or Unknown.\n",
    "                     )\n",
    "\n",
    "x_test=pad_sequences(sequences=seq_encoding_text_test,\n",
    "                    maxlen=max([len(s) for s in seq_encoding_text_train]), # Max length of the sequence.\n",
    "                    dtype = \"int32\", padding = \"post\", truncating = \"post\", \n",
    "                    value = 0 # Zero is used for representing None or Unknown.\n",
    "                     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Convert to Compressed Sparse Row matrix; otherwise, matrix too large, result memory error.\n",
    "# # https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html\n",
    "# from scipy.sparse import csr_matrix\n",
    "# x_train=csr_matrix(x_train)\n",
    "\n",
    "# # Define resample strategy.\n",
    "# def func_resample(method, sampling_strategy, x_train_vect, y_train):\n",
    "#     if method=='ADASYN':\n",
    "#         from imblearn.over_sampling import ADASYN\n",
    "#         resample = ADASYN(sampling_strategy=sampling_strategy, random_state=42)\n",
    "#     elif method=='RandomOverSampler':\n",
    "#         from imblearn.over_sampling import RandomOverSampler\n",
    "#         resample = RandomOverSampler(sampling_strategy=sampling_strategy, random_state=42)\n",
    "#     elif method=='SMOTE':\n",
    "#         from imblearn.over_sampling import SMOTE\n",
    "#         resample = SMOTE(sampling_strategy=sampling_strategy, random_state=42)\n",
    "#     elif method=='SMOTEENN':\n",
    "#         from imblearn.combine import SMOTEENN\n",
    "#         resample = SMOTEENN(sampling_strategy=sampling_strategy, random_state=42)\n",
    "#     elif method=='SMOTETomek':\n",
    "#         from imblearn.combine import SMOTETomek\n",
    "#         resample = SMOTETomek(sampling_strategy=sampling_strategy, random_state=42)\n",
    "#     x_train_vect_res, y_train_res = resample.fit_resample(x_train_vect, y_train)\n",
    "#     return [x_train_vect_res, y_train_res]\n",
    "\n",
    "# x_train_res, y_train_res = func_resample(method='ADASYN', sampling_strategy='minority', \n",
    "#                                          x_train_vect=x_train, y_train=y_train)\n",
    "\n",
    "x_train_res, y_train_res = [x_train, y_train]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Draft codes, ignore.**\n",
    "\n",
    "<s>\n",
    "\n",
    "```Python\n",
    "### Classifier: Not using pre-trained embedding.\n",
    "\n",
    "# https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/\n",
    "embedding_layer = Embedding(input_dim=len(tokenizer.word_index), # Size of vocabulary.\n",
    "                            input_length=max([len(s) for s in seq_encoding_text_train]), # Length of input, i.e., length of padded sequence.\n",
    "                            output_dim=32, # Size of the vector space in which words will be embedded.\n",
    "                           )\n",
    "\n",
    "from keras.layers import Input, Dense, Conv1D, MaxPooling1D, Flatten, BatchNormalization, GlobalMaxPooling1D, GRU, Dropout, LSTM\n",
    "from keras.models import Model\n",
    "\n",
    "sequence_input = Input(shape=(max([len(s) for s in seq_encoding_text_train]),), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "x = Flatten()(embedded_sequences)\n",
    "x = Dense(units=512, activation='relu')(x)\n",
    "x = Dense(units=128, activation='tanh')(x)\n",
    "preds = Dense(units=len(y_train[0]), activation='softmax')(x) #softmax\n",
    "\n",
    "model = Model(sequence_input, preds)\n",
    "\n",
    "model.summary()\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam', #'rmsprop',\n",
    "              metrics=['acc',precision, recall])\n",
    "\n",
    "history = model.fit(x_train, y_train, validation_split=0.8,\n",
    "#                     validation_data=(x_val, y_val),\n",
    "                    epochs=2, batch_size=50)\n",
    "\n",
    "# Add metrics.\n",
    "# https://stackoverflow.com/questions/43076609/how-to-calculate-precision-and-recall-in-keras\n",
    "import tensorflow as tf\n",
    "def as_keras_metric(method):\n",
    "    import functools\n",
    "    from keras import backend as K\n",
    "    import tensorflow as tf\n",
    "    @functools.wraps(method)\n",
    "    def wrapper(self, args, **kwargs):\n",
    "        \"\"\" Wrapper for turning tensorflow metrics into keras metrics \"\"\"\n",
    "        value, update_op = method(self, args, **kwargs)\n",
    "        K.get_session().run(tf.local_variables_initializer())\n",
    "        with tf.control_dependencies([update_op]):\n",
    "            value = tf.identity(value)\n",
    "        return value\n",
    "    return wrapper\n",
    "\n",
    "precision = as_keras_metric(tf.metrics.precision)\n",
    "recall = as_keras_metric(tf.metrics.recall)\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "\n",
    "# define the model\n",
    "model = Sequential()\n",
    "model.add(embedding_layer)\n",
    "model.add(Flatten())\n",
    "model.add(Dense(units=512, activation='sigmoid'))\n",
    "model.add(Dense(units=256, activation='sigmoid'))\n",
    "model.add(Dense(units=len(y_train[0]), activation='relu'))\n",
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc', precision, recall])\n",
    "# summarize the model\n",
    "print(model.summary())\n",
    "\n",
    "# fit the model\n",
    "# Batch size: https://stats.stackexchange.com/questions/153531/what-is-batch-size-in-neural-network\n",
    "history=model.fit(x_train, y_train, validation_split=0.3, epochs=25, verbose=1)\n",
    "\n",
    "# evaluate the model\n",
    "loss, accuracy = model.evaluate(x_val, y_val, verbose=1)\n",
    "print('Accuracy: %f' % (accuracy*100))\n",
    "```\n",
    "\n",
    "</s>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare embedding layer.\n",
    "Use pre-trained GloVe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "EMBEDDING_DIM=100\n",
    "glove_word_vector=api.load('glove-wiki-gigaword-'+str(EMBEDDING_DIM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((len(tokenizer.word_index)+1, EMBEDDING_DIM)) # Plus one: embedding matrix starts from 0, word index starts from 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word, index in tokenizer.word_index.items():\n",
    "    try:\n",
    "        embedding_matrix[index] = glove_word_vector.get_vector(word)\n",
    "    except:\n",
    "        pass\n",
    "        # words not found in embedding index will be all-zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/\n",
    "embedding_layer = Embedding(input_dim=len(tokenizer.word_index)+1, # Size of vocabulary.\n",
    "                            input_length=max([len(s) for s in seq_encoding_text_train]), # Length of input, i.e., length of padded sequence.\n",
    "                            output_dim=EMBEDDING_DIM, # Size of the vector space in which words will be embedded.\n",
    "                            weights=[embedding_matrix],\n",
    "                            trainable=False\n",
    "                           )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic tuning of training params."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, GlobalMaxPool1D, Conv1D\n",
    "\n",
    "with tf.device('/gpu:1'): # Specify which GPU to use.\n",
    "    # define the model\n",
    "    model = Sequential()\n",
    "    model.add(embedding_layer)\n",
    "    # model.add(Flatten())\n",
    "    model.add(Conv1D(128, 5, activation='softplus'))\n",
    "    model.add(GlobalMaxPool1D())\n",
    "    model.add(Dense(units=32, activation='sigmoid'))\n",
    "    model.add(Dense(units=32, activation='softplus'))\n",
    "    model.add(Dense(units=16, activation='tanh'))\n",
    "    model.add(Dense(units=16, activation='softplus'))\n",
    "    model.add(Dense(units=len(y_train[0]), activation='softmax'))\n",
    "    # compile the model\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc', \n",
    "    #                                                                      precision, recall\n",
    "                                                                        ])\n",
    "    # summarize the model\n",
    "    print(model.summary())\n",
    "\n",
    "    # fit the model\n",
    "    history=model.fit(x_train, y_train, validation_split=0.2, epochs=1, verbose=1)\n",
    "\n",
    "    '''\n",
    "    (10, sigmoid; 9, relu): loss: 0.4414 - acc: 0.8896 - precision: 0.1604 - recall: 0.8439 - val_loss: 0.3964 - val_acc: 0.8943 - val_precision: 0.1632 - val_recall: 0.8974\n",
    "    (10, softmax; 9, relu): loss: 0.5685 - acc: 0.8888 - precision: 0.1371 - recall: 0.8051 - val_loss: 0.5532 - val_acc: 0.8895 - val_precision: 0.1394 - val_recall: 0.8151\n",
    "    (10, relu;    9, relu): loss: 0.5377 - acc: 0.8838 - precision: 0.1646 - recall: 0.8411 - val_loss: 0.4271 - val_acc: 0.8903 - val_precision: 0.1558 - val_recall: 0.8884\n",
    "    (10, relu; 9, softmax): loss: 0.2596 - acc: 0.9037 - precision: 0.1110 - recall: 0.9992 - val_loss: 0.2303 - val_acc: 0.9135 - val_precision: 0.1111 - val_recall: 1.0000\n",
    "    (10, relu; 9, sigmoid): loss: 0.2681 - acc: 0.8975 - precision: 0.1110 - recall: 0.9992 - val_loss: 0.2272 - val_acc: 0.9121 - val_precision: 0.1111 - val_recall: 1.0000\n",
    "    (10, tanh; 9, sigmoid): loss: 0.2959 - acc: 0.8940 - precision: 0.1110 - recall: 0.9992 - val_loss: 0.2572 - val_acc: 0.9066 - val_precision: 0.1111 - val_recall: 1.0000\n",
    "    (10, relu;    9, tanh): loss: 0.4241 - acc: 0.8599 - precision: 0.1110 - recall: 0.9992 - val_loss: 0.3609 - val_acc: 0.8885 - val_precision: 0.1111 - val_recall: 1.0000\n",
    "\n",
    "    (32, relu; 16, tanh; 9, sigmoid): loss: 0.2590 - acc: 0.9034 - precision: 0.1110 - recall: 0.9992 - val_loss: 0.2186 - val_acc: 0.9180 - val_precision: 0.1111 - val_recall: 1.0000\n",
    "    (32, relu; 16, tanh; 9,    relu): loss: 0.5613 - acc: 0.8654 - precision: 0.1360 - recall: 0.7180 - val_loss: 0.3850 - val_acc: 0.8889 - val_precision: 0.1394 - val_recall: 0.8721\n",
    "    (32, relu; 16, relu; 9,    relu): loss: 0.3902 - acc: 0.8876 - precision: 0.1396 - recall: 0.9328 - val_loss: 0.3618 - val_acc: 0.8889 - val_precision: 0.1377 - val_recall: 0.9532\n",
    "    (32, softmax; 16, relu; 9, relu): loss: 0.6418 - acc: 0.8916 - precision: 0.1212 - recall: 0.7585 - val_loss: 0.6097 - val_acc: 0.8942 - val_precision: 0.1276 - val_recall: 0.7584\n",
    "    (32, sigmoid; 16, relu; 9, relu): loss: 0.5048 - acc: 0.8885 - precision: 0.1421 - recall: 0.7762 - val_loss: 0.3169 - val_acc: 0.8889 - val_precision: 0.1363 - val_recall: 0.8788\n",
    "\n",
    "    (32, sigmoid; 32, sigmoid; 16, relu; 16, relu; 9, relu): loss: 0.3325 - acc: 0.8879 - precision: 0.1251 - recall: 0.9766 - val_loss: 0.4644 - val_acc: 0.7910 - val_precision: 0.1257 - val_recall: 0.9785\n",
    "    (32, softmax; 32, softmax; 16, relu; 16, relu; 9, relu): loss: 1.1298 - acc: 0.8889 - precision: 0.0933 - recall: 0.4219 - val_loss: 1.1113 - val_acc: 0.8889 - val_precision: 0.0952 - val_recall: 0.4569\n",
    "\n",
    "    1. Don't use sigmoid/softmax/tanh for output layer.\n",
    "    2. Using relu near output layer increases loss but improve precision.\n",
    "    3. sigmoid/tanh/softmax decreases loss, but also decreases precision.\n",
    "    4. Possible strategy: use softmax near input layer, use relu near output layer.\n",
    "    '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try Grid Search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continue previous work.\n",
    "df_history=pd.read_csv('../../output/grid_search_history_broad_cat.tsv', sep='\\t', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_list_done=set(map(tuple, \n",
    "                        df_history[['conv_num_filters', 'conv_kernel_size', 'conv_act', 'out_act']].values.tolist()\n",
    "                       )\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, GlobalMaxPool1D, Conv1D\n",
    "from datetime import datetime\n",
    "\n",
    "# df_history=pd.DataFrame()\n",
    "for num_filters in [32, 64, 128]:\n",
    "    for kernel_size in [3,5,7]:\n",
    "        for conv_act in ['sigmoid', 'softplus', 'tanh', 'softmax']:\n",
    "            for out_act in ['sigmoid', 'softplus', 'tanh', 'softmax']:\n",
    "                param=tuple((num_filters, kernel_size, conv_act, out_act))\n",
    "                if param not in param_list_done:\n",
    "                    t1=datetime.now()\n",
    "                    # Run NN on a specified GPU.\n",
    "                    with tf.device('/device:GPU:0'):\n",
    "                        # define the model\n",
    "                        model = Sequential()\n",
    "                        model.add(embedding_layer)\n",
    "                        # model.add(Flatten())\n",
    "                        model.add(Conv1D(num_filters, kernel_size, activation=conv_act))\n",
    "                        model.add(GlobalMaxPool1D())\n",
    "                        model.add(Dense(units=32, activation='sigmoid'))\n",
    "                        model.add(Dense(units=32, activation='softplus'))\n",
    "                        model.add(Dense(units=16, activation='tanh'))\n",
    "                        model.add(Dense(units=16, activation='softplus'))\n",
    "                        model.add(Dense(units=len(y_train[0]), activation=out_act))\n",
    "                        # compile the model\n",
    "                        model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
    "                        # F1, precision, and recall removed. https://github.com/keras-team/keras/issues/5794\n",
    "                        # fit the model\n",
    "                        history=model.fit(x_train, y_train, validation_split=0.2, epochs=50, verbose=0)\n",
    "                        y_prob = model.predict(x_val, verbose=0)\n",
    "                    # Save history.\n",
    "                    acc = history.history['acc']\n",
    "                    val_acc = history.history['val_acc']\n",
    "                    loss = history.history['loss']\n",
    "                    val_loss = history.history['val_loss']\n",
    "                    epochs = range(1, len(acc) + 1)\n",
    "                    # Calculate on validation dataset.\n",
    "                    y_classes = y_prob.argmax(axis=-1)\n",
    "                    y_classes_prob=[s.max() for s in y_prob]\n",
    "                    y_classes_val=y_val.argmax(axis=-1)\n",
    "                    df_val=pd.DataFrame({'pred':y_classes, \n",
    "                                         'true':y_classes_val, \n",
    "                                         'prob':y_classes_prob})\n",
    "                    val_acc_real=len(df_val[df_val.pred==df_val.true])/len(df_val)\n",
    "                    # Save history to datafame.\n",
    "                    df_history_temp=pd.DataFrame()\n",
    "                    df_history_temp['acc']=acc\n",
    "                    df_history_temp['val_acc']=val_acc\n",
    "                    df_history_temp['val_acc_real']=[math.nan]*(len(epochs)-1)+[val_acc_real]\n",
    "                    df_history_temp['loss']=loss\n",
    "                    df_history_temp['val_loss']=val_loss\n",
    "                    df_history_temp['epochs']=epochs\n",
    "                    df_history_temp['conv_num_filters']=[num_filters]*len(epochs)\n",
    "                    df_history_temp['conv_kernel_size']=[kernel_size]*len(epochs)\n",
    "                    df_history_temp['conv_act']=[conv_act]*len(epochs)\n",
    "                    df_history_temp['out_act']=[out_act]*len(epochs)\n",
    "                    df_history_temp['time_stamp']=[str(t1)]+[math.nan]*(len(epochs)-2)+[str(datetime.now())]\n",
    "                    df_history=df_history.append(df_history_temp, ignore_index=True)\n",
    "                    df_history.to_csv('../../output/grid_search_history_broad_cat.tsv', sep='\\t')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Impossible to iterate all the parameter combinations; can run on GPU, but memory is not enough.**\n",
    "```Python\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "param_grid = dict(num_filters=[32, 64, 128],\n",
    "                  kernel_size=[3, 5, 7],\n",
    "                  activation_conv=['sigmoid', 'softplus', 'tanh', 'softmax'],\n",
    "                  activation_1=['sigmoid', 'softplus', 'tanh', 'softmax'],\n",
    "                  activation_2=['sigmoid', 'softplus', 'tanh', 'softmax'],\n",
    "                  activation_3=['sigmoid', 'softplus', 'tanh', 'softmax'],\n",
    "                  activation_4=['sigmoid', 'softplus', 'tanh', 'softmax'],\n",
    "                  neurons_1=[9, 18, 27, 36],\n",
    "                  neurons_2=[9, 18, 27, 36],\n",
    "                  neurons_3=[9, 18, 27, 36],\n",
    "                  neurons_4=[9, 18, 27, 36],\n",
    "                  epochs=range(1,110,10)\n",
    "                 )\n",
    "len(list(ParameterGrid(param_grid)))\n",
    "Out: 25952256\n",
    "    \n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, GlobalMaxPool1D, Conv1D\n",
    "import tensorflow as tf\n",
    "import random\n",
    "\n",
    "param_grid = dict(num_filters=[32, 64, 128],\n",
    "                  kernel_size=[3, 5, 7],\n",
    "                  activation_conv=['sigmoid', 'softplus', 'tanh', 'softmax'],\n",
    "                  activation_1=['sigmoid', 'softplus', 'tanh', 'softmax'],\n",
    "                  activation_2=['sigmoid', 'softplus', 'tanh', 'softmax'],\n",
    "                  activation_3=['sigmoid', 'softplus', 'tanh', 'softmax'],\n",
    "                  activation_4=['sigmoid', 'softplus', 'tanh', 'softmax'],\n",
    "                  neurons_1=[9, 18, 27, 36],\n",
    "                  neurons_2=[9, 18, 27, 36],\n",
    "                  neurons_3=[9, 18, 27, 36],\n",
    "                  neurons_4=[9, 18, 27, 36],\n",
    "                  epochs=range(1,110,10)\n",
    "                 )\n",
    "\n",
    "def create_model(num_filters, kernel_size, \n",
    "                 activation_conv, activation_1, activation_2, activation_3, activation_4,\n",
    "                 neurons_1, neurons_2, neurons_3, neurons_4,\n",
    "                ):\n",
    "    K.clear_session() # Release memory.\n",
    "    with tf.device('/gpu:0'): # Specify the GPU to use.\n",
    "        model = Sequential()\n",
    "        model.add(embedding_layer)\n",
    "        model.add(Conv1D(num_filters, kernel_size, activation='softplus'))\n",
    "        model.add(GlobalMaxPool1D())\n",
    "        model.add(Dense(units=32, activation='sigmoid'))\n",
    "        model.add(Dense(units=32, activation='softplus'))\n",
    "        model.add(Dense(units=16, activation='tanh'))\n",
    "        model.add(Dense(units=16, activation='softplus'))\n",
    "        model.add(Dense(#units=len(y_train[0]),\n",
    "                        units=1,\n",
    "                        activation='softmax'))\n",
    "        model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "        print(model.summary())\n",
    "        return model\n",
    "\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "model=KerasClassifier(build_fn=create_model, verbose=1)\n",
    "grid_search=GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=1, refit='f1_weighted', return_train_score=True,\n",
    "                         scoring=['accuracy', 'f1_weighted', 'precision_weighted', 'recall_weighted'],\n",
    "                         pre_dispatch=0, cv=5,\n",
    "                        )\n",
    "grid_search.fit(x_train, y_train, verbose=1)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision making: Optimizing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continue previous work.\n",
    "try:\n",
    "    df_history=pd.read_csv('../../output/grid_search_history_broad_cat_optimizing.tsv', sep='\\t', index_col=0)\n",
    "except:\n",
    "    df_history=pd.DataFrame(columns=pd.read_csv('../../output/grid_search_history_broad_cat.tsv', sep='\\t', index_col=0).columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_list_done=set(map(tuple, \n",
    "                        df_history[['conv_num_filters', 'conv_kernel_size', 'conv_act', 'out_act']].values.tolist()\n",
    "                       )\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, GlobalMaxPool1D, Conv1D\n",
    "from datetime import datetime\n",
    "\n",
    "# df_history=pd.DataFrame()\n",
    "for num_filters in [128, 256, 512, 1024]:\n",
    "    for kernel_size in [3]:\n",
    "        for conv_act in ['softplus']:\n",
    "            for out_act in ['sigmoid', 'softplus', 'softmax']:\n",
    "                param=tuple((num_filters, kernel_size, conv_act, out_act))\n",
    "                if param not in param_list_done:\n",
    "                    t1=datetime.now()\n",
    "                    # Run NN on a specified GPU.\n",
    "                    with tf.device('/device:GPU:0'):\n",
    "                        # define the model\n",
    "                        model = Sequential()\n",
    "                        model.add(embedding_layer)\n",
    "                        # model.add(Flatten())\n",
    "                        model.add(Conv1D(num_filters, kernel_size, activation=conv_act))\n",
    "                        model.add(GlobalMaxPool1D())\n",
    "                        model.add(Dense(units=32, activation='sigmoid'))\n",
    "                        model.add(Dense(units=32, activation='softplus'))\n",
    "                        model.add(Dense(units=16, activation='tanh'))\n",
    "                        model.add(Dense(units=16, activation='softplus'))\n",
    "                        model.add(Dense(units=len(y_train[0]), activation=out_act))\n",
    "                        # compile the model\n",
    "                        model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
    "                        # F1, precision, and recall removed. https://github.com/keras-team/keras/issues/5794\n",
    "                        # fit the model\n",
    "                        history=model.fit(x_train, y_train, validation_split=0.2, epochs=50, verbose=0)\n",
    "                        y_prob = model.predict(x_val, verbose=0)\n",
    "                    # Save history.\n",
    "                    acc = history.history['acc']\n",
    "                    val_acc = history.history['val_acc']\n",
    "                    loss = history.history['loss']\n",
    "                    val_loss = history.history['val_loss']\n",
    "                    epochs = range(1, len(acc) + 1)\n",
    "                    # Calculate on validation dataset.\n",
    "                    y_classes = y_prob.argmax(axis=-1)\n",
    "                    y_classes_prob=[s.max() for s in y_prob]\n",
    "                    y_classes_val=y_val.argmax(axis=-1)\n",
    "                    df_val=pd.DataFrame({'pred':y_classes, \n",
    "                                         'true':y_classes_val, \n",
    "                                         'prob':y_classes_prob})\n",
    "                    val_acc_real=len(df_val[df_val.pred==df_val.true])/len(df_val)\n",
    "                    # Save history to datafame.\n",
    "                    df_history_temp=pd.DataFrame()\n",
    "                    df_history_temp['acc']=acc\n",
    "                    df_history_temp['val_acc']=val_acc\n",
    "                    df_history_temp['val_acc_real']=[math.nan]*(len(epochs)-1)+[val_acc_real]\n",
    "                    df_history_temp['loss']=loss\n",
    "                    df_history_temp['val_loss']=val_loss\n",
    "                    df_history_temp['epochs']=epochs\n",
    "                    df_history_temp['conv_num_filters']=[num_filters]*len(epochs)\n",
    "                    df_history_temp['conv_kernel_size']=[kernel_size]*len(epochs)\n",
    "                    df_history_temp['conv_act']=[conv_act]*len(epochs)\n",
    "                    df_history_temp['out_act']=[out_act]*len(epochs)\n",
    "                    df_history_temp['time_stamp']=[str(t1)]+[math.nan]*(len(epochs)-2)+[str(datetime.now())]\n",
    "                    df_history=df_history.append(df_history_temp, ignore_index=True)\n",
    "                    df_history.to_csv('../../output/grid_search_history_broad_cat_optimizing.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and test model finalist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Best configuration**\n",
    "\n",
    "_Broad Category_\n",
    "\n",
    "|acc | val_acc | val_acc_real | loss | val_loss | epochs | conv_num_filters | conv_kernel_size | conv_act | out_act|\n",
    "|--|--|--|--|--|--|--|--|--|--|\n",
    "|0.820386905 | 0.776488095 | -- | 0.613613255 | 0.738004138 | 4 | 512 | 3 | softplus | softplus|\n",
    "\n",
    "_Major Group_\n",
    "\n",
    "|acc | val_acc | val_acc_real | loss | val_loss | epochs | conv_num_filters | conv_kernel_size | conv_act | out_act|\n",
    "|--|--|--|--|--|--|--|--|--|--|\n",
    "|0.764369048 | 0.710428571 | -- | 0.888776311 | 1.120441045 | 6 | 256 | 3 | softplus | softmax|\n",
    "|0.779 | 0.71 | -- | 0.834419103 | 1.140895644 | 7 | 256 | 3 | softplus | softplus|\n",
    "\n",
    "Use `softplus`+`softplus`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, GlobalMaxPool1D, Conv1D\n",
    "from datetime import datetime\n",
    "\n",
    "with tf.device('/device:GPU:0'):\n",
    "    # define the model\n",
    "    model = Sequential()\n",
    "    model.add(embedding_layer)\n",
    "    # model.add(Flatten())\n",
    "    model.add(Conv1D(512, 3, activation='softplus'))\n",
    "    model.add(GlobalMaxPool1D())\n",
    "    model.add(Dense(units=32, activation='sigmoid'))\n",
    "    model.add(Dense(units=32, activation='softplus'))\n",
    "    model.add(Dense(units=16, activation='tanh'))\n",
    "    model.add(Dense(units=16, activation='softplus'))\n",
    "    model.add(Dense(units=len(y_train[0]), activation='softplus'))\n",
    "    # compile the model\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
    "    # F1, precision, and recall removed. https://github.com/keras-team/keras/issues/5794\n",
    "    # fit the model\n",
    "    history=model.fit(x_train_res, y_train_res, validation_split=0.2, epochs=4, verbose=1)\n",
    "    print('Test on UCF-Testing')\n",
    "    y_prob = model.predict(x_test, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall ACC: 0.8323102028129614\n"
     ]
    }
   ],
   "source": [
    "# Calculate on validation dataset.\n",
    "# From probability --> serial coding, e.g., [4, 3, 2, 55] --> categorical, e.g., [[00010...], [001000..]...]\n",
    "y_train = lb.inverse_transform(np_utils.to_categorical(y_prob.argmax(axis=-1)))\n",
    "y_train_prob=[s.max() for s in y_prob]\n",
    "df_val=pd.DataFrame({'pred':y_train, \n",
    "                     'true':df_test['broad_cat'], \n",
    "                     'prob':y_train_prob})\n",
    "print('Overall ACC:', len(df_val[df_val.pred==df_val.true])/len(df_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   pre       rec       spe        f1       geo       iba       sup\n",
      "\n",
      "          I     0.8685    0.8481    0.9839    0.8582    0.9135    0.8231      4291\n",
      "         II     0.8458    0.8813    0.9680    0.8632    0.9236    0.8457      6419\n",
      "        III     0.7608    0.8958    0.9857    0.8228    0.9397    0.8750      1861\n",
      "         IV     0.7590    0.8729    0.9650    0.8120    0.9178    0.8346      4329\n",
      "         IX     0.9007    0.8530    0.9957    0.8762    0.9216    0.8372      1701\n",
      "          V     0.8522    0.8555    0.9353    0.8538    0.8945    0.7937     11723\n",
      "         VI     0.5932    0.0803    0.9994    0.1414    0.2832    0.0729       436\n",
      "        VII     0.8791    0.7592    0.9779    0.8148    0.8616    0.7262      6749\n",
      "       VIII     0.6538    0.7687    0.9881    0.7066    0.8715    0.7428      1098\n",
      "\n",
      "avg / total     0.8363    0.8347    0.9642    0.8316    0.8941    0.7947     38607\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from imblearn.metrics import classification_report_imbalanced\n",
    "print(classification_report_imbalanced(y_true=df_test['broad_cat'], y_pred=y_train, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Python\n",
    "from imblearn.metrics import classification_report_imbalanced\n",
    "print(classification_report_imbalanced(y_true=df_test['broad_cat'], y_pred=y_train))\n",
    "```\n",
    "**Chosen:** *epochs=4, no resampling.*\n",
    "\n",
    "    Overall ACC: 0.8346931903540808\n",
    "                       pre       rec       spe        f1       geo       iba       sup\n",
    "\n",
    "              I     0.8685    0.8481    0.9839    0.8582    0.9135    0.8231      4291\n",
    "             II     0.8458    0.8813    0.9680    0.8632    0.9236    0.8457      6419\n",
    "            III     0.7608    0.8958    0.9857    0.8228    0.9397    0.8750      1861\n",
    "             IV     0.7590    0.8729    0.9650    0.8120    0.9178    0.8346      4329\n",
    "             IX     0.9007    0.8530    0.9957    0.8762    0.9216    0.8372      1701\n",
    "              V     0.8522    0.8555    0.9353    0.8538    0.8945    0.7937     11723\n",
    "             VI     0.5932    0.0803    0.9994    0.1414    0.2832    0.0729       436\n",
    "            VII     0.8791    0.7592    0.9779    0.8148    0.8616    0.7262      6749\n",
    "           VIII     0.6538    0.7687    0.9881    0.7066    0.8715    0.7428      1098\n",
    "\n",
    "    avg / total     0.8363    0.8347    0.9642    0.8316    0.8941    0.7947     38607\n",
    "\n",
    "*epochs=2, resampling: method='ADASYN', sampling_strategy='minority'.*\n",
    "\n",
    "    Overall ACC: 0.8268707747299713\n",
    "                       pre       rec       spe        f1       geo       iba       sup\n",
    "\n",
    "              I       0.93      0.77      0.99      0.84      0.87      0.74      4291\n",
    "             II       0.87      0.85      0.97      0.86      0.91      0.81      6419\n",
    "            III       0.81      0.85      0.99      0.83      0.92      0.83      1861\n",
    "             IV       0.81      0.82      0.98      0.81      0.89      0.78      4329\n",
    "             IX       0.90      0.85      1.00      0.88      0.92      0.84      1701\n",
    "              V       0.84      0.87      0.93      0.85      0.90      0.80     11723\n",
    "             VI       0.31      0.48      0.99      0.37      0.69      0.45       436\n",
    "            VII       0.78      0.81      0.95      0.80      0.88      0.76      6749\n",
    "           VIII       0.73      0.69      0.99      0.71      0.83      0.67      1098\n",
    "\n",
    "    avg / total       0.83      0.83      0.96      0.83      0.89      0.78     38607\n",
    "\n",
    "*epochs=4, resampling: method='ADASYN', sampling_strategy='minority'.*\n",
    "\n",
    "    Overall ACC: 0.819670008029632\n",
    "                       pre       rec       spe        f1       geo       iba       sup\n",
    "\n",
    "              I       0.83      0.87      0.98      0.85      0.92      0.84      4291\n",
    "             II       0.91      0.78      0.99      0.84      0.88      0.76      6419\n",
    "            III       0.83      0.82      0.99      0.82      0.90      0.80      1861\n",
    "             IV       0.88      0.70      0.99      0.78      0.83      0.67      4329\n",
    "             IX       0.80      0.92      0.99      0.85      0.95      0.90      1701\n",
    "              V       0.77      0.90      0.88      0.83      0.89      0.80     11723\n",
    "             VI       0.44      0.21      1.00      0.29      0.46      0.20       436\n",
    "            VII       0.83      0.79      0.97      0.81      0.87      0.75      6749\n",
    "           VIII       0.71      0.70      0.99      0.71      0.83      0.68      1098\n",
    "\n",
    "    avg / total       0.82      0.82      0.95      0.82      0.88      0.77     38607\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save model for developing package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('../../output/broad_category_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 46612, 100)        14090900  \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 46610, 512)        154112    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                16416     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 9)                 153       \n",
      "=================================================================\n",
      "Total params: 14,263,437\n",
      "Trainable params: 172,537\n",
      "Non-trainable params: 14,090,900\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 46612, 100)        14090900  \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 46610, 512)        154112    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                16416     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 9)                 153       \n",
      "=================================================================\n",
      "Total params: 14,263,437\n",
      "Trainable params: 172,537\n",
      "Non-trainable params: 14,090,900\n",
      "_________________________________________________________________\n",
      "None None\n"
     ]
    }
   ],
   "source": [
    "# Check the saved models.\n",
    "from keras.models import load_model\n",
    "model_broad_cat=load_model('../../output/broad_category_model.h5')\n",
    "print(model_broad_cat.summary(), model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Draft, ignore hereafter, save as reference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEWCAYAAACXGLsWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzt3Xl41OW5//H3LfsmIKAiiEHFBTAsRsSqgEst1ApuKAgqVova2lptz5GjtiqWU6tWEcvxJ3VrFUWq1VLrWqXiViWsCkhBDBBBCCgIgkvg/v3xfJNMwiQzWSYzST6v65pr5rvfM4G551m+z2PujoiISEX2SncAIiKS+ZQsREQkISULERFJSMlCREQSUrIQEZGElCxERCQhJQupFWbWyMy2m1m3mtw3nczsUDOr8b7nZnaqmeXFLC83sxOT2bcK13rAzK6v6vEVnPc3ZvZITZ9X0qdxugOQzGRm22MWWwJfA7ui5cvdfXplzufuu4DWNb1vQ+Duh9fEeczsMmCsuw+JOfdlNXFuqf+ULCQudy/+so5+uV7m7v8sb38za+zuhbURm4jUPlVDSZVE1QxPmtkTZrYNGGtmx5nZv81si5mtN7MpZtYk2r+xmbmZZUXLj0XbXzCzbWb2jpl1r+y+0fZhZvYfM9tqZvea2VtmNq6cuJOJ8XIzW2lmn5vZlJhjG5nZ3Wa22cw+AoZW8PncaGYzyqybamZ3Ra8vM7Nl0fv5KPrVX9658s1sSPS6pZk9GsW2BDg6znVXReddYmbDo/VHAX8AToyq+DbFfLY3xxx/RfTeN5vZs2bWOZnPJhEzOzOKZ4uZvWZmh8dsu97M1pnZF2b2Ycx7HWhm86P1G8zsjmSvJyng7nroUeEDyANOLbPuN8A3wBmEHx0tgGOAYwkl1oOB/wBXRfs3BhzIipYfAzYBOUAT4EngsSrsuy+wDRgRbbsW+BYYV857SSbGvwFtgSzgs6L3DlwFLAG6Ah2AOeG/UNzrHAxsB1rFnHsjkBMtnxHtY8DJwE4gO9p2KpAXc658YEj0+k7gX0B74CBgaZl9zwM6R3+TC6IY9ou2XQb8q0ycjwE3R69Pi2LsCzQH/g94LZnPJs77/w3wSPT6yCiOk6O/0fXR594E6AWsBvaP9u0OHBy9nguMjl63AY5N9/+FhvxQyUKq4013/7u773b3ne4+193fdfdCd18FTAMGV3D8U+6e6+7fAtMJX1KV3fcHwEJ3/1u07W5CYokryRh/6+5b3T2P8MVcdK3zgLvdPd/dNwO3VXCdVcAHhCQG8F1gi7vnRtv/7u6rPHgNeBWI24hdxnnAb9z9c3dfTSgtxF53pruvj/4mjxMSfU4S5wUYAzzg7gvd/StgAjDYzLrG7FPeZ1ORUcAsd38t+hvdBuxNSNqFhMTUK6rK/Dj67CAk/R5m1sHdt7n7u0m+D0kBJQupjrWxC2Z2hJn9w8w+NbMvgIlAxwqO/zTm9Q4qbtQub98DYuNwdyf8Eo8ryRiTuhbhF3FFHgdGR68vICS5ojh+YGbvmtlnZraF8Ku+os+qSOeKYjCzcWa2KKru2QIckeR5Iby/4vO5+xfA50CXmH0q8zcr77y7CX+jLu6+HPgF4e+wMarW3D/a9RKgJ7DczN4zs+8n+T4kBZQspDrKdhu9n/Br+lB33xv4NaGaJZXWE6qFADAzo/SXW1nViXE9cGDMcqKuvU8Cp0a/zEcQkgdm1gJ4CvgtoYqoHfByknF8Wl4MZnYwcB9wJdAhOu+HMedN1M13HaFqq+h8bQjVXZ8kEVdlzrsX4W/2CYC7P+buxxOqoBoRPhfcfbm7jyJUNf4eeNrMmlczFqkiJQupSW2ArcCXZnYkcHktXPM5oL+ZnWFmjYGrgU4pinEm8HMz62JmHYDrKtrZ3TcAbwIPA8vdfUW0qRnQFCgAdpnZD4BTKhHD9WbWzsJ9KFfFbGtNSAgFhLx5GaFkUWQD0LWoQT+OJ4BLzSzbzJoRvrTfcPdyS2qViHm4mQ2Jrv1fhHamd83sSDM7Kbrezuixi/AGLjSzjlFJZGv03nZXMxapIiULqUm/AC4mfBHcT/hlnVLRF/L5wF3AZuAQYAHhvpCajvE+QtvC+4TG16eSOOZxQoP14zExbwGuAZ4hNBKfS0h6ybiJUMLJA14A/hxz3sXAFOC9aJ8jgNh6/leAFcAGM4utTio6/kVCddAz0fHdCO0Y1eLuSwif+X2ERDYUGB61XzQDbie0M31KKMncGB36fWCZhd52dwLnu/s31Y1HqsZCFa9I/WBmjQjVHue6+xvpjkekvlDJQuo8MxtqZm2jqoxfEXrYvJfmsETqFSULqQ9OAFYRqjKGAme6e3nVUCJSBaqGEhGRhFSyEBGRhOrNQIIdO3b0rKysdIchIlKnzJs3b5O7V9TdHKhHySIrK4vc3Nx0hyEiUqeYWaKRCABVQ4mISBKULEREJCElCxERSajetFmISO369ttvyc/P56uvvkp3KJKE5s2b07VrV5o0KW9osIopWYhIleTn59OmTRuysrIIg/1KpnJ3Nm/eTH5+Pt27d098QBwNvhpq+nTIyoK99grP06cnOkJEAL766is6dOigRFEHmBkdOnSoVimwQZcspk+H8eNhx46wvHp1WAYYU+2xNkXqPyWKuqO6f6sGXbK44YaSRFFkx46wXkRESjToZLFmTeXWi0jm2Lx5M3379qVv377sv//+dOnSpXj5m2+Sm/bikksuYfny5RXuM3XqVKbXUP30CSecwMKFC2vkXLWtQVdDdesWqp7irReRmjV9eii1r1kT/o9NmlS96t4OHToUf/HefPPNtG7dml/+8pel9nF33J299or/u/jhhx9OeJ2f/OQnVQ+yHmnQJYtJk6Bly9LrWrYM60Wk5hS1D65eDe4l7YOp6FCycuVKevfuzRVXXEH//v1Zv34948ePJycnh169ejFx4sTifYt+6RcWFtKuXTsmTJhAnz59OO6449i4cSMAN954I5MnTy7ef8KECQwYMIDDDz+ct99+G4Avv/ySc845hz59+jB69GhycnISliAee+wxjjrqKHr37s31118PQGFhIRdeeGHx+ilTpgBw991307NnT/r06cPYsWNr/DNLRoNOFmPGwLRpcNBBYBaep01T47ZITavt9sGlS5dy6aWXsmDBArp06cJtt91Gbm4uixYt4pVXXmHp0qV7HLN161YGDx7MokWLOO6443jooYfintvdee+997jjjjuKE8+9997L/vvvz6JFi5gwYQILFiyoML78/HxuvPFGZs+ezYIFC3jrrbd47rnnmDdvHps2beL999/ngw8+4KKLLgLg9ttvZ+HChSxatIg//OEP1fx0qqZBJwsIiSEvD3bvDs9KFCI1r7bbBw855BCOOeaY4uUnnniC/v37079/f5YtWxY3WbRo0YJhw4YBcPTRR5OXlxf33GefffYe+7z55puMGjUKgD59+tCrV68K43v33Xc5+eST6dixI02aNOGCCy5gzpw5HHrooSxfvpyrr76al156ibZt2wLQq1cvxo4dy/Tp06t8U111NfhkISKpV147YKraB1u1alX8esWKFdxzzz289tprLF68mKFDh8a936Bp06bFrxs1akRhYWHcczdr1myPfSo7iVx5+3fo0IHFixdzwgknMGXKFC6//HIAXnrpJa644gree+89cnJy2LVrV6WuVxOULEQk5dLZPvjFF1/Qpk0b9t57b9avX89LL71U49c44YQTmDlzJgDvv/9+3JJLrIEDBzJ79mw2b95MYWEhM2bMYPDgwRQUFODujBw5kltuuYX58+eza9cu8vPzOfnkk7njjjsoKChgR9k6vVrQoHtDiUjtKKrercneUMnq378/PXv2pHfv3hx88MEcf/zxNX6Nn/70p1x00UVkZ2fTv39/evfuXVyFFE/Xrl2ZOHEiQ4YMwd0544wzOP3005k/fz6XXnop7o6Z8bvf/Y7CwkIuuOACtm3bxu7du7nuuuto06ZNjb+HROrNHNw5OTmuyY9Eas+yZcs48sgj0x1GRigsLKSwsJDmzZuzYsUKTjvtNFasWEHjxpn1ezze38zM5rl7TqJjM+udiIjUQdu3b+eUU06hsLAQd+f+++/PuERRXfXr3YiIpEG7du2YN29eusNIKTVwi4hIQkoWIiKSUEqThZkNNbPlZrbSzCbE2X6tmS01s8Vm9qqZHRSz7WIzWxE9Lk5lnCIiUrGUJQszawRMBYYBPYHRZtazzG4LgBx3zwaeAm6Pjt0HuAk4FhgA3GRm7VMVq4iIVCyVJYsBwEp3X+Xu3wAzgBGxO7j7bHcvurvk30DX6PX3gFfc/TN3/xx4BRiawlhFpI4ZMmTIHjfYTZ48mR//+McVHte6dWsA1q1bx7nnnlvuuRN1xZ88eXKpm+O+//3vs2XLlmRCr9DNN9/MnXfeWe3z1LRUJosuwNqY5fxoXXkuBV6o4rEi0sCMHj2aGTNmlFo3Y8YMRo8endTxBxxwAE899VSVr182WTz//PO0a9euyufLdKlMFvHm8It7B6CZjQVygDsqc6yZjTezXDPLLSgoqHKgIlL3nHvuuTz33HN8/fXXAOTl5bFu3TpOOOGE4vse+vfvz1FHHcXf/va3PY7Py8ujd+/eAOzcuZNRo0aRnZ3N+eefz86dO4v3u/LKK4uHN7/pppsAmDJlCuvWreOkk07ipJNOAiArK4tNmzYBcNddd9G7d2969+5dPLx5Xl4eRx55JD/60Y/o1asXp512WqnrxLNw4UIGDhxIdnY2Z511Fp9//nnx9Xv27El2dnbxAIavv/568eRP/fr1Y9u2bVX+bONJ5X0W+cCBMctdgXVldzKzU4EbgMHu/nXMsUPKHPuvsse6+zRgGoQ7uGsiaBGpvJ//HGp6Ari+fSH6no2rQ4cODBgwgBdffJERI0YwY8YMzj//fMyM5s2b88wzz7D33nuzadMmBg4cyPDhw8udh/q+++6jZcuWLF68mMWLF9O/f//ibZMmTWKfffZh165dnHLKKSxevJif/exn3HXXXcyePZuOHTuWOte8efN4+OGHeffdd3F3jj32WAYPHkz79u1ZsWIFTzzxBH/84x8577zzePrppyucn+Kiiy7i3nvvZfDgwfz617/mlltuYfLkydx22218/PHHNGvWrLjq684772Tq1Kkcf/zxbN++nebNm1fi004slSWLuUAPM+tuZk2BUcCs2B3MrB9wPzDc3TfGbHoJOM3M2kcN26dF60REisVWRcVWQbk7119/PdnZ2Zx66ql88sknbNiwodzzzJkzp/hLOzs7m+zs7OJtM2fOpH///vTr148lS5YkHCTwzTff5KyzzqJVq1a0bt2as88+mzfeeAOA7t2707dvX6DiYdAhzK+xZcsWBg8eDMDFF1/MnDlzimMcM2YMjz32WPGd4scffzzXXnstU6ZMYcuWLTV+B3nKShbuXmhmVxG+5BsBD7n7EjObCOS6+yxCtVNr4C9Rxl/j7sPd/TMzu5WQcAAmuvtnqYpVRKqnohJAKp155plce+21zJ8/n507dxaXCKZPn05BQQHz5s2jSZMmZGVlxR2WPFa8UsfHH3/MnXfeydy5c2nfvj3jxo1LeJ6KxtsrGt4cwhDniaqhyvOPf/yDOXPmMGvWLG699VaWLFnChAkTOP3003n++ecZOHAg//znPzniiCOqdP54Unqfhbs/7+6Hufsh7j4pWvfrKFHg7qe6+37u3jd6DI859iF3PzR6JJ4oV0QanNatWzNkyBB++MMflmrY3rp1K/vuuy9NmjRh9uzZrF69usLzDBo0iOnRHK8ffPABixcvBsLw5q1ataJt27Zs2LCBF154ofiYNm3axG0XGDRoEM8++yw7duzgyy+/5JlnnuHEE0+s9Htr27Yt7du3Ly6VPProowwePJjdu3ezdu1aTjrpJG6//Xa2bNnC9u3b+eijjzjqqKO47rrryMnJ4cMPP6z0NSuisaFEpE4bPXo0Z599dqmeUWPGjOGMM84gJyeHvn37JvyFfeWVV3LJJZeQnZ1N3759GTBgABBmvevXrx+9evXaY3jz8ePHM2zYMDp37szs2bOL1/fv359x48YVn+Oyyy6jX79+FVY5ledPf/oTV1xxBTt27ODggw/m4YcfZteuXYwdO5atW7fi7lxzzTW0a9eOX/3qV8yePZtGjRrRs2fP4ln/aoqGKBeRKtEQ5XVPdYYo19hQIiKSkJKFiIgkpGQhIlVWX6qxG4Lq/q2ULESkSpo3b87mzZuVMOoAd2fz5s3VulFPvaFEpEq6du1Kfn4+GmqnbmjevDldu3ZNvGM5lCxEpEqaNGlC9+7d0x2G1BJVQ4mISEJKFiIikpCShYiIJKRkISIiCSlZiIhIQkoWIiKSkJKFiIgkpGQhIiIJKVmIiEhCShYiIpKQkoWIiCSkZCEiIgkpWYiISEIpTRZmNtTMlpvZSjObEGf7IDObb2aFZnZumW23m9kSM1tmZlPMzFIZq4iIlC9lycLMGgFTgWFAT2C0mfUss9saYBzweJljvwMcD2QDvYFjgMGpilVERCqWyvksBgAr3X0VgJnNAEYAS4t2cPe8aNvuMsc60BxoChjQBNiQwlhFRKQCqayG6gKsjVnOj9Yl5O7vALOB9dHjJXdfVnY/MxtvZrlmlqvZukREUieVySJeG0NSk/Wa2aHAkUBXQoI52cwG7XEy92nunuPuOZ06dapWsCIiUr5UJot84MCY5a7AuiSPPQv4t7tvd/ftwAvAwBqOT0REkpTKZDEX6GFm3c2sKTAKmJXksWuAwWbW2MyaEBq396iGEhGR2pGyZOHuhcBVwEuEL/qZ7r7EzCaa2XAAMzvGzPKBkcD9ZrYkOvwp4CPgfWARsMjd/56qWEVEpGLmnlQzQsbLycnx3NzcdIchIlKnmNk8d89JtJ/u4BYRkYSULEREJCElCxERSUjJQkREElKyEBGRhJQsREQkISULEakz3GHyZDjiCLjsMnj5ZSgsTHdUDYOShYjUCdu3w+jRcM010KoVzJwJ3/se7L8/XH45vPqqEkcqKVmISMb7z39g4ED4y1/gttsgNxc2boRnnoHTToPp0+HUU+GAA+DKK+Ff/4Jdu9Iddf2iZCEiGe3ZZyEnBzZsCNVO110HZtC8OZx5Jjz+OBQUwNNPw0knwZ//HJ67dIGrroI5c5Q4aoKShYhkpF274Prr4ayzQhvFvHlwyinx923RAs4+G558MpQ4Zs6EE0+Ehx6CwYPhwAPh6qvhrbdgd9mp1iQpShYiknE2bYJhw+C3v4Uf/SiUDrp1S+7YVq1g5MhQZbVxIzzxRKjCuv9+OOGEcJ5rroF33gkN5pIcDSQoIhklNxfOOSdUO02dCpdeWjPn3bYN/v73UOp44QX45puQOEaOhPPOg2OOCdVbDY0GEhSROufBB8Ovf4A336y5RAHQpg1ccEFoA9m4MbRtZGfDlClw7LFw8MGhPWTePJU44lGyEJG0++qrUN102WUwaFD4ws5J+Fu36tq2hQsvDCWNjRvhkUfgyCPhrrvCdQ89FP7nf2DhQiWOIkoWIpJWa9aExugHHggN2i+8AB071t7127WDiy+G558PVV8PPgg9esAdd0C/fnD44XDjjbB4ccNOHGqzEJG0+ec/YdQo+PbbUC00YkS6IyqxaVO4j2PmTHjttdCL6ogjQvvGeedBr17pjrBmJNtmoWQhIrXOHX73O7jhhlD989e/wmGHpTuq8hXdADhzZrjhb/du6NkTzjgjJJAePULV1b771r1GciULEclIX3wB48aFL9/zzw/VT61bpzuq5G3YEG4AfPLJcN9G7A1/bdqEpFGUPHr0yPxEomQhIhln6dJwk91HH8Gdd4Yb5TLxCzRZ334LeXmwciWsWFH6+eOPEyeSoud0JpJkk0XjFAcxFLgHaAQ84O63ldk+CJgMZAOj3P2pmG3dgAeAAwEHvu/ueTUd4zffhDs/f/rTMCiZiKTGzJnwwx+GUsRrr4VeT3VdkyYlpYdhw0pv+/ZbWL06JI/YRDJ/fiiZ1IVEEitlycLMGgFTge8C+cBcM5vl7ktjdlsDjAN+GecUfwYmufsrZtYaSMlN+p98En7lDB0KF10Uus516JCKK4k0TIWF4f6Fu+6C73wn3Fl9wAHpjir1mjQJX/iHHlpxIilKIpmeSFJZshgArHT3VQBmNgMYARQni6KSgpmVSgRm1hNo7O6vRPttT1WQ3bvDggUwaVIYzfLFF+Hee8NdnZmQzUXqsg0bQrvE66+HQf1+/3to2jTdUaVfbCIpK14iWbmy4kQyaFCY5yOVUpksugBrY5bzgWOTPPYwYIuZ/RXoDvwTmODupcaONLPxwHiAbskOHBNH8+Zw660hQVx6afjHPX06/N//hZErRaTy3nkHzj0XPv8cHn0Uxo5Nd0R1Q1USyfaU/ZwukcpkEe93ebKt6Y2BE4F+hKqqJwnVVQ+WOpn7NGAahAbuqgZaJDs7/AO/5x741a9C17g77gh3le6l2xdFkuIO990HP/95GO31nXegT590R1U/VJRIUi2VX4H5hMbpIl2BdZU4doG7r3L3QuBZoH8NxxdX48bwi1/A++/D0UeHGbhOPjlkcBGp2I4doVvsT34SJiXKzVWiqC9SmSzmAj3MrLuZNQVGAbMqcWx7M+sULZ9MTFtHbTjkkDBN4wMPhPFhsrPh9ts1baNIeVatCg3Yjz4Kt9wCs2ZB+/bpjkpqSsqSRVQiuAp4CVgGzHT3JWY20cyGA5jZMWaWD4wE7jezJdGxuwg9pF41s/cJVVp/TFWs5TELbRhLl4beDNddF0anXLiwtiMRyWzPPx9K4qtXwz/+Ab/+tapu6xvdlFcJTz8ditebNsF//3f4D9G8eUovKZLRdu8OnUNuuSVUNz39dBjqW+oOzWeRAuecE0oZF10UZvDq0wfeeCPdUYmkx+efw/DhcPPNYbjvt95SoqjPlCwqaZ99wry+r7wSurENGgRXXhnGuxFpKBYtCvM+vPxy6GL+yCPQsmW6o5JUUrKoolNPDT2mrrkGpk0LwxU/91y6oxJJvcceg+OOCxMWvf56+LGkG1jrPyWLamjVKgxh8PbbYQKVM84I0zYWFKQ7MpHqcw/zVq9YEapbZ84Ms9ldeCEMGBDuKD7uuHRHKbUlpQMJNhTHHhumgbztNvjNb0LRfPJkGDNGv7gk83zzTZif4dNPSx7r15deLnrs2LHn8ddeG+aiaKxvjwZFvaFq2JIl4Y7vf/87dLf9f/8PqjESiUhS3OGzz+J/4ZdNBJs3xz/HPvvA/vvv+ejcueT1AQeE/aT+yIghyhuiXr3gzTdh6tQw4XuvXqHn1I9/rH7nUjm7d8OWLaGrdkFBGJSvvESwYUPocFFW8+YlX/aHHRY6ZMRLBvvuC82a1f57lLpDJYsUyssLw4W8/HK4s/WBB8IUktIw7dxZ8sUf+1ze682bS48wWsQsfLnH++Vf9rH33qoKlYqpZJEBsrLCkOePPhoGVevbNwxQeN11YUAwqbt27QrVPhV92ZddF6/+H0KJs0MH6NgROnUKczoXve7YseRRlAA6dlR7gdQ+lSxqyYYN8LOfhR4l2dnw4IOhn7pkrtWr4a9/hWXL9vzi/+yz0E4QT+vWpb/sY7/0471u1w4aNard9yZSpEZLFmZ2CJDv7l+b2RDCNKh/dvct1Quz4dhvvzDB+wUXhPaLY48N92hMnKibmTLJmjXw1FMhqb/7bli3334lX+xHHbXnl33ZEoCGgJH6KKmShZktBHKALMLAgLOAw939+ymNrhIyvWQRa+vWMLbUtGlheIQ//jEMgy7pUZQg/vKX0IsNoF8/OO+8MCHWIYekNz6RVKrpsaF2R6PIngVMdvdrgM7VCbAha9sW7r8fZs8O9dWnnBKmRzQL7RzTp6c7wvpv7Vq4++7Q8eCgg8IcJl9/Df/7vyVzIU+YoEQhUiTZZrJvzWw0cDFwRrROTbTVNGQIXH89jB9fMi3i6tVw8cWh1HHMMaHhc599Sh6xy61aqadLZeTnl5Qg3n47rOvbNySIkSPTM/uYSF2RbLK4BLgCmOTuH5tZd+Cx1IXVcNxyy54TKu3aFUbwnDs3dLcsT9Om5SeSipYbUpL55JOSNoiiBNGnD0yaFBJEjx7pjU+krqh0bygzaw8c6O6LUxNS1dSlNotYe+0Vv1eNWbgpa+fOMBT05s2hB07RI3Y53rbKJJmyiWXffcMNXIcfHl7XtcTyySdhXoWZM0PShdADragN4rDD0hufSCap6d5Q/wKGR/svBArM7HV3v7ZaUQrduoWqp3jrAVq0CI8DDqjceYuSTHnJJHY5Ly/U0X/22Z73ArRtG5JG0eOII8LzoYdmVq+fdetKJwj3kCBuvTUkiMMPT3eEInVbstVQbd39CzO7DHjY3W8ys4wqWdRVkyaFNovYL+mWLcP66qhqkvnqqzCExH/+A8uXlzxmzw43FxYpaoyPl0g6d66d0sj69SUJ4s03Q4I46qjQHVkJQqRmJZssGptZZ+A84IYUxtPgjBkTnm+4IXTh7NYtJIqi9bWteXPo3j08vve90tu2b98ziSxfDnPmlE52bdqUVGPFJpIePap/T8mnn5YkiDfeCAmid+/Q9jNyZLiOiNS8ZO+zGAn8CnjL3a80s4OBO9z9nFQHmKy62mZRH+zeHdoJyiaR5ctDAoz9J9atW+kkUpRIunQpf6DFTz8Nd1LPnBkSU1GCGDkyPDTelkjVJdtmkdLhPsxsKHAP0Ah4wN1vK7N9EDCZcEf4KHd/qsz2vYFlwDPuflVF11KyyEw7d4b7Fj78cM9Esm1byX4tW+5ZGtmyJXRzff31kCB69SpJED17pu89idQnNd3A3RW4FzgecOBN4Gp3z6/gmEbAVOC7QD4w18xmufvSmN3WAOOAX5ZzmluB15OJUTJTixahoTk7u/R691BiWL68dCKZOzckiN27w349e8JNNylBiKRbsm0WDwOPAyOj5bHRuu9WcMwAYKW7rwIwsxnACKA4Wbh7XrRtd9mDzexoYD/gRcJQI1KPmIWG8M6dw82Jsb7+GlauDCOrqpFaJDMkO9xHJ3d/2N0Lo8cjQKcEx3QB1saMSQjvAAAM/0lEQVQs50frEjKzvYDfA/+VYL/xZpZrZrkFmvi63mjWLFQ5KVGIZI5kk8UmMxtrZo2ix1ignMkZi8XrPJlsA8mPgefdfW1FO7n7NHfPcfecTp0S5S4REamqZKuhfgj8Abib8IX/NmEIkIrkAwfGLHcF1iV5veOAE83sx0BroKmZbXf3CUkeLyIiNSipZOHuawh3cBczs58TejKVZy7QIxpH6hNgFHBBktcrvsvAzMYBOUoUIiLpk2w1VDwVDvURDWl+FWH+i2XATHdfYmYTzWw4gJkdY2b5hIbz+81sSTXiERGRFKnyfRZmttbdD0y8Z+3QfRYiIpVX05MfxVM/Ju8WEZGEKmyzMLNtxE8KBrRISUQiIpJxKkwW7t6mtgIREZHMVZ1qKBERaSCULEREJCElCxERSUjJQkREElKyEBGRhJQsREQkISULKTZ9OmRlhelNs7LCsogIJD/qrNRz06fD+PGwY0dYXr06LAOMGVP+cSLSMKhkIQDccENJoiiyY0dYLyKiZCEArFlTufUi0rAoWQgA3bpVbr2INCxKFgLApEnQsmXpdS1bhvUiIkoWAoRG7GnT4KCDwCw8T5umxm0RCdQbSoqNGaPkICLxqWQhIiIJKVmIiEhCShYiIpJQSpOFmQ01s+VmttLMJsTZPsjM5ptZoZmdG7O+r5m9Y2ZLzGyxmZ2fyjhFRKRiKUsWZtYImAoMA3oCo82sZ5nd1gDjgMfLrN8BXOTuvYChwGQza5eqWEVEpGKp7A01AFjp7qsAzGwGMAJYWrSDu+dF23bHHuju/4l5vc7MNgKdgC0pjFdERMqRymqoLsDamOX8aF2lmNkAoCnwUZxt480s18xyCwoKqhyoiIhULJXJwuKs80qdwKwz8ChwibvvLrvd3ae5e46753Tq1KmKYYqISCKpTBb5wIExy12BdckebGZ7A/8AbnT3f9dwbCIiUgmpTBZzgR5m1t3MmgKjgFnJHBjt/wzwZ3f/SwpjlAykSZhEMk/KkoW7FwJXAS8By4CZ7r7EzCaa2XAAMzvGzPKBkcD9ZrYkOvw8YBAwzswWRo++qYpVMkfRJEyrV4N7ySRMShgi6WXulWpGyFg5OTmem5ub7jCkmrKyQoIo66CDIC+vtqMRqf/MbJ675yTaT3dwS0bRJEwimUnJQjKKJmESyUxKFpJRNAmTSGZSspCMokmYRDKTJj+SjKNJmEQyj0oWIiKSkJKFiIgkpGQhIiIJKVmIiEhCShYiIpKQkoWIiCSkZCFSDo1+K1JC91mIxFE0+u2OHWG5aPRb0D0g0jCpZCESxw03lCSKIjt2hPUiDZGShUgcGv1WpDQlC5E4NPqtSGlKFiJxaPRbkdKULETi0Oi3IqWpN5RIOTT6rUgJlSxERCShlCYLMxtqZsvNbKWZTYizfZCZzTezQjM7t8y2i81sRfS4OJVxiohIxVKWLMysETAVGAb0BEabWc8yu60BxgGPlzl2H+Am4FhgAHCTmbVPVawiIlKxVJYsBgAr3X2Vu38DzABGxO7g7nnuvhjYXebY7wGvuPtn7v458AowNIWxiohIBVKZLLoAa2OW86N1NXasmY03s1wzyy0oKKhyoCKZTGNUSSZIZbKwOOu8Jo9192nunuPuOZ06dapUcCJ1QdEYVatXg3vJGFVKGFLbUpks8oEDY5a7Autq4ViRekNjVEmmSGWymAv0MLPuZtYUGAXMSvLYl4DTzKx91LB9WrROpEHRGFWSKVKWLNy9ELiK8CW/DJjp7kvMbKKZDQcws2PMLB8YCdxvZkuiYz8DbiUknLnAxGidSIOiMaokU5h7ss0ImS0nJ8dzc3PTHYZIjSo7rwaEMao09IjUFDOb5+45ifbTHdwiGUxjVEmm0NhQIhlOY1RJJlDJQkREElKyEBGRhJQsREQkISULEUmKhh1p2NTALSIJle3CWzTsCKjxvaFQyUJEEtKwI6JkISIJadgRUbIQkYQ07IgoWYhIQpMmhWFGYrVsGdZLw6BkISIJadgRUW8oEUmKhh1p2FSyEJE6Q/d6pI9KFiJSJ+hej/RSyUJE6gTd65FeShYiUifoXo/0UrIQkTpB93qkl5KFiNQJutcjvZQsRKRO0L0e6ZXSZGFmQ81suZmtNLMJcbY3M7Mno+3vmllWtL6Jmf3JzN43s2Vm9j+pjFNE6oYxYyAvD3bvDs/pShQNsQtvypKFmTUCpgLDgJ7AaDPrWWa3S4HP3f1Q4G7gd9H6kUAzdz8KOBq4vCiRiIikU1EX3tWrwb2kC299TxipLFkMAFa6+yp3/waYAYwos88I4E/R66eAU8zMAAdamVljoAXwDfBFCmMVEUlKQ+3Cm8pk0QVYG7OcH62Lu4+7FwJbgQ6ExPElsB5YA9zp7p+VvYCZjTezXDPLLSgoqPl3ICJSRkPtwpvKZGFx1nmS+wwAdgEHAN2BX5jZwXvs6D7N3XPcPadTp07VjVdEJKGG2oU3lckiHzgwZrkrsK68faIqp7bAZ8AFwIvu/q27bwTeAnJSGKuISFIaahfeVCaLuUAPM+tuZk2BUcCsMvvMAi6OXp8LvObuTqh6OtmCVsBA4MMUxioikpSG2oU3ZckiaoO4CngJWAbMdPclZjbRzIZHuz0IdDCzlcC1QFH32qlAa+ADQtJ52N0XpypWEZHKaIhdeC38kK/7cnJyPDc3N91hiIjUirKj8EKoDqtsKcfM5rl7wmp+3cEtIlIH1XYXXiULEZE6qLa78CpZiIjUQbXdhVfJQkSkDqrtLrxKFiIidVBtd+HVHNwiInXUmDG1121XJQsREUlIyUJERBJSshARkYSULEREJCElCxERSajejA1lZgXA6nTHUU0dgU3pDiKD6PMoTZ9HCX0WpVXn8zjI3RNOCFRvkkV9YGa5yQzo1VDo8yhNn0cJfRal1cbnoWooERFJSMlCREQSUrLILNPSHUCG0edRmj6PEvosSkv556E2CxERSUglCxERSUjJQkREElKyyABmdqCZzTazZWa2xMyuTndM6WZmjcxsgZk9l+5Y0s3M2pnZU2b2YfRv5Lh0x5ROZnZN9P/kAzN7wsyapzum2mRmD5nZRjP7IGbdPmb2ipmtiJ7b1/R1lSwyQyHwC3c/EhgI/MTMeqY5pnS7GliW7iAyxD3Ai+5+BNCHBvy5mFkX4GdAjrv3BhoBo9IbVa17BBhaZt0E4FV37wG8Gi3XKCWLDODu6919fvR6G+HLoEt6o0ofM+sKnA48kO5Y0s3M9gYGAQ8CuPs37r4lvVGlXWOghZk1BloC69IcT61y9znAZ2VWjwD+FL3+E3BmTV9XySLDmFkW0A94N72RpNVk4L+B3ekOJAMcDBQAD0fVcg+YWat0B5Uu7v4JcCewBlgPbHX3l9MbVUbYz93XQ/jxCexb0xdQssggZtYaeBr4ubt/ke540sHMfgBsdPd56Y4lQzQG+gP3uXs/4EtSUMVQV0R18SOA7sABQCszG5veqBoGJYsMYWZNCIliurv/Nd3xpNHxwHAzywNmACeb2WPpDSmt8oF8dy8qaT5FSB4N1anAx+5e4O7fAn8FvpPmmDLBBjPrDBA9b6zpCyhZZAAzM0Kd9DJ3vyvd8aSTu/+Pu3d19yxCw+Vr7t5gfzm6+6fAWjM7PFp1CrA0jSGl2xpgoJm1jP7fnEIDbvCPMQu4OHp9MfC3mr5A45o+oVTJ8cCFwPtmtjBad727P5/GmCRz/BSYbmZNgVXAJWmOJ23c/V0zewqYT+hFuIAGNvSHmT0BDAE6mlk+cBNwGzDTzC4lJNSRNX5dDfchIiKJqBpKREQSUrIQEZGElCxERCQhJQsREUlIyUJERBJSshBJwMx2mdnCmEeN3UFtZlmxo4eKZCrdZyGS2E5375vuIETSSSULkSoyszwz+52ZvRc9Do3WH2Rmr5rZ4ui5W7R+PzN7xswWRY+iYSoamdkfozkaXjazFtH+PzOzpdF5ZqTpbYoAShYiyWhRphrq/JhtX7j7AOAPhNFyiV7/2d2zgenAlGj9FOB1d+9DGN9pSbS+BzDV3XsBW4BzovUTgH7Rea5I1ZsTSYbu4BZJwMy2u3vrOOvzgJPdfVU0EOSn7t7BzDYBnd3922j9enfvaGYFQFd3/zrmHFnAK9GkNZjZdUATd/+Nmb0IbAeeBZ519+0pfqsi5VLJQqR6vJzX5e0Tz9cxr3dR0pZ4OjAVOBqYF032I5IWShYi1XN+zPM70eu3KZnqcwzwZvT6VeBKKJ5jfO/yTmpmewEHuvtswkRQ7YA9SjcitUW/VEQSaxEzGjCE+bCLus82M7N3CT+8RkfrfgY8ZGb/RZjlrmiU2KuBadHIoLsIiWN9OddsBDxmZm0BA+7WdKqSTmqzEKmiqM0ix903pTsWkVRTNZSIiCSkkoWIiCSkkoWIiCSkZCEiIgkpWYiISEJKFiIikpCShYiIJPT/ASqnjpON5DwBAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "# \"bo\" is for \"blue dot\"\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "# b is for \"solid blue line\"\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()   # clear figure\n",
    "history_dict = history.history\n",
    "acc_values = history_dict['acc']\n",
    "val_acc_values = history_dict['val_acc']\n",
    "\n",
    "plt.plot(epochs, acc_values, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc_values, 'b', label='Validation acc')\n",
    "\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36000/36000 [==============================] - 51s 1ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7514166666666666"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''https://machinelearningmastery.com/how-to-make-classification-and-regression-predictions-for-deep-learning-models-in-keras/\n",
    "In the case of a two-class (binary) classification problem, the sigmoid activation function is often used in the output layer. \n",
    "The predicted probability is taken as the likelihood of the observation belonging to class 1, or inverted (1 – probability) to give the probability for class 0.\n",
    "In the case of a multi-class classification problem, the softmax activation function \n",
    "is often used on the output layer and the likelihood of the observation for each class is returned as a vector.\n",
    "'''\n",
    "\n",
    "y_prob = model.predict(x_val, verbose=1)\n",
    "y_classes = y_prob.argmax(axis=-1)\n",
    "y_classes_prob=[s.max() for s in y_prob]\n",
    "y_classes_val=y_val.argmax(axis=-1)\n",
    "\n",
    "df_val=pd.DataFrame({'pred':y_classes, \n",
    "                     'true':y_classes_val, \n",
    "                     'prob':y_classes_prob})\n",
    "len(df_val[df_val.pred==df_val.true])/len(df_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120000/120000 [==============================] - 118s 980us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8065"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''https://machinelearningmastery.com/how-to-make-classification-and-regression-predictions-for-deep-learning-models-in-keras/\n",
    "In the case of a two-class (binary) classification problem, the sigmoid activation function is often used in the output layer. \n",
    "The predicted probability is taken as the likelihood of the observation belonging to class 1, or inverted (1 – probability) to give the probability for class 0.\n",
    "In the case of a multi-class classification problem, the softmax activation function \n",
    "is often used on the output layer and the likelihood of the observation for each class is returned as a vector.\n",
    "'''\n",
    "\n",
    "y_prob = model.predict(x_val, verbose=1)\n",
    "y_classes = y_prob.argmax(axis=-1)\n",
    "y_classes_prob=[s.max() for s in y_prob]\n",
    "y_classes_val=y_val.argmax(axis=-1)\n",
    "\n",
    "df_val=pd.DataFrame({'pred':y_classes, \n",
    "                     'true':y_classes_val, \n",
    "                     'prob':y_classes_prob})\n",
    "len(df_val[df_val.pred==df_val.true])/len(df_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9097281678082192 0.5191111111111111\n"
     ]
    }
   ],
   "source": [
    "df_95=df_val[df_val.prob>.95]\n",
    "print(len(df_95[df_95.pred==df_95.true])/len(df_95), len(df_95)/len(df_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred</th>\n",
       "      <th>true</th>\n",
       "      <th>prob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6838</th>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0.739892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16717</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.981657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9961</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.988655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15989</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.500976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34186</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.821712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26873</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.976101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33774</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.981773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25580</th>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>0.836229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18159</th>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0.980795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21771</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.995547</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       pred  true      prob\n",
       "6838      3     5  0.739892\n",
       "16717     1     1  0.981657\n",
       "9961      3     3  0.988655\n",
       "15989     0     1  0.500976\n",
       "34186     5     5  0.821712\n",
       "26873     1     1  0.976101\n",
       "33774     1     1  0.981773\n",
       "25580     7     7  0.836229\n",
       "18159     1     7  0.980795\n",
       "21771     3     3  0.995547"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_val.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"8\" halign=\"left\">prob</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pred</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14574.0</td>\n",
       "      <td>0.947898</td>\n",
       "      <td>0.140834</td>\n",
       "      <td>0.181094</td>\n",
       "      <td>0.994786</td>\n",
       "      <td>0.997717</td>\n",
       "      <td>0.998336</td>\n",
       "      <td>0.999913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20047.0</td>\n",
       "      <td>0.938727</td>\n",
       "      <td>0.150843</td>\n",
       "      <td>0.199948</td>\n",
       "      <td>0.982048</td>\n",
       "      <td>0.996910</td>\n",
       "      <td>0.998982</td>\n",
       "      <td>0.999559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6813.0</td>\n",
       "      <td>0.937554</td>\n",
       "      <td>0.145990</td>\n",
       "      <td>0.230144</td>\n",
       "      <td>0.983532</td>\n",
       "      <td>0.995575</td>\n",
       "      <td>0.997143</td>\n",
       "      <td>0.999125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13946.0</td>\n",
       "      <td>0.919894</td>\n",
       "      <td>0.163432</td>\n",
       "      <td>0.202485</td>\n",
       "      <td>0.950082</td>\n",
       "      <td>0.994085</td>\n",
       "      <td>0.997833</td>\n",
       "      <td>0.999361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4788.0</td>\n",
       "      <td>0.915918</td>\n",
       "      <td>0.158874</td>\n",
       "      <td>0.223830</td>\n",
       "      <td>0.934571</td>\n",
       "      <td>0.991566</td>\n",
       "      <td>0.997457</td>\n",
       "      <td>0.999467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>34993.0</td>\n",
       "      <td>0.951979</td>\n",
       "      <td>0.125865</td>\n",
       "      <td>0.173467</td>\n",
       "      <td>0.987138</td>\n",
       "      <td>0.997629</td>\n",
       "      <td>0.998319</td>\n",
       "      <td>0.999602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2456.0</td>\n",
       "      <td>0.769482</td>\n",
       "      <td>0.228869</td>\n",
       "      <td>0.182271</td>\n",
       "      <td>0.593661</td>\n",
       "      <td>0.862013</td>\n",
       "      <td>0.967078</td>\n",
       "      <td>0.996433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>18602.0</td>\n",
       "      <td>0.923707</td>\n",
       "      <td>0.159183</td>\n",
       "      <td>0.198981</td>\n",
       "      <td>0.957083</td>\n",
       "      <td>0.996431</td>\n",
       "      <td>0.998265</td>\n",
       "      <td>0.999453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3781.0</td>\n",
       "      <td>0.864114</td>\n",
       "      <td>0.213472</td>\n",
       "      <td>0.189253</td>\n",
       "      <td>0.806864</td>\n",
       "      <td>0.985537</td>\n",
       "      <td>0.997941</td>\n",
       "      <td>0.999207</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         prob                                                              \\\n",
       "        count      mean       std       min       25%       50%       75%   \n",
       "pred                                                                        \n",
       "0     14574.0  0.947898  0.140834  0.181094  0.994786  0.997717  0.998336   \n",
       "1     20047.0  0.938727  0.150843  0.199948  0.982048  0.996910  0.998982   \n",
       "2      6813.0  0.937554  0.145990  0.230144  0.983532  0.995575  0.997143   \n",
       "3     13946.0  0.919894  0.163432  0.202485  0.950082  0.994085  0.997833   \n",
       "4      4788.0  0.915918  0.158874  0.223830  0.934571  0.991566  0.997457   \n",
       "5     34993.0  0.951979  0.125865  0.173467  0.987138  0.997629  0.998319   \n",
       "6      2456.0  0.769482  0.228869  0.182271  0.593661  0.862013  0.967078   \n",
       "7     18602.0  0.923707  0.159183  0.198981  0.957083  0.996431  0.998265   \n",
       "8      3781.0  0.864114  0.213472  0.189253  0.806864  0.985537  0.997941   \n",
       "\n",
       "                \n",
       "           max  \n",
       "pred            \n",
       "0     0.999913  \n",
       "1     0.999559  \n",
       "2     0.999125  \n",
       "3     0.999361  \n",
       "4     0.999467  \n",
       "5     0.999602  \n",
       "6     0.996433  \n",
       "7     0.999453  \n",
       "8     0.999207  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_val.groupby('pred')[['prob']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"8\" halign=\"left\">prob</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pred</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3878.0</td>\n",
       "      <td>0.893846</td>\n",
       "      <td>0.169222</td>\n",
       "      <td>0.200335</td>\n",
       "      <td>0.890413</td>\n",
       "      <td>0.978879</td>\n",
       "      <td>0.989682</td>\n",
       "      <td>0.994728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5504.0</td>\n",
       "      <td>0.870570</td>\n",
       "      <td>0.164746</td>\n",
       "      <td>0.188946</td>\n",
       "      <td>0.826522</td>\n",
       "      <td>0.955931</td>\n",
       "      <td>0.977011</td>\n",
       "      <td>0.985090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1998.0</td>\n",
       "      <td>0.871377</td>\n",
       "      <td>0.178440</td>\n",
       "      <td>0.190420</td>\n",
       "      <td>0.817443</td>\n",
       "      <td>0.967236</td>\n",
       "      <td>0.989169</td>\n",
       "      <td>0.995268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4295.0</td>\n",
       "      <td>0.879368</td>\n",
       "      <td>0.177278</td>\n",
       "      <td>0.184492</td>\n",
       "      <td>0.840099</td>\n",
       "      <td>0.973080</td>\n",
       "      <td>0.992297</td>\n",
       "      <td>0.997352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1517.0</td>\n",
       "      <td>0.836238</td>\n",
       "      <td>0.184050</td>\n",
       "      <td>0.189743</td>\n",
       "      <td>0.744091</td>\n",
       "      <td>0.923662</td>\n",
       "      <td>0.979553</td>\n",
       "      <td>0.990551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10663.0</td>\n",
       "      <td>0.871253</td>\n",
       "      <td>0.171818</td>\n",
       "      <td>0.206576</td>\n",
       "      <td>0.822933</td>\n",
       "      <td>0.959143</td>\n",
       "      <td>0.986311</td>\n",
       "      <td>0.994159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>343.0</td>\n",
       "      <td>0.457318</td>\n",
       "      <td>0.129805</td>\n",
       "      <td>0.175460</td>\n",
       "      <td>0.357263</td>\n",
       "      <td>0.448693</td>\n",
       "      <td>0.557864</td>\n",
       "      <td>0.725925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>6602.0</td>\n",
       "      <td>0.823981</td>\n",
       "      <td>0.209800</td>\n",
       "      <td>0.192370</td>\n",
       "      <td>0.682801</td>\n",
       "      <td>0.936526</td>\n",
       "      <td>0.990312</td>\n",
       "      <td>0.997062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1200.0</td>\n",
       "      <td>0.692921</td>\n",
       "      <td>0.200447</td>\n",
       "      <td>0.202602</td>\n",
       "      <td>0.532967</td>\n",
       "      <td>0.748837</td>\n",
       "      <td>0.869758</td>\n",
       "      <td>0.951971</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         prob                                                              \\\n",
       "        count      mean       std       min       25%       50%       75%   \n",
       "pred                                                                        \n",
       "0      3878.0  0.893846  0.169222  0.200335  0.890413  0.978879  0.989682   \n",
       "1      5504.0  0.870570  0.164746  0.188946  0.826522  0.955931  0.977011   \n",
       "2      1998.0  0.871377  0.178440  0.190420  0.817443  0.967236  0.989169   \n",
       "3      4295.0  0.879368  0.177278  0.184492  0.840099  0.973080  0.992297   \n",
       "4      1517.0  0.836238  0.184050  0.189743  0.744091  0.923662  0.979553   \n",
       "5     10663.0  0.871253  0.171818  0.206576  0.822933  0.959143  0.986311   \n",
       "6       343.0  0.457318  0.129805  0.175460  0.357263  0.448693  0.557864   \n",
       "7      6602.0  0.823981  0.209800  0.192370  0.682801  0.936526  0.990312   \n",
       "8      1200.0  0.692921  0.200447  0.202602  0.532967  0.748837  0.869758   \n",
       "\n",
       "                \n",
       "           max  \n",
       "pred            \n",
       "0     0.994728  \n",
       "1     0.985090  \n",
       "2     0.995268  \n",
       "3     0.997352  \n",
       "4     0.990551  \n",
       "5     0.994159  \n",
       "6     0.725925  \n",
       "7     0.997062  \n",
       "8     0.951971  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_val.groupby('pred')[['prob']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_13 (InputLayer)        (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "embedding_4 (Embedding)      (None, 1000, 200)         6692600   \n",
      "_________________________________________________________________\n",
      "flatten_13 (Flatten)         (None, 200000)            0         \n",
      "_________________________________________________________________\n",
      "dense_38 (Dense)             (None, 512)               102400512 \n",
      "_________________________________________________________________\n",
      "dense_39 (Dense)             (None, 128)               65664     \n",
      "_________________________________________________________________\n",
      "dense_40 (Dense)             (None, 9)                 1161      \n",
      "=================================================================\n",
      "Total params: 109,159,937\n",
      "Trainable params: 102,467,337\n",
      "Non-trainable params: 6,692,600\n",
      "_________________________________________________________________\n",
      "Train on 8399 samples, validate on 33601 samples\n",
      "Epoch 1/2\n",
      "8399/8399 [==============================] - 125s 15ms/step - loss: 0.2489 - acc: 0.9057 - precision: 0.1104 - recall: 0.9940 - val_loss: 0.2185 - val_acc: 0.9155 - val_precision: 0.1111 - val_recall: 1.0000\n",
      "Epoch 2/2\n",
      "8399/8399 [==============================] - 121s 14ms/step - loss: 0.1594 - acc: 0.9394 - precision: 0.1111 - recall: 1.0000 - val_loss: 0.2231 - val_acc: 0.9167 - val_precision: 0.1111 - val_recall: 1.0000\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Dense, Conv1D, MaxPooling1D, Flatten, BatchNormalization, GlobalMaxPooling1D, GRU, Dropout, LSTM\n",
    "from keras.models import Model\n",
    "\n",
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "'''\n",
    "x = GRU(units=128, activation='tanh', return_sequences=True)(embedded_sequences)\n",
    "\n",
    "x = LSTM(units=256, activation='tanh', return_sequences=False)(embedded_sequences)\n",
    "x = Dropout(0.2)(x)\n",
    "\n",
    "#x = LSTM(units=128, activation='tanh', return_sequences=True)(x)\n",
    "#x = Dropout(0.2)(x)\n",
    "'''\n",
    "# x = Dropout(0.2)(x)\n",
    "x = Flatten()(embedded_sequences)\n",
    "x = Dense(units=512, activation='relu')(x)\n",
    "x = Dense(units=128, activation='tanh')(x)\n",
    "preds = Dense(units=9, activation='softmax')(x) #softmax\n",
    "\n",
    "# x = Dense(units=512, activation='relu')(x)\n",
    "# x = Dense(units=128, activation='relu')(x)\n",
    "# preds = Dense(units=25, activation='sigmoid')(x) #softmax\n",
    "\n",
    "model = Model(sequence_input, preds)\n",
    "\n",
    "model.summary()\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam', #'rmsprop',\n",
    "              metrics=['acc',precision, recall])\n",
    "\n",
    "history = model.fit(x_train, y_train, validation_split=0.8,\n",
    "#                     validation_data=(x_val, y_val),\n",
    "                    epochs=2, batch_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 25s 412us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.19112927243113517, 0.9295742606123288, 0.1111111119389534, 1.0]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score = model.evaluate(x_val, y_val, batch_size=500, verbose=1)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37694"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_val[df_val.pred==df_val.true])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 7, 5, 3, 4, 2])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_95.pred.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
