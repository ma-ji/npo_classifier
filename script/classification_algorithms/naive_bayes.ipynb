{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection, preprocessing, naive_bayes, metrics\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn import decomposition, ensemble\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "229472"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test reading file.\n",
    "import os\n",
    "file_list=os.listdir('../../dataset/df_train.pkl.gz/')\n",
    "df_train=pd.DataFrame()\n",
    "for file in file_list:\n",
    "    df_train=pd.concat([df_train, \n",
    "                        pd.read_pickle('../../dataset/df_train.pkl.gz/'+file, compression='gzip')])\n",
    "len(df_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare parrallel envionment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipyparallel as ipp\n",
    "c = ipp.Client()\n",
    "print(c.ids)\n",
    "dview = c[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dview.execute('from sklearn import model_selection, preprocessing, naive_bayes, metrics')\n",
    "dview.execute('from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer')\n",
    "dview.execute('from sklearn import decomposition, ensemble')\n",
    "dview.execute('from nltk.stem import PorterStemmer')\n",
    "dview.execute('from nltk import word_tokenize')\n",
    "dview.execute('from nltk.stem import WordNetLemmatizer')\n",
    "dview.execute('import pandas as pd')\n",
    "dview['df_train']=df_train\n",
    "dview['df_performance']=pd.DataFrame(columns=['trial', 'txt_field', 'classifier', 'tokenizer', 'vect_type', 'average_mtd', 'accuracy', 'precision', 'recall'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try Mission Statements - MultinomialNB - LemmaTokenizer - Count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dview.parallel(block=True)\n",
    "def func_mission_MNB_lemma_count(trial):\n",
    "    global df_train, df_performance\n",
    "    \n",
    "    ##########################################################\n",
    "    ####### Set environments for different functions #########\n",
    "    txt_field='mission' # 'mission', 'prgrm_dsc', 'mission_prgrm'\n",
    "    classifier=naive_bayes.MultinomialNB()\n",
    "    tokenizer='lemma' # 'lemma', 'stemming'\n",
    "    vect_type='count' # 'count', 'tfidf'\n",
    "    average_mtd='macro' # Use unweighted mean.\n",
    "    ####### Set environments for different functions #########\n",
    "    ##########################################################\n",
    "    \n",
    "    ##########################################################\n",
    "    ################ Prepare dataframe for ML ################\n",
    "    #### Sample ####\n",
    "    small_num=0\n",
    "    while small_num<100: # Make sure each category has at least 100 records.\n",
    "        trainDF = df_train[df_train.mission.notna() & df_train.NTEE1.notna()].sample(100000)\n",
    "        small_num=trainDF.groupby('NTEE1').count().sort_values('EIN').iloc[0]['EIN']\n",
    "    #### Sample ####\n",
    "    trainDF['text'] = trainDF[txt_field].astype(str)\n",
    "    trainDF['label'] = trainDF['NTEE1'].astype(str)\n",
    "    # split the dataset into training and validation datasets \n",
    "    x_train, x_valid, y_train, y_valid = model_selection.train_test_split(trainDF['text'], trainDF['label'],\n",
    "                                                                          train_size=0.7, shuffle=True)\n",
    "    ################ Prepare dataframe for ML ################\n",
    "    ##########################################################\n",
    "\n",
    "    ##########################################################\n",
    "    ################ Define tokenizer ################\n",
    "    # Source: http://jonathansoma.com/lede/algorithms-2017/classes/more-text-analysis/counting-and-stemming/\n",
    "    # Use NLTK's PorterStemmer\n",
    "    def stemming_tokenizer(str_input):\n",
    "        tokens = word_tokenize(str_input)\n",
    "        return [PorterStemmer().stem(token) for token in tokens]\n",
    "\n",
    "    # Source: https://scikit-learn.org/stable/modules/feature_extraction.html\n",
    "    # Use NLTK's Lemmatizer\n",
    "    class LemmaTokenizer(object):\n",
    "        def __init__(self):\n",
    "            self.wnl = WordNetLemmatizer()\n",
    "        def __call__(self, doc):\n",
    "             return [self.wnl.lemmatize(t) for t in word_tokenize(doc)]\n",
    "            \n",
    "    if tokenizer=='lemma':\n",
    "        tokenizer=LemmaTokenizer()\n",
    "    elif tokenizer=='stemming':\n",
    "        tokenizer=stemming_tokenizer()\n",
    "    ################ Define tokenizer ################\n",
    "    ##########################################################\n",
    "    \n",
    "    ##########################################################\n",
    "    ######### Text Vectorization and Transformation ##########\n",
    "    # 1. Use Porter Stemmer.\n",
    "    # 2. Use word level, character level does not make sense for current situation.\n",
    "    # 3. Use count (freq) and tf-idf vectorizer. see: \n",
    "    # Bengfort, B., Bilbro, R., & Ojeda, T. (2018). Applied Text Analysis with Python: Enabling Language-Aware Data Products with Machine Learning (1 edition). Beijing Boston Farnham Sebastopol Tokyo: O’Reilly Media.\n",
    "    # Page: 67.\n",
    "    \n",
    "    if vect_type=='count':\n",
    "        ##### Token counts #####\n",
    "        # create the transform\n",
    "        vectorizer = CountVectorizer(stop_words='english', \n",
    "                                     tokenizer=tokenizer, \n",
    "                                     analyzer='word'\n",
    "                                    )\n",
    "        # tokenize and build vocab\n",
    "        vectorizer.fit(trainDF['text'])\n",
    "        # Encode document: transform the training and validation data using count vectorizer object\n",
    "        x_train_vect =  vectorizer.transform(x_train)\n",
    "        x_valid_vect =  vectorizer.transform(x_valid)\n",
    "    elif vect_type=='tfidf':\n",
    "        ##### TF-IDF #####\n",
    "        # create the transform\n",
    "        vectorizer = TfidfVectorizer(stop_words='english', \n",
    "                                     tokenizer=tokenizer, \n",
    "                                     analyzer='word'\n",
    "                                    )\n",
    "        # tokenize and build vocab\n",
    "        vectorizer.fit(trainDF['text'])\n",
    "        # Encode document: transform the training and validation data using count vectorizer object\n",
    "        x_train_vect =  vectorizer.transform(x_train)\n",
    "        x_valid_vect =  vectorizer.transform(x_valid)\n",
    "\n",
    "    ######### Text Vectorization and Transformation ##########\n",
    "    ##########################################################\n",
    "    \n",
    "    classifier.fit(x_train_vect, y_train)\n",
    "    predictions = classifier.predict(x_valid_vect)\n",
    "    df_performance = df_performance.append({'trial':str(trial), \n",
    "                                            'txt_field':txt_field, \n",
    "                                            'classifier':str(classifier), \n",
    "                                            'tokenizer':type(tokenizer).__name__, \n",
    "                                            'vect_type':vect_type, \n",
    "                                            'average_mtd':average_mtd,\n",
    "                                            'accuracy':metrics.accuracy_score(predictions, y_valid), \n",
    "                                            'precision':metrics.precision_score(predictions, y_valid, average=average_mtd),\n",
    "                                            'recall':metrics.recall_score(predictions, y_valid, average=average_mtd),\n",
    "                                           }, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "func_mission_MNB_lemma_count(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "append() got an unexpected keyword argument 'inplace'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-76-57adcbac3cb2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_performance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'vect_type'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: append() got an unexpected keyword argument 'inplace'"
     ]
    }
   ],
   "source": [
    "df_performance.append({'vect_type':'test'}, ignore_index=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "If using all scalar values, you must pass an index",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-73-ce2f7a8c56ef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'vect_type'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    346\u001b[0m                                  dtype=dtype, copy=copy)\n\u001b[1;32m    347\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 348\u001b[0;31m             \u001b[0mmgr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    349\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMaskedArray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m             \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmrecords\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmrecords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_init_dict\u001b[0;34m(self, data, index, columns, dtype)\u001b[0m\n\u001b[1;32m    457\u001b[0m             \u001b[0marrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 459\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_arrays_to_mgr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    460\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_init_ndarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_arrays_to_mgr\u001b[0;34m(arrays, arr_names, index, columns, dtype)\u001b[0m\n\u001b[1;32m   7354\u001b[0m     \u001b[0;31m# figure out the index, if necessary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7355\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 7356\u001b[0;31m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   7357\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7358\u001b[0m     \u001b[0;31m# don't force copy because getting jammed in an ndarray anyway\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mextract_index\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m   7391\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7392\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mindexes\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mraw_lengths\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 7393\u001b[0;31m             raise ValueError('If using all scalar values, you must pass'\n\u001b[0m\u001b[1;32m   7394\u001b[0m                              ' an index')\n\u001b[1;32m   7395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: If using all scalar values, you must pass an index"
     ]
    }
   ],
   "source": [
    "pd.DataFrame({'vect_type':'test'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.661060</td>\n",
       "      <td>0.425503</td>\n",
       "      <td>0.658984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.002628</td>\n",
       "      <td>0.002539</td>\n",
       "      <td>0.020294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.655533</td>\n",
       "      <td>0.419339</td>\n",
       "      <td>0.627483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.659017</td>\n",
       "      <td>0.423844</td>\n",
       "      <td>0.644080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.661350</td>\n",
       "      <td>0.425360</td>\n",
       "      <td>0.650408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.662775</td>\n",
       "      <td>0.427471</td>\n",
       "      <td>0.677598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.668267</td>\n",
       "      <td>0.432884</td>\n",
       "      <td>0.705346</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         accuracy   precision      recall\n",
       "count  100.000000  100.000000  100.000000\n",
       "mean     0.661060    0.425503    0.658984\n",
       "std      0.002628    0.002539    0.020294\n",
       "min      0.655533    0.419339    0.627483\n",
       "25%      0.659017    0.423844    0.644080\n",
       "50%      0.661350    0.425360    0.650408\n",
       "75%      0.662775    0.427471    0.677598\n",
       "max      0.668267    0.432884    0.705346"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_mission_MNB_lemma_count.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try Mission Statements - ComplementNB - LemmaTokenizer - Count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 85/100 [3:10:23<33:50, 135.39s/it]"
     ]
    }
   ],
   "source": [
    "df_mission_CNB_lemma_count=pd.DataFrame(columns=['accuracy', 'precision', 'recall'])\n",
    "for trial in tqdm(range(0, 100)):\n",
    "    \n",
    "    ##########################################################\n",
    "    ################ Prepare dataframe for ML ################\n",
    "    #### Sample ####\n",
    "    small_num=0\n",
    "    while small_num<100: # Make sure each category has at least 100 records.\n",
    "        trainDF = df_train[df_train.mission.notna() & df_train.NTEE1.notna()].sample(100000)\n",
    "        small_num=trainDF.groupby('NTEE1').count().sort_values('EIN').iloc[0]['EIN']\n",
    "    #### Sample ####\n",
    "    trainDF['text'] = trainDF['mission'].astype(str)\n",
    "    trainDF['label'] = trainDF['NTEE1'].astype(str)\n",
    "    # split the dataset into training and validation datasets \n",
    "    x_train, x_valid, y_train, y_valid = model_selection.train_test_split(trainDF['text'], trainDF['label'],\n",
    "                                                                          train_size=0.7, shuffle=True)\n",
    "    ################ Prepare dataframe for ML ################\n",
    "    ##########################################################\n",
    "\n",
    "    ##########################################################\n",
    "    ################ Define tokenizer ################\n",
    "    # Source: http://jonathansoma.com/lede/algorithms-2017/classes/more-text-analysis/counting-and-stemming/\n",
    "    # Use NLTK's PorterStemmer\n",
    "    def stemming_tokenizer(str_input):\n",
    "        tokens = word_tokenize(str_input)\n",
    "        return [PorterStemmer().stem(token) for token in tokens]\n",
    "\n",
    "    # Source: https://scikit-learn.org/stable/modules/feature_extraction.html\n",
    "    # Use NLTK's Lemmatizer\n",
    "    class LemmaTokenizer(object):\n",
    "        def __init__(self):\n",
    "            self.wnl = WordNetLemmatizer()\n",
    "        def __call__(self, doc):\n",
    "             return [self.wnl.lemmatize(t) for t in word_tokenize(doc)]\n",
    "    ################ Define tokenizer ################\n",
    "    ##########################################################\n",
    "\n",
    "    ##########################################################\n",
    "    ######### Text Vectorization and Transformation ##########\n",
    "    # 1. Use Porter Stemmer.\n",
    "    # 2. Use word level, character level does not make sense for current situation.\n",
    "    # 3. Use count (freq) and tf-idf vectorizer. see: \n",
    "    # Bengfort, B., Bilbro, R., & Ojeda, T. (2018). Applied Text Analysis with Python: Enabling Language-Aware Data Products with Machine Learning (1 edition). Beijing Boston Farnham Sebastopol Tokyo: O’Reilly Media.\n",
    "    # Page: 67.\n",
    "\n",
    "    ##### Token counts #####\n",
    "    # create the transform\n",
    "    count_vect = CountVectorizer(stop_words='english', \n",
    "                                 tokenizer=LemmaTokenizer(), \n",
    "                                 analyzer='word'\n",
    "                                )\n",
    "    # tokenize and build vocab\n",
    "    count_vect.fit(trainDF['text'])\n",
    "    # Encode document: transform the training and validation data using count vectorizer object\n",
    "    x_train_vect_count =  count_vect.transform(x_train)\n",
    "    x_valid_vect_count =  count_vect.transform(x_valid)\n",
    "    ##### Token counts #####\n",
    "\n",
    "    ##### TF-IDF #####\n",
    "    # create the transform\n",
    "    tfidf_vect = TfidfVectorizer(stop_words='english', \n",
    "                                 tokenizer=LemmaTokenizer(), \n",
    "                                 analyzer='word'\n",
    "                                )\n",
    "    # tokenize and build vocab\n",
    "    tfidf_vect.fit(trainDF['text'])\n",
    "    # Encode document: transform the training and validation data using count vectorizer object\n",
    "    x_train_vect_tfidf =  tfidf_vect.transform(x_train)\n",
    "    x_valid_vect_tfidf =  tfidf_vect.transform(x_valid)\n",
    "    ##### TF-IDF #####\n",
    "\n",
    "    ######### Text Vectorization and Transformation ##########\n",
    "    ##########################################################\n",
    "\n",
    "    def func_performance(classifier, x_train, y_train, x_valid, y_valid):\n",
    "        # fit the training dataset on the classifier\n",
    "        classifier.fit(x_train, y_train)\n",
    "        # predict the labels on validation dataset\n",
    "        predictions = classifier.predict(x_valid)\n",
    "        return [metrics.accuracy_score(predictions, y_valid), \n",
    "                metrics.precision_score(predictions, y_valid, \n",
    "                                        average='macro', # Use unweighted mean.\n",
    "                                       ),\n",
    "                metrics.recall_score(predictions, y_valid, \n",
    "                                     average='macro',  # Use unweighted mean.\n",
    "                                    )]\n",
    "\n",
    "    performance_result=func_performance(classifier=naive_bayes.ComplementNB(), \n",
    "                                        x_train=x_train_vect_count,\n",
    "                                        y_train= y_train, \n",
    "                                        x_valid=x_valid_vect_count,\n",
    "                                        y_valid=y_valid\n",
    "                                       )\n",
    "    df_mission_CNB_lemma_count.loc[trial]=performance_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mission_CNB_lemma_count.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try Program Description - MultinomialNB - LemmaTokenizer - Count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [11:35:52<00:00, 419.77s/it]\n"
     ]
    }
   ],
   "source": [
    "df_prgrm_MNB_lemma_count=pd.DataFrame(columns=['accuracy', 'precision', 'recall'])\n",
    "for trial in tqdm(range(0, 100)):\n",
    "    \n",
    "    ##########################################################\n",
    "    ################ Prepare dataframe for ML ################\n",
    "    #### Sample ####\n",
    "    small_num=0\n",
    "    while small_num<100: # Make sure each category has at least 100 records.\n",
    "        trainDF = df_train[df_train.mission.notna() & df_train.NTEE1.notna()].sample(100000)\n",
    "        small_num=trainDF.groupby('NTEE1').count().sort_values('EIN').iloc[0]['EIN']\n",
    "    #### Sample ####\n",
    "    trainDF['text'] = trainDF['prgrm_dsc'].astype(str)\n",
    "    trainDF['label'] = trainDF['NTEE1'].astype(str)\n",
    "    # split the dataset into training and validation datasets \n",
    "    x_train, x_valid, y_train, y_valid = model_selection.train_test_split(trainDF['text'], trainDF['label'],\n",
    "                                                                          train_size=0.7, shuffle=True)\n",
    "    ################ Prepare dataframe for ML ################\n",
    "    ##########################################################\n",
    "\n",
    "    ##########################################################\n",
    "    ################ Define tokenizer ################\n",
    "    # Source: http://jonathansoma.com/lede/algorithms-2017/classes/more-text-analysis/counting-and-stemming/\n",
    "    # Use NLTK's PorterStemmer\n",
    "    def stemming_tokenizer(str_input):\n",
    "        tokens = word_tokenize(str_input)\n",
    "        return [PorterStemmer().stem(token) for token in tokens]\n",
    "\n",
    "    # Source: https://scikit-learn.org/stable/modules/feature_extraction.html\n",
    "    # Use NLTK's Lemmatizer\n",
    "    class LemmaTokenizer(object):\n",
    "        def __init__(self):\n",
    "            self.wnl = WordNetLemmatizer()\n",
    "        def __call__(self, doc):\n",
    "             return [self.wnl.lemmatize(t) for t in word_tokenize(doc)]\n",
    "    ################ Define tokenizer ################\n",
    "    ##########################################################\n",
    "\n",
    "    ##########################################################\n",
    "    ######### Text Vectorization and Transformation ##########\n",
    "    # 1. Use Porter Stemmer.\n",
    "    # 2. Use word level, character level does not make sense for current situation.\n",
    "    # 3. Use count (freq) and tf-idf vectorizer. see: \n",
    "    # Bengfort, B., Bilbro, R., & Ojeda, T. (2018). Applied Text Analysis with Python: Enabling Language-Aware Data Products with Machine Learning (1 edition). Beijing Boston Farnham Sebastopol Tokyo: O’Reilly Media.\n",
    "    # Page: 67.\n",
    "\n",
    "    ##### Token counts #####\n",
    "    # create the transform\n",
    "    count_vect = CountVectorizer(stop_words='english', \n",
    "                                 tokenizer=LemmaTokenizer(), \n",
    "                                 analyzer='word'\n",
    "                                )\n",
    "    # tokenize and build vocab\n",
    "    count_vect.fit(trainDF['text'])\n",
    "    # Encode document: transform the training and validation data using count vectorizer object\n",
    "    x_train_vect_count =  count_vect.transform(x_train)\n",
    "    x_valid_vect_count =  count_vect.transform(x_valid)\n",
    "    ##### Token counts #####\n",
    "\n",
    "    ##### TF-IDF #####\n",
    "    # create the transform\n",
    "    tfidf_vect = TfidfVectorizer(stop_words='english', \n",
    "                                 tokenizer=LemmaTokenizer(), \n",
    "                                 analyzer='word'\n",
    "                                )\n",
    "    # tokenize and build vocab\n",
    "    tfidf_vect.fit(trainDF['text'])\n",
    "    # Encode document: transform the training and validation data using count vectorizer object\n",
    "    x_train_vect_tfidf =  tfidf_vect.transform(x_train)\n",
    "    x_valid_vect_tfidf =  tfidf_vect.transform(x_valid)\n",
    "    ##### TF-IDF #####\n",
    "\n",
    "    ######### Text Vectorization and Transformation ##########\n",
    "    ##########################################################\n",
    "\n",
    "    def func_performance(classifier, x_train, y_train, x_valid, y_valid):\n",
    "        # fit the training dataset on the classifier\n",
    "        classifier.fit(x_train, y_train)\n",
    "        # predict the labels on validation dataset\n",
    "        predictions = classifier.predict(x_valid)\n",
    "        return [metrics.accuracy_score(predictions, y_valid), \n",
    "                metrics.precision_score(predictions, y_valid, \n",
    "                                        average='macro', # Use unweighted mean.\n",
    "                                       ),\n",
    "                metrics.recall_score(predictions, y_valid, \n",
    "                                     average='macro',  # Use unweighted mean.\n",
    "                                    )]\n",
    "\n",
    "    performance_result=func_performance(classifier=naive_bayes.MultinomialNB(), \n",
    "                                        x_train=x_train_vect_count,\n",
    "                                        y_train= y_train, \n",
    "                                        x_valid=x_valid_vect_count,\n",
    "                                        y_valid=y_valid\n",
    "                                       )\n",
    "    df_prgrm_MNB_lemma_count.loc[trial]=performance_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prgrm_MNB_lemma_count.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try Program Description - ComplementNB - LemmaTokenizer - Count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prgrm_CNB_lemma_count=pd.DataFrame(columns=['accuracy', 'precision', 'recall'])\n",
    "for trial in tqdm(range(0, 100)):\n",
    "    \n",
    "    ##########################################################\n",
    "    ################ Prepare dataframe for ML ################\n",
    "    #### Sample ####\n",
    "    small_num=0\n",
    "    while small_num<100: # Make sure each category has at least 100 records.\n",
    "        trainDF = df_train[df_train.mission.notna() & df_train.NTEE1.notna()].sample(100000)\n",
    "        small_num=trainDF.groupby('NTEE1').count().sort_values('EIN').iloc[0]['EIN']\n",
    "    #### Sample ####\n",
    "    trainDF['text'] = trainDF['prgrm_dsc'].astype(str)\n",
    "    trainDF['label'] = trainDF['NTEE1'].astype(str)\n",
    "    # split the dataset into training and validation datasets \n",
    "    x_train, x_valid, y_train, y_valid = model_selection.train_test_split(trainDF['text'], trainDF['label'],\n",
    "                                                                          train_size=0.7, shuffle=True)\n",
    "    ################ Prepare dataframe for ML ################\n",
    "    ##########################################################\n",
    "\n",
    "    ##########################################################\n",
    "    ################ Define tokenizer ################\n",
    "    # Source: http://jonathansoma.com/lede/algorithms-2017/classes/more-text-analysis/counting-and-stemming/\n",
    "    # Use NLTK's PorterStemmer\n",
    "    def stemming_tokenizer(str_input):\n",
    "        tokens = word_tokenize(str_input)\n",
    "        return [PorterStemmer().stem(token) for token in tokens]\n",
    "\n",
    "    # Source: https://scikit-learn.org/stable/modules/feature_extraction.html\n",
    "    # Use NLTK's Lemmatizer\n",
    "    class LemmaTokenizer(object):\n",
    "        def __init__(self):\n",
    "            self.wnl = WordNetLemmatizer()\n",
    "        def __call__(self, doc):\n",
    "             return [self.wnl.lemmatize(t) for t in word_tokenize(doc)]\n",
    "    ################ Define tokenizer ################\n",
    "    ##########################################################\n",
    "\n",
    "    ##########################################################\n",
    "    ######### Text Vectorization and Transformation ##########\n",
    "    # 1. Use Porter Stemmer.\n",
    "    # 2. Use word level, character level does not make sense for current situation.\n",
    "    # 3. Use count (freq) and tf-idf vectorizer. see: \n",
    "    # Bengfort, B., Bilbro, R., & Ojeda, T. (2018). Applied Text Analysis with Python: Enabling Language-Aware Data Products with Machine Learning (1 edition). Beijing Boston Farnham Sebastopol Tokyo: O’Reilly Media.\n",
    "    # Page: 67.\n",
    "\n",
    "    ##### Token counts #####\n",
    "    # create the transform\n",
    "    count_vect = CountVectorizer(stop_words='english', \n",
    "                                 tokenizer=LemmaTokenizer(), \n",
    "                                 analyzer='word'\n",
    "                                )\n",
    "    # tokenize and build vocab\n",
    "    count_vect.fit(trainDF['text'])\n",
    "    # Encode document: transform the training and validation data using count vectorizer object\n",
    "    x_train_vect_count =  count_vect.transform(x_train)\n",
    "    x_valid_vect_count =  count_vect.transform(x_valid)\n",
    "    ##### Token counts #####\n",
    "\n",
    "    ##### TF-IDF #####\n",
    "    # create the transform\n",
    "    tfidf_vect = TfidfVectorizer(stop_words='english', \n",
    "                                 tokenizer=LemmaTokenizer(), \n",
    "                                 analyzer='word'\n",
    "                                )\n",
    "    # tokenize and build vocab\n",
    "    tfidf_vect.fit(trainDF['text'])\n",
    "    # Encode document: transform the training and validation data using count vectorizer object\n",
    "    x_train_vect_tfidf =  tfidf_vect.transform(x_train)\n",
    "    x_valid_vect_tfidf =  tfidf_vect.transform(x_valid)\n",
    "    ##### TF-IDF #####\n",
    "\n",
    "    ######### Text Vectorization and Transformation ##########\n",
    "    ##########################################################\n",
    "\n",
    "    def func_performance(classifier, x_train, y_train, x_valid, y_valid):\n",
    "        # fit the training dataset on the classifier\n",
    "        classifier.fit(x_train, y_train)\n",
    "        # predict the labels on validation dataset\n",
    "        predictions = classifier.predict(x_valid)\n",
    "        return [metrics.accuracy_score(predictions, y_valid), \n",
    "                metrics.precision_score(predictions, y_valid, \n",
    "                                        average='macro', # Use unweighted mean.\n",
    "                                       ),\n",
    "                metrics.recall_score(predictions, y_valid, \n",
    "                                     average='macro',  # Use unweighted mean.\n",
    "                                    )]\n",
    "\n",
    "    performance_result=func_performance(classifier=naive_bayes.ComplementNB(), \n",
    "                                        x_train=x_train_vect_count,\n",
    "                                        y_train= y_train, \n",
    "                                        x_valid=x_valid_vect_count,\n",
    "                                        y_valid=y_valid\n",
    "                                       )\n",
    "    df_prgrm_CNB_lemma_count.loc[trial]=performance_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prgrm_CNB_lemma_count.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB, TF IDF Vectors:  [0.5583666666666667, 0.29982304712090974, 0.5961215483457072, datetime.timedelta(microseconds=498992)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes on Word Level TF IDF Vectors\n",
    "# Naive Bayes on Count Vectors\n",
    "accuracy = train_model(classifier=naive_bayes.MultinomialNB(), \n",
    "                       x_train=x_train_vect_tfidf,\n",
    "                       y_train= y_train, \n",
    "                       x_valid=x_valid_vect_tfidf,\n",
    "                       y_valid=y_valid\n",
    "                      )\n",
    "results.loc[len(results)] = [\"NB, Count Vectors\", accuracy[0], accuracy[1], accuracy[2], accuracy[3]]\n",
    "print(\"NB, TF IDF Vectors: \", accuracy)\n",
    "\n",
    "\n",
    "# Raw: MNB, Count Vectors:  [0.6732666666666667, 0.4503852200520778, 0.6661678975327818, datetime.timedelta(microseconds=489443)]\n",
    "# Raw: CNB, Count Vectors:  [0.7013333333333334, 0.5275971009306734, 0.6170202787685387, datetime.timedelta(microseconds=521657)]\n",
    "# Stemmed: MNB, Count Vectors:  [0.6601, 0.4243013182900721, 0.639930088219624, datetime.timedelta(microseconds=443571)]\n",
    "# Stemmed: CNB, Count Vectors:  [0.7, 0.5320034132229355, 0.6383184621220114, datetime.timedelta(microseconds=504788)]\n",
    "# Lemma: MNB, Count Vectors:  [0.6615, 0.42366552445612, 0.6783346552845068, datetime.timedelta(microseconds=572416)]\n",
    "# Lemma: CNB, Count Vectors:  [0.7021333333333334, 0.5341465196920103, 0.6318440519847688, datetime.timedelta(microseconds=586597)]\n",
    "\n",
    "# Raw: MNB, TF IDF Vectors:  [0.5859, 0.32327746269021723, 0.5485416630089535, datetime.timedelta(microseconds=536424)]\n",
    "# Raw: CNB, TF IDF Vectors:  [0.6992333333333334, 0.5306246321804787, 0.6136847132057796, datetime.timedelta(microseconds=580513)]\n",
    "# Stemmed: MNB, TF IDF Vectors:  [0.553, 0.2949552239378839, 0.536824161513825, datetime.timedelta(microseconds=453431)]\n",
    "# Stemmed: CNB, TF IDF Vectors:  [0.6967666666666666, 0.5349583083797411, 0.6223488506468897, datetime.timedelta(microseconds=520307)]\n",
    "# Lemma: MNB, TF IDF Vectors:  [0.5589666666666666, 0.297005157029138, 0.5293106710625075, datetime.timedelta(microseconds=529497)]\n",
    "# Lemma: CNB, TF IDF Vectors:  [0.7012333333333334, 0.536867845262306, 0.6273414839488708, datetime.timedelta(microseconds=546401)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Looks like `Lemma-CNB-Count` produces best results.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc-autonumbering": false,
  "toc-showcode": true,
  "toc-showmarkdowntxt": true,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
