{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:**\n",
    "- [x] Build NTEE-10 major groups.\n",
    "- [x] Vectorize output labels.\n",
    "- [x] Vectorize input texts.\n",
    "- [x] Spell check.\n",
    "- [x] Resampling - Add more records of 'VI' (International, Foreign Affairs - Q).\n",
    "- [x] Grid search. - Not feasible, need too much resources (i.e., memory, CPU).\n",
    "- [x] Spellcheck.\n",
    "- [x] Train/test dataset complete split.\n",
    "- [ ] Solve reproducibility problem, [Ref1](https://github.com/keras-team/keras/issues/7676) [Ref2](https://github.com/keras-team/keras/issues/4875) [Ref3 (solution)](https://keras.io/getting-started/faq/#how-can-i-obtain-reproducible-results-using-keras-during-development)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# obtain reproducible results\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random as rn\n",
    "\n",
    "# The below is necessary for starting Numpy generated random numbers\n",
    "# in a well-defined initial state.\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# The below is necessary for starting core Python generated random numbers\n",
    "# in a well-defined state.\n",
    "\n",
    "rn.seed(12345)\n",
    "\n",
    "# Force TensorFlow to use single thread.\n",
    "# Multiple threads are a potential source of non-reproducible results.\n",
    "# For further details, see: https://stackoverflow.com/questions/42022950/\n",
    "\n",
    "session_conf = tf.ConfigProto(intra_op_parallelism_threads=1,\n",
    "                              inter_op_parallelism_threads=1)\n",
    "\n",
    "from keras import backend as K\n",
    "\n",
    "# The below tf.set_random_seed() will make random number generation\n",
    "# in the TensorFlow backend have a well-defined initial state.\n",
    "# For further details, see:\n",
    "# https://www.tensorflow.org/api_docs/python/tf/set_random_seed\n",
    "\n",
    "tf.set_random_seed(1234)\n",
    "\n",
    "sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
    "K.set_session(sess)\n",
    "\n",
    "# Rest of code follows ...\n",
    "\n",
    "# Check GPU device.\n",
    "print(K.tensorflow_backend._get_available_gpus())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://richliao.github.io/supervised/classification/2016/11/26/textclassifier-convolutional/\n",
    "#https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html\n",
    "#RNN\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from multiprocessing import Pool\n",
    "from spellchecker import SpellChecker\n",
    "import string\n",
    "import math\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "import keras.preprocessing.text as kpt\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Embedding\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk import word_tokenize\n",
    "\n",
    "# For encoding labels.\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read and compile tranining and validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(154424, 38607)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_file_path='../../dataset/UCF/train/'\n",
    "file_list=os.listdir(train_file_path)\n",
    "df_train=pd.DataFrame()\n",
    "for file in file_list:\n",
    "    df_train=pd.concat([df_train, pd.read_pickle(train_file_path+file, compression='gzip')])\n",
    "\n",
    "test_file_path='../../dataset/UCF/test/'\n",
    "file_list=os.listdir(test_file_path)\n",
    "df_test=pd.DataFrame()\n",
    "for file in file_list:\n",
    "    df_test=pd.concat([df_test, pd.read_pickle(test_file_path+file, compression='gzip')])\n",
    "    \n",
    "len(df_train), len(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DLN</th>\n",
       "      <th>EIN</th>\n",
       "      <th>FILING_TYPE</th>\n",
       "      <th>IRS990EZ_p3_DscrptnPrgrmSrvcAccmTxt</th>\n",
       "      <th>IRS990EZ_p3_PrmryExmptPrpsTxt</th>\n",
       "      <th>IRS990PF_p16b_RltnshpSttmntTxt</th>\n",
       "      <th>IRS990PF_p9a_DscrptnTxt</th>\n",
       "      <th>IRS990ScheduleO_ExplntnTxt</th>\n",
       "      <th>IRS990_p1_ActvtyOrMssnDsc</th>\n",
       "      <th>IRS990_p3_DscS</th>\n",
       "      <th>...</th>\n",
       "      <th>SUB_DATE</th>\n",
       "      <th>TAXPAYER_NAME</th>\n",
       "      <th>TAX_PERIOD</th>\n",
       "      <th>YEAR</th>\n",
       "      <th>95_and_before</th>\n",
       "      <th>NTEE1</th>\n",
       "      <th>mission</th>\n",
       "      <th>prgrm_dsc</th>\n",
       "      <th>mission_spellchk</th>\n",
       "      <th>prgrm_dsc_spellchk</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1363470</th>\n",
       "      <td>9.349332e+13</td>\n",
       "      <td>390768547</td>\n",
       "      <td>EFILE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>TO PROVIDE A MEMBERSHIP ORGANIZATION FOR THE M...</td>\n",
       "      <td>PARADE OF HOMES - ANNUAL SHOW EXHIBITING THE L...</td>\n",
       "      <td>...</td>\n",
       "      <td>12/31/2014</td>\n",
       "      <td>MADISON AREA BUILDERS ASSOCIATION INC</td>\n",
       "      <td>201312.0</td>\n",
       "      <td>2014.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>S</td>\n",
       "      <td>TO PROVIDE A MEMBERSHIP ORGANIZATION FOR THE M...</td>\n",
       "      <td>DREAM HOME SHOWCASE - ANNUAL SHOW TO PROVIDE B...</td>\n",
       "      <td>TO PROVIDE A MEMBERSHIP ORGANIZATION FOR THE M...</td>\n",
       "      <td>DREAM HOME SHOWCASE - ANNUAL SHOW TO PROVIDE B...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>424991</th>\n",
       "      <td>9.349331e+13</td>\n",
       "      <td>943309195</td>\n",
       "      <td>EFILE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>TO IMPROVE THE DENTAL HEALTH OF CHILDREN IN NE...</td>\n",
       "      <td>DENTAL HEALTH EDUCATION, PREVENTION, DETECTION...</td>\n",
       "      <td>...</td>\n",
       "      <td>02/03/2016</td>\n",
       "      <td>PRASAD CHILDRENS DENTAL HEALTH PROGRAM INC</td>\n",
       "      <td>201412.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>E</td>\n",
       "      <td>TO IMPROVE THE DENTAL HEALTH OF CHILDREN IN NE...</td>\n",
       "      <td>DENTAL HEALTH EDUCATION, PREVENTION, DETECTION...</td>\n",
       "      <td>TO IMPROVE THE DENTAL HEALTH OF CHILDREN IN NE...</td>\n",
       "      <td>DENTAL HEALTH EDUCATION , PREVENTION , DETECTI...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1816561</th>\n",
       "      <td>9.349226e+13</td>\n",
       "      <td>262616556</td>\n",
       "      <td>EFILE</td>\n",
       "      <td>ROTARY FOUNDATION</td>\n",
       "      <td>COMMUNITY AND WORLDWIDE OUTREACH.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>10/29/2014 8:28:06 AM</td>\n",
       "      <td>ROTARY CLUB OF MILTON-WINDWARD FOUNDATION INCO...</td>\n",
       "      <td>201312.0</td>\n",
       "      <td>2014.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>S</td>\n",
       "      <td>COMMUNITY AND WORLDWIDE OUTREACH.</td>\n",
       "      <td>ROTARY FOUNDATION</td>\n",
       "      <td>COMMUNITY AND WORLDWIDE OUTREACH .</td>\n",
       "      <td>ROTARY FOUNDATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1558658</th>\n",
       "      <td>9.349313e+13</td>\n",
       "      <td>410646500</td>\n",
       "      <td>EFILE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>Provided social recreational activities for th...</td>\n",
       "      <td>Provided social-recreational activities for al...</td>\n",
       "      <td>...</td>\n",
       "      <td>7/16/2015</td>\n",
       "      <td>AMERICAN LEGION POST 241</td>\n",
       "      <td>201406.0</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>W</td>\n",
       "      <td>PROVIDED SOCIAL RECREATIONAL ACTIVITIES FOR TH...</td>\n",
       "      <td>PROVIDED SOCIAL-RECREATIONAL ACTIVITIES FOR AL...</td>\n",
       "      <td>PROVIDED SOCIAL RECREATIONAL ACTIVITIES FOR TH...</td>\n",
       "      <td>PROVIDED SOCIAL-RECREATIONAL ACTIVITIES FOR AL...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1781284</th>\n",
       "      <td>9.349323e+13</td>\n",
       "      <td>60646844</td>\n",
       "      <td>EFILE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>INPATIENT SERVICES SAINT MARY'S REMAINS COMMIT...</td>\n",
       "      <td>SAINT MARY'S HOSPITAL PROVIDES EXCELLENT HEALT...</td>\n",
       "      <td>INPATIENT SAINT MARY'S REMAINS COMMITTED TO PR...</td>\n",
       "      <td>...</td>\n",
       "      <td>10/5/2015</td>\n",
       "      <td>ST MARYS HOSPITAL INC</td>\n",
       "      <td>201409.0</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>E</td>\n",
       "      <td>SAINT MARY'S HOSPITAL PROVIDES EXCELLENT HEALT...</td>\n",
       "      <td>INPATIENT SAINT MARY'S REMAINS COMMITTED TO PR...</td>\n",
       "      <td>SAINT MARY IS HOSPITAL PROVIDES EXCELLENT HEAL...</td>\n",
       "      <td>INPATIENT SAINT MARY IS REMAINS COMMITTED TO P...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  DLN        EIN FILING_TYPE  \\\n",
       "1363470  9.349332e+13  390768547       EFILE   \n",
       "424991   9.349331e+13  943309195       EFILE   \n",
       "1816561  9.349226e+13  262616556       EFILE   \n",
       "1558658  9.349313e+13  410646500       EFILE   \n",
       "1781284  9.349323e+13   60646844       EFILE   \n",
       "\n",
       "        IRS990EZ_p3_DscrptnPrgrmSrvcAccmTxt  \\\n",
       "1363470                                 NaN   \n",
       "424991                                  NaN   \n",
       "1816561                   ROTARY FOUNDATION   \n",
       "1558658                                 NaN   \n",
       "1781284                                 NaN   \n",
       "\n",
       "             IRS990EZ_p3_PrmryExmptPrpsTxt IRS990PF_p16b_RltnshpSttmntTxt  \\\n",
       "1363470                                NaN                            NaN   \n",
       "424991                                 NaN                            NaN   \n",
       "1816561  COMMUNITY AND WORLDWIDE OUTREACH.                            NaN   \n",
       "1558658                                NaN                            NaN   \n",
       "1781284                                NaN                            NaN   \n",
       "\n",
       "        IRS990PF_p9a_DscrptnTxt  \\\n",
       "1363470                     NaN   \n",
       "424991                      NaN   \n",
       "1816561                     NaN   \n",
       "1558658                     NaN   \n",
       "1781284                     NaN   \n",
       "\n",
       "                                IRS990ScheduleO_ExplntnTxt  \\\n",
       "1363470                                                      \n",
       "424991                                                       \n",
       "1816561                                                      \n",
       "1558658                                                      \n",
       "1781284  INPATIENT SERVICES SAINT MARY'S REMAINS COMMIT...   \n",
       "\n",
       "                                 IRS990_p1_ActvtyOrMssnDsc  \\\n",
       "1363470  TO PROVIDE A MEMBERSHIP ORGANIZATION FOR THE M...   \n",
       "424991   TO IMPROVE THE DENTAL HEALTH OF CHILDREN IN NE...   \n",
       "1816561                                                NaN   \n",
       "1558658  Provided social recreational activities for th...   \n",
       "1781284  SAINT MARY'S HOSPITAL PROVIDES EXCELLENT HEALT...   \n",
       "\n",
       "                                            IRS990_p3_DscS  ...  \\\n",
       "1363470  PARADE OF HOMES - ANNUAL SHOW EXHIBITING THE L...  ...   \n",
       "424991   DENTAL HEALTH EDUCATION, PREVENTION, DETECTION...  ...   \n",
       "1816561                                                NaN  ...   \n",
       "1558658  Provided social-recreational activities for al...  ...   \n",
       "1781284  INPATIENT SAINT MARY'S REMAINS COMMITTED TO PR...  ...   \n",
       "\n",
       "                      SUB_DATE  \\\n",
       "1363470             12/31/2014   \n",
       "424991              02/03/2016   \n",
       "1816561  10/29/2014 8:28:06 AM   \n",
       "1558658              7/16/2015   \n",
       "1781284              10/5/2015   \n",
       "\n",
       "                                             TAXPAYER_NAME  TAX_PERIOD  \\\n",
       "1363470              MADISON AREA BUILDERS ASSOCIATION INC    201312.0   \n",
       "424991          PRASAD CHILDRENS DENTAL HEALTH PROGRAM INC    201412.0   \n",
       "1816561  ROTARY CLUB OF MILTON-WINDWARD FOUNDATION INCO...    201312.0   \n",
       "1558658                           AMERICAN LEGION POST 241    201406.0   \n",
       "1781284                              ST MARYS HOSPITAL INC    201409.0   \n",
       "\n",
       "           YEAR 95_and_before NTEE1  \\\n",
       "1363470  2014.0           1.0     S   \n",
       "424991   2016.0           0.0     E   \n",
       "1816561  2014.0           0.0     S   \n",
       "1558658  2015.0           1.0     W   \n",
       "1781284  2015.0           1.0     E   \n",
       "\n",
       "                                                   mission  \\\n",
       "1363470  TO PROVIDE A MEMBERSHIP ORGANIZATION FOR THE M...   \n",
       "424991   TO IMPROVE THE DENTAL HEALTH OF CHILDREN IN NE...   \n",
       "1816561                  COMMUNITY AND WORLDWIDE OUTREACH.   \n",
       "1558658  PROVIDED SOCIAL RECREATIONAL ACTIVITIES FOR TH...   \n",
       "1781284  SAINT MARY'S HOSPITAL PROVIDES EXCELLENT HEALT...   \n",
       "\n",
       "                                                 prgrm_dsc  \\\n",
       "1363470  DREAM HOME SHOWCASE - ANNUAL SHOW TO PROVIDE B...   \n",
       "424991   DENTAL HEALTH EDUCATION, PREVENTION, DETECTION...   \n",
       "1816561                                  ROTARY FOUNDATION   \n",
       "1558658  PROVIDED SOCIAL-RECREATIONAL ACTIVITIES FOR AL...   \n",
       "1781284  INPATIENT SAINT MARY'S REMAINS COMMITTED TO PR...   \n",
       "\n",
       "                                          mission_spellchk  \\\n",
       "1363470  TO PROVIDE A MEMBERSHIP ORGANIZATION FOR THE M...   \n",
       "424991   TO IMPROVE THE DENTAL HEALTH OF CHILDREN IN NE...   \n",
       "1816561                 COMMUNITY AND WORLDWIDE OUTREACH .   \n",
       "1558658  PROVIDED SOCIAL RECREATIONAL ACTIVITIES FOR TH...   \n",
       "1781284  SAINT MARY IS HOSPITAL PROVIDES EXCELLENT HEAL...   \n",
       "\n",
       "                                        prgrm_dsc_spellchk  \n",
       "1363470  DREAM HOME SHOWCASE - ANNUAL SHOW TO PROVIDE B...  \n",
       "424991   DENTAL HEALTH EDUCATION , PREVENTION , DETECTI...  \n",
       "1816561                                  ROTARY FOUNDATION  \n",
       "1558658  PROVIDED SOCIAL-RECREATIONAL ACTIVITIES FOR AL...  \n",
       "1781284  INPATIENT SAINT MARY IS REMAINS COMMITTED TO P...  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I. Arts, Culture, and Humanities - A  \n",
    "II. Education - B  \n",
    "III. Environment and Animals - C, D  \n",
    "IV. Health - E, F, G, H  \n",
    "V. Human Services - I, J, K, L, M, N, O, P  \n",
    "VI. International, Foreign Affairs - Q  \n",
    "VII. Public, Societal Benefit - R, S, T, U, V, W  \n",
    "VIII. Religion Related - X  \n",
    "IX. Mutual/Membership Benefit - Y  \n",
    "X. Unknown, Unclassified - Z  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "154424 25 9\n",
      "38607 25 9\n"
     ]
    }
   ],
   "source": [
    "# Code as 10 broad categories.\n",
    "broad_cat_dict={'I': ['A'],\n",
    "                'II': ['B'],\n",
    "                'III': ['C', 'D'],\n",
    "                'IV': ['E', 'F', 'G', 'H'],\n",
    "                'V': ['I', 'J', 'K', 'L', 'M', 'N', 'O', 'P'],\n",
    "                'VI': ['Q'],\n",
    "                'VII': ['R', 'S', 'T', 'U', 'V', 'W'],\n",
    "                'VIII': ['X'],\n",
    "                'IX': ['Y'],\n",
    "                'X': ['Z'],\n",
    "               }\n",
    "def ntee2cat(string):\n",
    "    global broad_cat_dict\n",
    "    return [s for s in broad_cat_dict.keys() if string in broad_cat_dict[s]][0]\n",
    "\n",
    "df_train['mission_prgrm_spellchk']=df_train['TAXPAYER_NAME']+' '+df_train['mission_spellchk']+' '+df_train['prgrm_dsc_spellchk'] # Using spell-checked.\n",
    "df_train['broad_cat']=df_train['NTEE1'].apply(ntee2cat)\n",
    "print(len(df_train['mission_prgrm_spellchk']), len(df_train['NTEE1'].drop_duplicates()), len(df_train['broad_cat'].drop_duplicates()))\n",
    "\n",
    "df_test['mission_prgrm_spellchk']=df_test['TAXPAYER_NAME']+' '+df_test['mission_spellchk']+' '+df_test['prgrm_dsc_spellchk'] # Using spell-checked.\n",
    "df_test['broad_cat']=df_test['NTEE1'].apply(ntee2cat)\n",
    "print(len(df_test['mission_prgrm_spellchk']), len(df_test['NTEE1'].drop_duplicates()), len(df_test['broad_cat'].drop_duplicates()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1363470    MADISON AREA BUILDERS ASSOCIATION INC TO PROVI...\n",
       "424991     PRASAD CHILDRENS DENTAL HEALTH PROGRAM INC TO ...\n",
       "1816561    ROTARY CLUB OF MILTON-WINDWARD FOUNDATION INCO...\n",
       "1558658    AMERICAN LEGION POST 241 PROVIDED SOCIAL RECRE...\n",
       "1781284    ST MARYS HOSPITAL INC SAINT MARY IS HOSPITAL P...\n",
       "2202860    NORTHERN ARIZONA VOLUNTEER MEDICAL AND SURGICA...\n",
       "297296     BEL CANTO COMPANY INC TO PROVIDE CHORAL SINGIN...\n",
       "394101     UNIVERSITY OF THE ANDES FOUNDATION INC THE PRI...\n",
       "141481     CCBC PROPERTIES INC TO OWN , LEASE , AND MANAG...\n",
       "1552925    SHAKEN BABY PREVENTION OF IDAHO INC TO BRING E...\n",
       "Name: mission_prgrm_spellchk, dtype: object"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['mission_prgrm_spellchk'][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<s>\n",
    "\n",
    "```Python\n",
    "# Build training and testing data frame.\n",
    "small_num=0\n",
    "while small_num<500: # Make sure each category has at least 500 records.\n",
    "    sampleDF = df_train[df_train.mission.notna() & df_train.NTEE1.notna()].sample(120000)\n",
    "    trainDF, valDF =train_test_split(sampleDF, test_size=.3)\n",
    "    small_num=trainDF.groupby('NTEE_M').count().sort_values('EIN').iloc[0]['EIN']\n",
    "```\n",
    "\n",
    "</s>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "broad_cat\n",
      "I       0.110151\n",
      "II      0.167247\n",
      "III     0.048969\n",
      "IV      0.109025\n",
      "IX      0.042998\n",
      "V       0.302634\n",
      "VI      0.012867\n",
      "VII     0.176540\n",
      "VIII    0.029568\n",
      "Name: EIN, dtype: float64 \n",
      "\n",
      " broad_cat\n",
      "I       0.111146\n",
      "II      0.166265\n",
      "III     0.048204\n",
      "IV      0.112130\n",
      "IX      0.044059\n",
      "V       0.303650\n",
      "VI      0.011293\n",
      "VII     0.174813\n",
      "VIII    0.028440\n",
      "Name: EIN, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# See the composition by NTEE major groups.\n",
    "print(df_train.groupby('broad_cat').count()['EIN']/len(df_train), '\\n'*2, df_test.groupby('broad_cat').count()['EIN']/len(df_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare labels.\n",
    "*One hot encoding.* Prepare after resampling; otherwise, shape of `y_train` will shrink from 25 to 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1363470    VII\n",
       "424991      IV\n",
       "1816561    VII\n",
       "1558658    VII\n",
       "1781284     IV\n",
       "2202860     IV\n",
       "297296       I\n",
       "394101      II\n",
       "141481      IV\n",
       "1552925      V\n",
       "Name: broad_cat, dtype: object"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['broad_cat'][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "lb = preprocessing.LabelBinarizer()\n",
    "lb.fit(list(df_train.broad_cat.unique()))\n",
    "\n",
    "y_train=lb.transform(df_train['broad_cat'])\n",
    "# y_test=lb.transform(df_test['broad_cat']) # No need to transform Y for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# Save LabelBinarizer class for developing package.\n",
    "with open('../../output/lb_broad_cat.pkl', 'wb') as output:\n",
    "    pickle.dump(lb, output, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Not efficient codes, ignore._\n",
    "\n",
    "<s>\n",
    "    \n",
    "``` Python\n",
    "def one_hot(label_list, class_list):\n",
    "    int_encoder=LabelEncoder().fit(class_list) # Build the encoder.\n",
    "    label_int_encoded=int_encoder.transform(label_list) # One-dimensional integer encoded.\n",
    "    return np_utils.to_categorical(label_int_encoded) # Multi-dimensional binary/one-hot encoded.\n",
    "\n",
    "y_train=one_hot(label_list=df_train['NTEE1'], class_list=list(df_train.NTEE1.unique()))\n",
    "y_test=one_hot(label_list=df_test['NTEE1'], class_list=list(df_train.NTEE1.unique()))\n",
    "```\n",
    "\n",
    "### One-dimensional integer encoding for grid search.\n",
    "\n",
    "```Python\n",
    "def int_encode(label_list, class_list):\n",
    "    int_encoder=LabelEncoder().fit(class_list) # Build the encoder.\n",
    "    label_int_encoded=int_encoder.transform(label_list) # One-dimensional integer encoded.\n",
    "    return label_int_encoded # Multi-dimensional binary/one-hot encoded.\n",
    "\n",
    "y_train=int_encode(label_list=trainDF['NTEE_M'], class_list=list(major_group_dict.keys()))\n",
    "y_val=int_encode(label_list=valDF['NTEE_M'], class_list=list(major_group_dict.keys()))\n",
    "\n",
    "# Use integer encoding: https://github.com/keras-team/keras/issues/9331\n",
    "# Use sparse_categorical_crossentropy: https://keras.io/losses/\n",
    "```\n",
    "    \n",
    "</s>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare input text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Wrong codes, stop words not removed because capitalizations are not consistent**\n",
    "- If choose word embedding, no difference if not removed. \n",
    "- See reference: https://stackoverflow.com/questions/34721984/stopword-removing-when-using-the-word2vec/40447086#40447086\n",
    "\n",
    "<s>\n",
    "    \n",
    "```Python\n",
    "stop_list=stopwords.words('english')+list(string.punctuation)\n",
    "def stopwords_remove(string):\n",
    "    global stop_list\n",
    "    tokens=word_tokenize(string)\n",
    "    return [s for s in tokens if s not in stop_list]\n",
    "\n",
    "text_token_list_train=df_train['mission_prgrm_spellchk'].apply(stopwords_remove)\n",
    "text_token_list_test=df_test['mission_prgrm_spellchk'].apply(stopwords_remove)\n",
    "```\n",
    "</s>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_token_list_train=df_train['mission_prgrm_spellchk']\n",
    "text_token_list_test=df_test['mission_prgrm_spellchk']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Moved to preprocessing pipeline.**\n",
    "\n",
    "<s>\n",
    "\n",
    "```Python\n",
    "# Spell check function. Return corrected word if unknown; return original word if known.\n",
    "def spellcheck(word_string_list):\n",
    "    return [SpellChecker().correction(word=s).upper() for s in word_string_list]\n",
    "\n",
    "# Parallel computing\n",
    "p = Pool(48)\n",
    "text_token_list_train=p.map(spellcheck, text_token_list_train)\n",
    "text_token_list_val=p.map(spellcheck, text_token_list_val)\n",
    "# Pool.map keep the original order of data passed to map.\n",
    "# https://stackoverflow.com/questions/41273960/python-3-does-pool-keep-the-original-order-of-data-passed-to-map\n",
    "```\n",
    "\n",
    "</s>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('and', 1), ('the', 2), ('to', 3), ('of', 4), ('in', 5)]\n"
     ]
    }
   ],
   "source": [
    "# Build word index for train and validation texts.\n",
    "tokenizer=Tokenizer()\n",
    "tokenizer.fit_on_texts(text_token_list_train.to_list()+text_token_list_test.to_list())\n",
    "print(list(tokenizer.word_index.items())[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# Save tokenizer class for developing package.\n",
    "with open('../../output/tokenizer.pkl', 'wb') as output:\n",
    "    pickle.dump(tokenizer, output, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text to sequences.\n",
    "seq_encoding_text_train=tokenizer.texts_to_sequences(text_token_list_train)\n",
    "seq_encoding_text_test=tokenizer.texts_to_sequences(text_token_list_test)\n",
    "\n",
    "# Pads sequences to the same length (i.e., prepare matrix).\n",
    "x_train=pad_sequences(sequences=seq_encoding_text_train,\n",
    "                      maxlen=max([len(s) for s in seq_encoding_text_train]), # Max length of the sequence.\n",
    "                      dtype = \"int32\", padding = \"post\", truncating = \"post\", \n",
    "                      value = 0 # Zero is used for representing None or Unknown.\n",
    "                     )\n",
    "\n",
    "x_test=pad_sequences(sequences=seq_encoding_text_test,\n",
    "                    maxlen=max([len(s) for s in seq_encoding_text_train]), # Max length of the sequence.\n",
    "                    dtype = \"int32\", padding = \"post\", truncating = \"post\", \n",
    "                    value = 0 # Zero is used for representing None or Unknown.\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2226,    64,  2561, ...,     0,     0,     0],\n",
       "       [33293,   942,   442, ...,     0,     0,     0],\n",
       "       [  354,    69,     4, ...,     0,     0,     0],\n",
       "       ...,\n",
       "       [  132,     4,     2, ...,     0,     0,     0],\n",
       "       [67880,  1274,    18, ...,     0,     0,     0],\n",
       "       [21742,  2036,   283, ...,     0,     0,     0]], dtype=int32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Convert to Compressed Sparse Row matrix; otherwise, matrix too large, result memory error.\n",
    "# # https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html\n",
    "# from scipy.sparse import csr_matrix\n",
    "# x_train=csr_matrix(x_train)\n",
    "\n",
    "# # Define resample strategy.\n",
    "# def func_resample(method, sampling_strategy, x_train_vect, y_train):\n",
    "#     if method=='ADASYN':\n",
    "#         from imblearn.over_sampling import ADASYN\n",
    "#         resample = ADASYN(sampling_strategy=sampling_strategy, random_state=42)\n",
    "#     elif method=='RandomOverSampler':\n",
    "#         from imblearn.over_sampling import RandomOverSampler\n",
    "#         resample = RandomOverSampler(sampling_strategy=sampling_strategy, random_state=42)\n",
    "#     elif method=='SMOTE':\n",
    "#         from imblearn.over_sampling import SMOTE\n",
    "#         resample = SMOTE(sampling_strategy=sampling_strategy, random_state=42)\n",
    "#     elif method=='SMOTEENN':\n",
    "#         from imblearn.combine import SMOTEENN\n",
    "#         resample = SMOTEENN(sampling_strategy=sampling_strategy, random_state=42)\n",
    "#     elif method=='SMOTETomek':\n",
    "#         from imblearn.combine import SMOTETomek\n",
    "#         resample = SMOTETomek(sampling_strategy=sampling_strategy, random_state=42)\n",
    "#     x_train_vect_res, y_train_res = resample.fit_resample(x_train_vect, y_train)\n",
    "#     return [x_train_vect_res, y_train_res]\n",
    "\n",
    "# x_train_res, y_train_res = func_resample(method='ADASYN', sampling_strategy='minority', \n",
    "#                                          x_train_vect=x_train, y_train=y_train)\n",
    "\n",
    "x_train_res, y_train_res = [x_train, y_train]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Draft codes, ignore.**\n",
    "\n",
    "<s>\n",
    "\n",
    "```Python\n",
    "### Classifier: Not using pre-trained embedding.\n",
    "\n",
    "# https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/\n",
    "embedding_layer = Embedding(input_dim=len(tokenizer.word_index), # Size of vocabulary.\n",
    "                            input_length=max([len(s) for s in seq_encoding_text_train]), # Length of input, i.e., length of padded sequence.\n",
    "                            output_dim=32, # Size of the vector space in which words will be embedded.\n",
    "                           )\n",
    "\n",
    "from keras.layers import Input, Dense, Conv1D, MaxPooling1D, Flatten, BatchNormalization, GlobalMaxPooling1D, GRU, Dropout, LSTM\n",
    "from keras.models import Model\n",
    "\n",
    "sequence_input = Input(shape=(max([len(s) for s in seq_encoding_text_train]),), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "x = Flatten()(embedded_sequences)\n",
    "x = Dense(units=512, activation='relu')(x)\n",
    "x = Dense(units=128, activation='tanh')(x)\n",
    "preds = Dense(units=len(y_train[0]), activation='softmax')(x) #softmax\n",
    "\n",
    "model = Model(sequence_input, preds)\n",
    "\n",
    "model.summary()\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam', #'rmsprop',\n",
    "              metrics=['acc',precision, recall])\n",
    "\n",
    "history = model.fit(x_train, y_train, validation_split=0.8,\n",
    "#                     validation_data=(x_val, y_val),\n",
    "                    epochs=2, batch_size=50)\n",
    "\n",
    "# Add metrics.\n",
    "# https://stackoverflow.com/questions/43076609/how-to-calculate-precision-and-recall-in-keras\n",
    "import tensorflow as tf\n",
    "def as_keras_metric(method):\n",
    "    import functools\n",
    "    from keras import backend as K\n",
    "    import tensorflow as tf\n",
    "    @functools.wraps(method)\n",
    "    def wrapper(self, args, **kwargs):\n",
    "        \"\"\" Wrapper for turning tensorflow metrics into keras metrics \"\"\"\n",
    "        value, update_op = method(self, args, **kwargs)\n",
    "        K.get_session().run(tf.local_variables_initializer())\n",
    "        with tf.control_dependencies([update_op]):\n",
    "            value = tf.identity(value)\n",
    "        return value\n",
    "    return wrapper\n",
    "\n",
    "precision = as_keras_metric(tf.metrics.precision)\n",
    "recall = as_keras_metric(tf.metrics.recall)\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "\n",
    "# define the model\n",
    "model = Sequential()\n",
    "model.add(embedding_layer)\n",
    "model.add(Flatten())\n",
    "model.add(Dense(units=512, activation='sigmoid'))\n",
    "model.add(Dense(units=256, activation='sigmoid'))\n",
    "model.add(Dense(units=len(y_train[0]), activation='relu'))\n",
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc', precision, recall])\n",
    "# summarize the model\n",
    "print(model.summary())\n",
    "\n",
    "# fit the model\n",
    "# Batch size: https://stats.stackexchange.com/questions/153531/what-is-batch-size-in-neural-network\n",
    "history=model.fit(x_train, y_train, validation_split=0.3, epochs=25, verbose=1)\n",
    "\n",
    "# evaluate the model\n",
    "loss, accuracy = model.evaluate(x_val, y_val, verbose=1)\n",
    "print('Accuracy: %f' % (accuracy*100))\n",
    "```\n",
    "\n",
    "</s>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare embedding layer.\n",
    "Use pre-trained GloVe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "EMBEDDING_DIM=100\n",
    "glove_word_vector=api.load('glove-wiki-gigaword-'+str(EMBEDDING_DIM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((len(tokenizer.word_index)+1, EMBEDDING_DIM)) # Plus one: embedding matrix starts from 0, word index starts from 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word, index in tokenizer.word_index.items():\n",
    "    try:\n",
    "        embedding_matrix[index] = glove_word_vector.get_vector(word)\n",
    "    except:\n",
    "        pass\n",
    "        # words not found in embedding index will be all-zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/\n",
    "embedding_layer = Embedding(input_dim=len(tokenizer.word_index)+1, # Size of vocabulary.\n",
    "                            input_length=max([len(s) for s in seq_encoding_text_train]), # Length of input, i.e., length of padded sequence.\n",
    "                            output_dim=EMBEDDING_DIM, # Size of the vector space in which words will be embedded.\n",
    "                            weights=[embedding_matrix],\n",
    "                            trainable=False\n",
    "                           )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic tuning of training params."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, GlobalMaxPool1D, Conv1D\n",
    "\n",
    "with tf.device('/gpu:1'): # Specify which GPU to use.\n",
    "    # define the model\n",
    "    model = Sequential()\n",
    "    model.add(embedding_layer)\n",
    "    # model.add(Flatten())\n",
    "    model.add(Conv1D(128, 5, activation='softplus'))\n",
    "    model.add(GlobalMaxPool1D())\n",
    "    model.add(Dense(units=32, activation='sigmoid'))\n",
    "    model.add(Dense(units=32, activation='softplus'))\n",
    "    model.add(Dense(units=16, activation='tanh'))\n",
    "    model.add(Dense(units=16, activation='softplus'))\n",
    "    model.add(Dense(units=len(y_train[0]), activation='softmax'))\n",
    "    # compile the model\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc', \n",
    "    #                                                                      precision, recall\n",
    "                                                                        ])\n",
    "    # summarize the model\n",
    "    print(model.summary())\n",
    "\n",
    "    # fit the model\n",
    "    history=model.fit(x_train, y_train, validation_split=0.2, epochs=1, verbose=1)\n",
    "\n",
    "    '''\n",
    "    (10, sigmoid; 9, relu): loss: 0.4414 - acc: 0.8896 - precision: 0.1604 - recall: 0.8439 - val_loss: 0.3964 - val_acc: 0.8943 - val_precision: 0.1632 - val_recall: 0.8974\n",
    "    (10, softmax; 9, relu): loss: 0.5685 - acc: 0.8888 - precision: 0.1371 - recall: 0.8051 - val_loss: 0.5532 - val_acc: 0.8895 - val_precision: 0.1394 - val_recall: 0.8151\n",
    "    (10, relu;    9, relu): loss: 0.5377 - acc: 0.8838 - precision: 0.1646 - recall: 0.8411 - val_loss: 0.4271 - val_acc: 0.8903 - val_precision: 0.1558 - val_recall: 0.8884\n",
    "    (10, relu; 9, softmax): loss: 0.2596 - acc: 0.9037 - precision: 0.1110 - recall: 0.9992 - val_loss: 0.2303 - val_acc: 0.9135 - val_precision: 0.1111 - val_recall: 1.0000\n",
    "    (10, relu; 9, sigmoid): loss: 0.2681 - acc: 0.8975 - precision: 0.1110 - recall: 0.9992 - val_loss: 0.2272 - val_acc: 0.9121 - val_precision: 0.1111 - val_recall: 1.0000\n",
    "    (10, tanh; 9, sigmoid): loss: 0.2959 - acc: 0.8940 - precision: 0.1110 - recall: 0.9992 - val_loss: 0.2572 - val_acc: 0.9066 - val_precision: 0.1111 - val_recall: 1.0000\n",
    "    (10, relu;    9, tanh): loss: 0.4241 - acc: 0.8599 - precision: 0.1110 - recall: 0.9992 - val_loss: 0.3609 - val_acc: 0.8885 - val_precision: 0.1111 - val_recall: 1.0000\n",
    "\n",
    "    (32, relu; 16, tanh; 9, sigmoid): loss: 0.2590 - acc: 0.9034 - precision: 0.1110 - recall: 0.9992 - val_loss: 0.2186 - val_acc: 0.9180 - val_precision: 0.1111 - val_recall: 1.0000\n",
    "    (32, relu; 16, tanh; 9,    relu): loss: 0.5613 - acc: 0.8654 - precision: 0.1360 - recall: 0.7180 - val_loss: 0.3850 - val_acc: 0.8889 - val_precision: 0.1394 - val_recall: 0.8721\n",
    "    (32, relu; 16, relu; 9,    relu): loss: 0.3902 - acc: 0.8876 - precision: 0.1396 - recall: 0.9328 - val_loss: 0.3618 - val_acc: 0.8889 - val_precision: 0.1377 - val_recall: 0.9532\n",
    "    (32, softmax; 16, relu; 9, relu): loss: 0.6418 - acc: 0.8916 - precision: 0.1212 - recall: 0.7585 - val_loss: 0.6097 - val_acc: 0.8942 - val_precision: 0.1276 - val_recall: 0.7584\n",
    "    (32, sigmoid; 16, relu; 9, relu): loss: 0.5048 - acc: 0.8885 - precision: 0.1421 - recall: 0.7762 - val_loss: 0.3169 - val_acc: 0.8889 - val_precision: 0.1363 - val_recall: 0.8788\n",
    "\n",
    "    (32, sigmoid; 32, sigmoid; 16, relu; 16, relu; 9, relu): loss: 0.3325 - acc: 0.8879 - precision: 0.1251 - recall: 0.9766 - val_loss: 0.4644 - val_acc: 0.7910 - val_precision: 0.1257 - val_recall: 0.9785\n",
    "    (32, softmax; 32, softmax; 16, relu; 16, relu; 9, relu): loss: 1.1298 - acc: 0.8889 - precision: 0.0933 - recall: 0.4219 - val_loss: 1.1113 - val_acc: 0.8889 - val_precision: 0.0952 - val_recall: 0.4569\n",
    "\n",
    "    1. Don't use sigmoid/softmax/tanh for output layer.\n",
    "    2. Using relu near output layer increases loss but improve precision.\n",
    "    3. sigmoid/tanh/softmax decreases loss, but also decreases precision.\n",
    "    4. Possible strategy: use softmax near input layer, use relu near output layer.\n",
    "    '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try Grid Search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continue previous work.\n",
    "df_history=pd.read_csv('../../output/grid_search_history_broad_cat.tsv', sep='\\t', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_list_done=set(map(tuple, \n",
    "                        df_history[['conv_num_filters', 'conv_kernel_size', 'conv_act', 'out_act']].values.tolist()\n",
    "                       )\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, GlobalMaxPool1D, Conv1D\n",
    "from datetime import datetime\n",
    "\n",
    "# df_history=pd.DataFrame()\n",
    "for num_filters in [32, 64, 128]:\n",
    "    for kernel_size in [3,5,7]:\n",
    "        for conv_act in ['sigmoid', 'softplus', 'tanh', 'softmax']:\n",
    "            for out_act in ['sigmoid', 'softplus', 'tanh', 'softmax']:\n",
    "                param=tuple((num_filters, kernel_size, conv_act, out_act))\n",
    "                if param not in param_list_done:\n",
    "                    t1=datetime.now()\n",
    "                    # Run NN on a specified GPU.\n",
    "                    with tf.device('/device:GPU:0'):\n",
    "                        # define the model\n",
    "                        model = Sequential()\n",
    "                        model.add(embedding_layer)\n",
    "                        # model.add(Flatten())\n",
    "                        model.add(Conv1D(num_filters, kernel_size, activation=conv_act))\n",
    "                        model.add(GlobalMaxPool1D())\n",
    "                        model.add(Dense(units=32, activation='sigmoid'))\n",
    "                        model.add(Dense(units=32, activation='softplus'))\n",
    "                        model.add(Dense(units=16, activation='tanh'))\n",
    "                        model.add(Dense(units=16, activation='softplus'))\n",
    "                        model.add(Dense(units=len(y_train[0]), activation=out_act))\n",
    "                        # compile the model\n",
    "                        model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
    "                        # F1, precision, and recall removed. https://github.com/keras-team/keras/issues/5794\n",
    "                        # fit the model\n",
    "                        history=model.fit(x_train, y_train, validation_split=0.2, epochs=50, verbose=0)\n",
    "                        y_prob = model.predict(x_val, verbose=0)\n",
    "                    # Save history.\n",
    "                    acc = history.history['acc']\n",
    "                    val_acc = history.history['val_acc']\n",
    "                    loss = history.history['loss']\n",
    "                    val_loss = history.history['val_loss']\n",
    "                    epochs = range(1, len(acc) + 1)\n",
    "                    # Calculate on validation dataset.\n",
    "                    y_classes = y_prob.argmax(axis=-1)\n",
    "                    y_classes_prob=[s.max() for s in y_prob]\n",
    "                    y_classes_val=y_val.argmax(axis=-1)\n",
    "                    df_val=pd.DataFrame({'pred':y_classes, \n",
    "                                         'true':y_classes_val, \n",
    "                                         'prob':y_classes_prob})\n",
    "                    val_acc_real=len(df_val[df_val.pred==df_val.true])/len(df_val)\n",
    "                    # Save history to datafame.\n",
    "                    df_history_temp=pd.DataFrame()\n",
    "                    df_history_temp['acc']=acc\n",
    "                    df_history_temp['val_acc']=val_acc\n",
    "                    df_history_temp['val_acc_real']=[math.nan]*(len(epochs)-1)+[val_acc_real]\n",
    "                    df_history_temp['loss']=loss\n",
    "                    df_history_temp['val_loss']=val_loss\n",
    "                    df_history_temp['epochs']=epochs\n",
    "                    df_history_temp['conv_num_filters']=[num_filters]*len(epochs)\n",
    "                    df_history_temp['conv_kernel_size']=[kernel_size]*len(epochs)\n",
    "                    df_history_temp['conv_act']=[conv_act]*len(epochs)\n",
    "                    df_history_temp['out_act']=[out_act]*len(epochs)\n",
    "                    df_history_temp['time_stamp']=[str(t1)]+[math.nan]*(len(epochs)-2)+[str(datetime.now())]\n",
    "                    df_history=df_history.append(df_history_temp, ignore_index=True)\n",
    "                    df_history.to_csv('../../output/grid_search_history_broad_cat.tsv', sep='\\t')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Impossible to iterate all the parameter combinations; can run on GPU, but memory is not enough.**\n",
    "```Python\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "param_grid = dict(num_filters=[32, 64, 128],\n",
    "                  kernel_size=[3, 5, 7],\n",
    "                  activation_conv=['sigmoid', 'softplus', 'tanh', 'softmax'],\n",
    "                  activation_1=['sigmoid', 'softplus', 'tanh', 'softmax'],\n",
    "                  activation_2=['sigmoid', 'softplus', 'tanh', 'softmax'],\n",
    "                  activation_3=['sigmoid', 'softplus', 'tanh', 'softmax'],\n",
    "                  activation_4=['sigmoid', 'softplus', 'tanh', 'softmax'],\n",
    "                  neurons_1=[9, 18, 27, 36],\n",
    "                  neurons_2=[9, 18, 27, 36],\n",
    "                  neurons_3=[9, 18, 27, 36],\n",
    "                  neurons_4=[9, 18, 27, 36],\n",
    "                  epochs=range(1,110,10)\n",
    "                 )\n",
    "len(list(ParameterGrid(param_grid)))\n",
    "Out: 25952256\n",
    "    \n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, GlobalMaxPool1D, Conv1D\n",
    "import tensorflow as tf\n",
    "import random\n",
    "\n",
    "param_grid = dict(num_filters=[32, 64, 128],\n",
    "                  kernel_size=[3, 5, 7],\n",
    "                  activation_conv=['sigmoid', 'softplus', 'tanh', 'softmax'],\n",
    "                  activation_1=['sigmoid', 'softplus', 'tanh', 'softmax'],\n",
    "                  activation_2=['sigmoid', 'softplus', 'tanh', 'softmax'],\n",
    "                  activation_3=['sigmoid', 'softplus', 'tanh', 'softmax'],\n",
    "                  activation_4=['sigmoid', 'softplus', 'tanh', 'softmax'],\n",
    "                  neurons_1=[9, 18, 27, 36],\n",
    "                  neurons_2=[9, 18, 27, 36],\n",
    "                  neurons_3=[9, 18, 27, 36],\n",
    "                  neurons_4=[9, 18, 27, 36],\n",
    "                  epochs=range(1,110,10)\n",
    "                 )\n",
    "\n",
    "def create_model(num_filters, kernel_size, \n",
    "                 activation_conv, activation_1, activation_2, activation_3, activation_4,\n",
    "                 neurons_1, neurons_2, neurons_3, neurons_4,\n",
    "                ):\n",
    "    K.clear_session() # Release memory.\n",
    "    with tf.device('/gpu:0'): # Specify the GPU to use.\n",
    "        model = Sequential()\n",
    "        model.add(embedding_layer)\n",
    "        model.add(Conv1D(num_filters, kernel_size, activation='softplus'))\n",
    "        model.add(GlobalMaxPool1D())\n",
    "        model.add(Dense(units=32, activation='sigmoid'))\n",
    "        model.add(Dense(units=32, activation='softplus'))\n",
    "        model.add(Dense(units=16, activation='tanh'))\n",
    "        model.add(Dense(units=16, activation='softplus'))\n",
    "        model.add(Dense(#units=len(y_train[0]),\n",
    "                        units=1,\n",
    "                        activation='softmax'))\n",
    "        model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "        print(model.summary())\n",
    "        return model\n",
    "\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "model=KerasClassifier(build_fn=create_model, verbose=1)\n",
    "grid_search=GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=1, refit='f1_weighted', return_train_score=True,\n",
    "                         scoring=['accuracy', 'f1_weighted', 'precision_weighted', 'recall_weighted'],\n",
    "                         pre_dispatch=0, cv=5,\n",
    "                        )\n",
    "grid_search.fit(x_train, y_train, verbose=1)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision making: Optimizing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continue previous work.\n",
    "try:\n",
    "    df_history=pd.read_csv('../../output/grid_search_history_broad_cat_optimizing.tsv', sep='\\t', index_col=0)\n",
    "except:\n",
    "    df_history=pd.DataFrame(columns=pd.read_csv('../../output/grid_search_history_broad_cat.tsv', sep='\\t', index_col=0).columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_list_done=set(map(tuple, \n",
    "                        df_history[['conv_num_filters', 'conv_kernel_size', 'conv_act', 'out_act']].values.tolist()\n",
    "                       )\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, GlobalMaxPool1D, Conv1D\n",
    "from datetime import datetime\n",
    "\n",
    "# df_history=pd.DataFrame()\n",
    "for num_filters in [128, 256, 512, 1024]:\n",
    "    for kernel_size in [3]:\n",
    "        for conv_act in ['softplus']:\n",
    "            for out_act in ['sigmoid', 'softplus', 'softmax']:\n",
    "                param=tuple((num_filters, kernel_size, conv_act, out_act))\n",
    "                if param not in param_list_done:\n",
    "                    t1=datetime.now()\n",
    "                    # Run NN on a specified GPU.\n",
    "                    with tf.device('/device:GPU:0'):\n",
    "                        # define the model\n",
    "                        model = Sequential()\n",
    "                        model.add(embedding_layer)\n",
    "                        # model.add(Flatten())\n",
    "                        model.add(Conv1D(num_filters, kernel_size, activation=conv_act))\n",
    "                        model.add(GlobalMaxPool1D())\n",
    "                        model.add(Dense(units=32, activation='sigmoid'))\n",
    "                        model.add(Dense(units=32, activation='softplus'))\n",
    "                        model.add(Dense(units=16, activation='tanh'))\n",
    "                        model.add(Dense(units=16, activation='softplus'))\n",
    "                        model.add(Dense(units=len(y_train[0]), activation=out_act))\n",
    "                        # compile the model\n",
    "                        model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
    "                        # F1, precision, and recall removed. https://github.com/keras-team/keras/issues/5794\n",
    "                        # fit the model\n",
    "                        history=model.fit(x_train, y_train, validation_split=0.2, epochs=50, verbose=0)\n",
    "                        y_prob = model.predict(x_val, verbose=0)\n",
    "                    # Save history.\n",
    "                    acc = history.history['acc']\n",
    "                    val_acc = history.history['val_acc']\n",
    "                    loss = history.history['loss']\n",
    "                    val_loss = history.history['val_loss']\n",
    "                    epochs = range(1, len(acc) + 1)\n",
    "                    # Calculate on validation dataset.\n",
    "                    y_classes = y_prob.argmax(axis=-1)\n",
    "                    y_classes_prob=[s.max() for s in y_prob]\n",
    "                    y_classes_val=y_val.argmax(axis=-1)\n",
    "                    df_val=pd.DataFrame({'pred':y_classes, \n",
    "                                         'true':y_classes_val, \n",
    "                                         'prob':y_classes_prob})\n",
    "                    val_acc_real=len(df_val[df_val.pred==df_val.true])/len(df_val)\n",
    "                    # Save history to datafame.\n",
    "                    df_history_temp=pd.DataFrame()\n",
    "                    df_history_temp['acc']=acc\n",
    "                    df_history_temp['val_acc']=val_acc\n",
    "                    df_history_temp['val_acc_real']=[math.nan]*(len(epochs)-1)+[val_acc_real]\n",
    "                    df_history_temp['loss']=loss\n",
    "                    df_history_temp['val_loss']=val_loss\n",
    "                    df_history_temp['epochs']=epochs\n",
    "                    df_history_temp['conv_num_filters']=[num_filters]*len(epochs)\n",
    "                    df_history_temp['conv_kernel_size']=[kernel_size]*len(epochs)\n",
    "                    df_history_temp['conv_act']=[conv_act]*len(epochs)\n",
    "                    df_history_temp['out_act']=[out_act]*len(epochs)\n",
    "                    df_history_temp['time_stamp']=[str(t1)]+[math.nan]*(len(epochs)-2)+[str(datetime.now())]\n",
    "                    df_history=df_history.append(df_history_temp, ignore_index=True)\n",
    "                    df_history.to_csv('../../output/grid_search_history_broad_cat_optimizing.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and test model finalist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Best configuration**\n",
    "\n",
    "_Broad Category_\n",
    "\n",
    "|acc | val_acc | val_acc_real | loss | val_loss | epochs | conv_num_filters | conv_kernel_size | conv_act | out_act|\n",
    "|--|--|--|--|--|--|--|--|--|--|\n",
    "|0.820386905 | 0.776488095 | -- | 0.613613255 | 0.738004138 | 4 | 512 | 3 | softplus | softplus|\n",
    "\n",
    "_Major Group_\n",
    "\n",
    "|acc | val_acc | val_acc_real | loss | val_loss | epochs | conv_num_filters | conv_kernel_size | conv_act | out_act|\n",
    "|--|--|--|--|--|--|--|--|--|--|\n",
    "|0.764369048 | 0.710428571 | -- | 0.888776311 | 1.120441045 | 6 | 256 | 3 | softplus | softmax|\n",
    "|0.779 | 0.71 | -- | 0.834419103 | 1.140895644 | 7 | 256 | 3 | softplus | softplus|\n",
    "\n",
    "Use `softplus`+`softplus`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, GlobalMaxPool1D, Conv1D\n",
    "from datetime import datetime\n",
    "\n",
    "with tf.device('/device:GPU:0'):\n",
    "    # define the model\n",
    "    model = Sequential()\n",
    "    model.add(embedding_layer)\n",
    "    # model.add(Flatten())\n",
    "    model.add(Conv1D(512, 3, activation='softplus'))\n",
    "    model.add(GlobalMaxPool1D())\n",
    "    model.add(Dense(units=32, activation='sigmoid'))\n",
    "    model.add(Dense(units=32, activation='softplus'))\n",
    "    model.add(Dense(units=16, activation='tanh'))\n",
    "    model.add(Dense(units=16, activation='softplus'))\n",
    "    model.add(Dense(units=len(y_train[0]), activation='softplus'))\n",
    "    # compile the model\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
    "    # F1, precision, and recall removed. https://github.com/keras-team/keras/issues/5794\n",
    "    # fit the model\n",
    "    history=model.fit(x_train_res, y_train_res, validation_split=0.2, epochs=4, verbose=1)\n",
    "    print('Test on UCF-Testing')\n",
    "    y_prob = model.predict(x_test, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall ACC: 0.8323102028129614\n"
     ]
    }
   ],
   "source": [
    "# Calculate on validation dataset.\n",
    "# From probability --> serial coding, e.g., [4, 3, 2, 55] --> categorical, e.g., [[00010...], [001000..]...]\n",
    "y_train = lb.inverse_transform(np_utils.to_categorical(y_prob.argmax(axis=-1)))\n",
    "y_train_prob=[s.max() for s in y_prob]\n",
    "df_val=pd.DataFrame({'pred':y_train, \n",
    "                     'true':df_test['broad_cat'], \n",
    "                     'prob':y_train_prob})\n",
    "print('Overall ACC:', len(df_val[df_val.pred==df_val.true])/len(df_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   pre       rec       spe        f1       geo       iba       sup\n",
      "\n",
      "          I     0.8685    0.8481    0.9839    0.8582    0.9135    0.8231      4291\n",
      "         II     0.8458    0.8813    0.9680    0.8632    0.9236    0.8457      6419\n",
      "        III     0.7608    0.8958    0.9857    0.8228    0.9397    0.8750      1861\n",
      "         IV     0.7590    0.8729    0.9650    0.8120    0.9178    0.8346      4329\n",
      "         IX     0.9007    0.8530    0.9957    0.8762    0.9216    0.8372      1701\n",
      "          V     0.8522    0.8555    0.9353    0.8538    0.8945    0.7937     11723\n",
      "         VI     0.5932    0.0803    0.9994    0.1414    0.2832    0.0729       436\n",
      "        VII     0.8791    0.7592    0.9779    0.8148    0.8616    0.7262      6749\n",
      "       VIII     0.6538    0.7687    0.9881    0.7066    0.8715    0.7428      1098\n",
      "\n",
      "avg / total     0.8363    0.8347    0.9642    0.8316    0.8941    0.7947     38607\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from imblearn.metrics import classification_report_imbalanced\n",
    "print(classification_report_imbalanced(y_true=df_test['broad_cat'], y_pred=y_train, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Python\n",
    "from imblearn.metrics import classification_report_imbalanced\n",
    "print(classification_report_imbalanced(y_true=df_test['broad_cat'], y_pred=y_train))\n",
    "```\n",
    "**Chosen:** *epochs=4, no resampling.*\n",
    "\n",
    "    Overall ACC: 0.8346931903540808\n",
    "                       pre       rec       spe        f1       geo       iba       sup\n",
    "\n",
    "              I     0.8685    0.8481    0.9839    0.8582    0.9135    0.8231      4291\n",
    "             II     0.8458    0.8813    0.9680    0.8632    0.9236    0.8457      6419\n",
    "            III     0.7608    0.8958    0.9857    0.8228    0.9397    0.8750      1861\n",
    "             IV     0.7590    0.8729    0.9650    0.8120    0.9178    0.8346      4329\n",
    "             IX     0.9007    0.8530    0.9957    0.8762    0.9216    0.8372      1701\n",
    "              V     0.8522    0.8555    0.9353    0.8538    0.8945    0.7937     11723\n",
    "             VI     0.5932    0.0803    0.9994    0.1414    0.2832    0.0729       436\n",
    "            VII     0.8791    0.7592    0.9779    0.8148    0.8616    0.7262      6749\n",
    "           VIII     0.6538    0.7687    0.9881    0.7066    0.8715    0.7428      1098\n",
    "\n",
    "    avg / total     0.8363    0.8347    0.9642    0.8316    0.8941    0.7947     38607\n",
    "\n",
    "*epochs=2, resampling: method='ADASYN', sampling_strategy='minority'.*\n",
    "\n",
    "    Overall ACC: 0.8268707747299713\n",
    "                       pre       rec       spe        f1       geo       iba       sup\n",
    "\n",
    "              I       0.93      0.77      0.99      0.84      0.87      0.74      4291\n",
    "             II       0.87      0.85      0.97      0.86      0.91      0.81      6419\n",
    "            III       0.81      0.85      0.99      0.83      0.92      0.83      1861\n",
    "             IV       0.81      0.82      0.98      0.81      0.89      0.78      4329\n",
    "             IX       0.90      0.85      1.00      0.88      0.92      0.84      1701\n",
    "              V       0.84      0.87      0.93      0.85      0.90      0.80     11723\n",
    "             VI       0.31      0.48      0.99      0.37      0.69      0.45       436\n",
    "            VII       0.78      0.81      0.95      0.80      0.88      0.76      6749\n",
    "           VIII       0.73      0.69      0.99      0.71      0.83      0.67      1098\n",
    "\n",
    "    avg / total       0.83      0.83      0.96      0.83      0.89      0.78     38607\n",
    "\n",
    "*epochs=4, resampling: method='ADASYN', sampling_strategy='minority'.*\n",
    "\n",
    "    Overall ACC: 0.819670008029632\n",
    "                       pre       rec       spe        f1       geo       iba       sup\n",
    "\n",
    "              I       0.83      0.87      0.98      0.85      0.92      0.84      4291\n",
    "             II       0.91      0.78      0.99      0.84      0.88      0.76      6419\n",
    "            III       0.83      0.82      0.99      0.82      0.90      0.80      1861\n",
    "             IV       0.88      0.70      0.99      0.78      0.83      0.67      4329\n",
    "             IX       0.80      0.92      0.99      0.85      0.95      0.90      1701\n",
    "              V       0.77      0.90      0.88      0.83      0.89      0.80     11723\n",
    "             VI       0.44      0.21      1.00      0.29      0.46      0.20       436\n",
    "            VII       0.83      0.79      0.97      0.81      0.87      0.75      6749\n",
    "           VIII       0.71      0.70      0.99      0.71      0.83      0.68      1098\n",
    "\n",
    "    avg / total       0.82      0.82      0.95      0.82      0.88      0.77     38607\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save model for developing package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('../../output/broad_category_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 46612, 100)        14090900  \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 46610, 512)        154112    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                16416     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 9)                 153       \n",
      "=================================================================\n",
      "Total params: 14,263,437\n",
      "Trainable params: 172,537\n",
      "Non-trainable params: 14,090,900\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 46612, 100)        14090900  \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 46610, 512)        154112    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                16416     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 9)                 153       \n",
      "=================================================================\n",
      "Total params: 14,263,437\n",
      "Trainable params: 172,537\n",
      "Non-trainable params: 14,090,900\n",
      "_________________________________________________________________\n",
      "None None\n"
     ]
    }
   ],
   "source": [
    "# Check the saved models.\n",
    "from keras.models import load_model\n",
    "model_broad_cat=load_model('../../output/broad_category_model.h5')\n",
    "print(model_broad_cat.summary(), model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Draft, ignore hereafter, save as reference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "# \"bo\" is for \"blue dot\"\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "# b is for \"solid blue line\"\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()   # clear figure\n",
    "history_dict = history.history\n",
    "acc_values = history_dict['acc']\n",
    "val_acc_values = history_dict['val_acc']\n",
    "\n",
    "plt.plot(epochs, acc_values, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc_values, 'b', label='Validation acc')\n",
    "\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36000/36000 [==============================] - 51s 1ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7514166666666666"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''https://machinelearningmastery.com/how-to-make-classification-and-regression-predictions-for-deep-learning-models-in-keras/\n",
    "In the case of a two-class (binary) classification problem, the sigmoid activation function is often used in the output layer. \n",
    "The predicted probability is taken as the likelihood of the observation belonging to class 1, or inverted (1 â€“ probability) to give the probability for class 0.\n",
    "In the case of a multi-class classification problem, the softmax activation function \n",
    "is often used on the output layer and the likelihood of the observation for each class is returned as a vector.\n",
    "'''\n",
    "\n",
    "y_prob = model.predict(x_val, verbose=1)\n",
    "y_classes = y_prob.argmax(axis=-1)\n",
    "y_classes_prob=[s.max() for s in y_prob]\n",
    "y_classes_val=y_val.argmax(axis=-1)\n",
    "\n",
    "df_val=pd.DataFrame({'pred':y_classes, \n",
    "                     'true':y_classes_val, \n",
    "                     'prob':y_classes_prob})\n",
    "len(df_val[df_val.pred==df_val.true])/len(df_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120000/120000 [==============================] - 118s 980us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8065"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''https://machinelearningmastery.com/how-to-make-classification-and-regression-predictions-for-deep-learning-models-in-keras/\n",
    "In the case of a two-class (binary) classification problem, the sigmoid activation function is often used in the output layer. \n",
    "The predicted probability is taken as the likelihood of the observation belonging to class 1, or inverted (1 â€“ probability) to give the probability for class 0.\n",
    "In the case of a multi-class classification problem, the softmax activation function \n",
    "is often used on the output layer and the likelihood of the observation for each class is returned as a vector.\n",
    "'''\n",
    "\n",
    "y_prob = model.predict(x_val, verbose=1)\n",
    "y_classes = y_prob.argmax(axis=-1)\n",
    "y_classes_prob=[s.max() for s in y_prob]\n",
    "y_classes_val=y_val.argmax(axis=-1)\n",
    "\n",
    "df_val=pd.DataFrame({'pred':y_classes, \n",
    "                     'true':y_classes_val, \n",
    "                     'prob':y_classes_prob})\n",
    "len(df_val[df_val.pred==df_val.true])/len(df_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9097281678082192 0.5191111111111111\n"
     ]
    }
   ],
   "source": [
    "df_95=df_val[df_val.prob>.95]\n",
    "print(len(df_95[df_95.pred==df_95.true])/len(df_95), len(df_95)/len(df_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred</th>\n",
       "      <th>true</th>\n",
       "      <th>prob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6838</th>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0.739892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16717</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.981657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9961</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.988655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15989</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.500976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34186</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.821712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26873</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.976101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33774</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.981773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25580</th>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>0.836229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18159</th>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0.980795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21771</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.995547</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       pred  true      prob\n",
       "6838      3     5  0.739892\n",
       "16717     1     1  0.981657\n",
       "9961      3     3  0.988655\n",
       "15989     0     1  0.500976\n",
       "34186     5     5  0.821712\n",
       "26873     1     1  0.976101\n",
       "33774     1     1  0.981773\n",
       "25580     7     7  0.836229\n",
       "18159     1     7  0.980795\n",
       "21771     3     3  0.995547"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_val.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"8\" halign=\"left\">prob</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pred</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14574.0</td>\n",
       "      <td>0.947898</td>\n",
       "      <td>0.140834</td>\n",
       "      <td>0.181094</td>\n",
       "      <td>0.994786</td>\n",
       "      <td>0.997717</td>\n",
       "      <td>0.998336</td>\n",
       "      <td>0.999913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20047.0</td>\n",
       "      <td>0.938727</td>\n",
       "      <td>0.150843</td>\n",
       "      <td>0.199948</td>\n",
       "      <td>0.982048</td>\n",
       "      <td>0.996910</td>\n",
       "      <td>0.998982</td>\n",
       "      <td>0.999559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6813.0</td>\n",
       "      <td>0.937554</td>\n",
       "      <td>0.145990</td>\n",
       "      <td>0.230144</td>\n",
       "      <td>0.983532</td>\n",
       "      <td>0.995575</td>\n",
       "      <td>0.997143</td>\n",
       "      <td>0.999125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13946.0</td>\n",
       "      <td>0.919894</td>\n",
       "      <td>0.163432</td>\n",
       "      <td>0.202485</td>\n",
       "      <td>0.950082</td>\n",
       "      <td>0.994085</td>\n",
       "      <td>0.997833</td>\n",
       "      <td>0.999361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4788.0</td>\n",
       "      <td>0.915918</td>\n",
       "      <td>0.158874</td>\n",
       "      <td>0.223830</td>\n",
       "      <td>0.934571</td>\n",
       "      <td>0.991566</td>\n",
       "      <td>0.997457</td>\n",
       "      <td>0.999467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>34993.0</td>\n",
       "      <td>0.951979</td>\n",
       "      <td>0.125865</td>\n",
       "      <td>0.173467</td>\n",
       "      <td>0.987138</td>\n",
       "      <td>0.997629</td>\n",
       "      <td>0.998319</td>\n",
       "      <td>0.999602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2456.0</td>\n",
       "      <td>0.769482</td>\n",
       "      <td>0.228869</td>\n",
       "      <td>0.182271</td>\n",
       "      <td>0.593661</td>\n",
       "      <td>0.862013</td>\n",
       "      <td>0.967078</td>\n",
       "      <td>0.996433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>18602.0</td>\n",
       "      <td>0.923707</td>\n",
       "      <td>0.159183</td>\n",
       "      <td>0.198981</td>\n",
       "      <td>0.957083</td>\n",
       "      <td>0.996431</td>\n",
       "      <td>0.998265</td>\n",
       "      <td>0.999453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3781.0</td>\n",
       "      <td>0.864114</td>\n",
       "      <td>0.213472</td>\n",
       "      <td>0.189253</td>\n",
       "      <td>0.806864</td>\n",
       "      <td>0.985537</td>\n",
       "      <td>0.997941</td>\n",
       "      <td>0.999207</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         prob                                                              \\\n",
       "        count      mean       std       min       25%       50%       75%   \n",
       "pred                                                                        \n",
       "0     14574.0  0.947898  0.140834  0.181094  0.994786  0.997717  0.998336   \n",
       "1     20047.0  0.938727  0.150843  0.199948  0.982048  0.996910  0.998982   \n",
       "2      6813.0  0.937554  0.145990  0.230144  0.983532  0.995575  0.997143   \n",
       "3     13946.0  0.919894  0.163432  0.202485  0.950082  0.994085  0.997833   \n",
       "4      4788.0  0.915918  0.158874  0.223830  0.934571  0.991566  0.997457   \n",
       "5     34993.0  0.951979  0.125865  0.173467  0.987138  0.997629  0.998319   \n",
       "6      2456.0  0.769482  0.228869  0.182271  0.593661  0.862013  0.967078   \n",
       "7     18602.0  0.923707  0.159183  0.198981  0.957083  0.996431  0.998265   \n",
       "8      3781.0  0.864114  0.213472  0.189253  0.806864  0.985537  0.997941   \n",
       "\n",
       "                \n",
       "           max  \n",
       "pred            \n",
       "0     0.999913  \n",
       "1     0.999559  \n",
       "2     0.999125  \n",
       "3     0.999361  \n",
       "4     0.999467  \n",
       "5     0.999602  \n",
       "6     0.996433  \n",
       "7     0.999453  \n",
       "8     0.999207  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_val.groupby('pred')[['prob']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"8\" halign=\"left\">prob</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pred</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3878.0</td>\n",
       "      <td>0.893846</td>\n",
       "      <td>0.169222</td>\n",
       "      <td>0.200335</td>\n",
       "      <td>0.890413</td>\n",
       "      <td>0.978879</td>\n",
       "      <td>0.989682</td>\n",
       "      <td>0.994728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5504.0</td>\n",
       "      <td>0.870570</td>\n",
       "      <td>0.164746</td>\n",
       "      <td>0.188946</td>\n",
       "      <td>0.826522</td>\n",
       "      <td>0.955931</td>\n",
       "      <td>0.977011</td>\n",
       "      <td>0.985090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1998.0</td>\n",
       "      <td>0.871377</td>\n",
       "      <td>0.178440</td>\n",
       "      <td>0.190420</td>\n",
       "      <td>0.817443</td>\n",
       "      <td>0.967236</td>\n",
       "      <td>0.989169</td>\n",
       "      <td>0.995268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4295.0</td>\n",
       "      <td>0.879368</td>\n",
       "      <td>0.177278</td>\n",
       "      <td>0.184492</td>\n",
       "      <td>0.840099</td>\n",
       "      <td>0.973080</td>\n",
       "      <td>0.992297</td>\n",
       "      <td>0.997352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1517.0</td>\n",
       "      <td>0.836238</td>\n",
       "      <td>0.184050</td>\n",
       "      <td>0.189743</td>\n",
       "      <td>0.744091</td>\n",
       "      <td>0.923662</td>\n",
       "      <td>0.979553</td>\n",
       "      <td>0.990551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10663.0</td>\n",
       "      <td>0.871253</td>\n",
       "      <td>0.171818</td>\n",
       "      <td>0.206576</td>\n",
       "      <td>0.822933</td>\n",
       "      <td>0.959143</td>\n",
       "      <td>0.986311</td>\n",
       "      <td>0.994159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>343.0</td>\n",
       "      <td>0.457318</td>\n",
       "      <td>0.129805</td>\n",
       "      <td>0.175460</td>\n",
       "      <td>0.357263</td>\n",
       "      <td>0.448693</td>\n",
       "      <td>0.557864</td>\n",
       "      <td>0.725925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>6602.0</td>\n",
       "      <td>0.823981</td>\n",
       "      <td>0.209800</td>\n",
       "      <td>0.192370</td>\n",
       "      <td>0.682801</td>\n",
       "      <td>0.936526</td>\n",
       "      <td>0.990312</td>\n",
       "      <td>0.997062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1200.0</td>\n",
       "      <td>0.692921</td>\n",
       "      <td>0.200447</td>\n",
       "      <td>0.202602</td>\n",
       "      <td>0.532967</td>\n",
       "      <td>0.748837</td>\n",
       "      <td>0.869758</td>\n",
       "      <td>0.951971</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         prob                                                              \\\n",
       "        count      mean       std       min       25%       50%       75%   \n",
       "pred                                                                        \n",
       "0      3878.0  0.893846  0.169222  0.200335  0.890413  0.978879  0.989682   \n",
       "1      5504.0  0.870570  0.164746  0.188946  0.826522  0.955931  0.977011   \n",
       "2      1998.0  0.871377  0.178440  0.190420  0.817443  0.967236  0.989169   \n",
       "3      4295.0  0.879368  0.177278  0.184492  0.840099  0.973080  0.992297   \n",
       "4      1517.0  0.836238  0.184050  0.189743  0.744091  0.923662  0.979553   \n",
       "5     10663.0  0.871253  0.171818  0.206576  0.822933  0.959143  0.986311   \n",
       "6       343.0  0.457318  0.129805  0.175460  0.357263  0.448693  0.557864   \n",
       "7      6602.0  0.823981  0.209800  0.192370  0.682801  0.936526  0.990312   \n",
       "8      1200.0  0.692921  0.200447  0.202602  0.532967  0.748837  0.869758   \n",
       "\n",
       "                \n",
       "           max  \n",
       "pred            \n",
       "0     0.994728  \n",
       "1     0.985090  \n",
       "2     0.995268  \n",
       "3     0.997352  \n",
       "4     0.990551  \n",
       "5     0.994159  \n",
       "6     0.725925  \n",
       "7     0.997062  \n",
       "8     0.951971  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_val.groupby('pred')[['prob']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_13 (InputLayer)        (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "embedding_4 (Embedding)      (None, 1000, 200)         6692600   \n",
      "_________________________________________________________________\n",
      "flatten_13 (Flatten)         (None, 200000)            0         \n",
      "_________________________________________________________________\n",
      "dense_38 (Dense)             (None, 512)               102400512 \n",
      "_________________________________________________________________\n",
      "dense_39 (Dense)             (None, 128)               65664     \n",
      "_________________________________________________________________\n",
      "dense_40 (Dense)             (None, 9)                 1161      \n",
      "=================================================================\n",
      "Total params: 109,159,937\n",
      "Trainable params: 102,467,337\n",
      "Non-trainable params: 6,692,600\n",
      "_________________________________________________________________\n",
      "Train on 8399 samples, validate on 33601 samples\n",
      "Epoch 1/2\n",
      "8399/8399 [==============================] - 125s 15ms/step - loss: 0.2489 - acc: 0.9057 - precision: 0.1104 - recall: 0.9940 - val_loss: 0.2185 - val_acc: 0.9155 - val_precision: 0.1111 - val_recall: 1.0000\n",
      "Epoch 2/2\n",
      "8399/8399 [==============================] - 121s 14ms/step - loss: 0.1594 - acc: 0.9394 - precision: 0.1111 - recall: 1.0000 - val_loss: 0.2231 - val_acc: 0.9167 - val_precision: 0.1111 - val_recall: 1.0000\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Dense, Conv1D, MaxPooling1D, Flatten, BatchNormalization, GlobalMaxPooling1D, GRU, Dropout, LSTM\n",
    "from keras.models import Model\n",
    "\n",
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "'''\n",
    "x = GRU(units=128, activation='tanh', return_sequences=True)(embedded_sequences)\n",
    "\n",
    "x = LSTM(units=256, activation='tanh', return_sequences=False)(embedded_sequences)\n",
    "x = Dropout(0.2)(x)\n",
    "\n",
    "#x = LSTM(units=128, activation='tanh', return_sequences=True)(x)\n",
    "#x = Dropout(0.2)(x)\n",
    "'''\n",
    "# x = Dropout(0.2)(x)\n",
    "x = Flatten()(embedded_sequences)\n",
    "x = Dense(units=512, activation='relu')(x)\n",
    "x = Dense(units=128, activation='tanh')(x)\n",
    "preds = Dense(units=9, activation='softmax')(x) #softmax\n",
    "\n",
    "# x = Dense(units=512, activation='relu')(x)\n",
    "# x = Dense(units=128, activation='relu')(x)\n",
    "# preds = Dense(units=25, activation='sigmoid')(x) #softmax\n",
    "\n",
    "model = Model(sequence_input, preds)\n",
    "\n",
    "model.summary()\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam', #'rmsprop',\n",
    "              metrics=['acc',precision, recall])\n",
    "\n",
    "history = model.fit(x_train, y_train, validation_split=0.8,\n",
    "#                     validation_data=(x_val, y_val),\n",
    "                    epochs=2, batch_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 25s 412us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.19112927243113517, 0.9295742606123288, 0.1111111119389534, 1.0]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score = model.evaluate(x_val, y_val, batch_size=500, verbose=1)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37694"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_val[df_val.pred==df_val.true])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 7, 5, 3, 4, 2])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_95.pred.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
