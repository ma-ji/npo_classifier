{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inside:  [18100, 18200] 1520\n",
      "inside:  [18200, 18300] 1521\n",
      "inside:  [18300, 18400] 1522\n",
      "inside:  [18000, 18100] 1519\n",
      "inside:  [18400, 18500] 1521\n",
      "inside:  [18500, 18600] 1520\n",
      "inside:  [18600, 18700] 1522\n",
      "inside:  [18700, 18800] 1519\n"
     ]
    },
    {
     "ename": "EOFError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRemoteTraceback\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;31mRemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/Users/Rushi/anaconda/lib/python3.6/site-packages/pandas/io/pickle.py\", line 166, in try_read\n    return read_wrapper(lambda f: pkl.load(f))\n  File \"/Users/Rushi/anaconda/lib/python3.6/site-packages/pandas/io/pickle.py\", line 149, in read_wrapper\n    return func(f)\n  File \"/Users/Rushi/anaconda/lib/python3.6/site-packages/pandas/io/pickle.py\", line 166, in <lambda>\n    return read_wrapper(lambda f: pkl.load(f))\nEOFError: Ran out of input\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/Users/Rushi/anaconda/lib/python3.6/site-packages/pandas/io/pickle.py\", line 171, in try_read\n    lambda f: pc.load(f, encoding=encoding, compat=False))\n  File \"/Users/Rushi/anaconda/lib/python3.6/site-packages/pandas/io/pickle.py\", line 149, in read_wrapper\n    return func(f)\n  File \"/Users/Rushi/anaconda/lib/python3.6/site-packages/pandas/io/pickle.py\", line 171, in <lambda>\n    lambda f: pc.load(f, encoding=encoding, compat=False))\n  File \"/Users/Rushi/anaconda/lib/python3.6/site-packages/pandas/compat/pickle_compat.py\", line 212, in load\n    return up.load()\n  File \"/Users/Rushi/anaconda/lib/python3.6/pickle.py\", line 1048, in load\n    raise EOFError\nEOFError\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/Users/Rushi/anaconda/lib/python3.6/site-packages/pandas/io/pickle.py\", line 177, in read_pickle\n    return try_read(path)\n  File \"/Users/Rushi/anaconda/lib/python3.6/site-packages/pandas/io/pickle.py\", line 175, in try_read\n    lambda f: pc.load(f, encoding=encoding, compat=True))\n  File \"/Users/Rushi/anaconda/lib/python3.6/site-packages/pandas/io/pickle.py\", line 149, in read_wrapper\n    return func(f)\n  File \"/Users/Rushi/anaconda/lib/python3.6/site-packages/pandas/io/pickle.py\", line 175, in <lambda>\n    lambda f: pc.load(f, encoding=encoding, compat=True))\n  File \"/Users/Rushi/anaconda/lib/python3.6/site-packages/pandas/compat/pickle_compat.py\", line 212, in load\n    return up.load()\n  File \"/Users/Rushi/anaconda/lib/python3.6/pickle.py\", line 1048, in load\n    raise EOFError\nEOFError\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/Users/Rushi/anaconda/lib/python3.6/site-packages/pandas/io/pickle.py\", line 166, in try_read\n    return read_wrapper(lambda f: pkl.load(f))\n  File \"/Users/Rushi/anaconda/lib/python3.6/site-packages/pandas/io/pickle.py\", line 149, in read_wrapper\n    return func(f)\n  File \"/Users/Rushi/anaconda/lib/python3.6/site-packages/pandas/io/pickle.py\", line 166, in <lambda>\n    return read_wrapper(lambda f: pkl.load(f))\nEOFError: Ran out of input\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/Users/Rushi/anaconda/lib/python3.6/site-packages/pandas/io/pickle.py\", line 171, in try_read\n    lambda f: pc.load(f, encoding=encoding, compat=False))\n  File \"/Users/Rushi/anaconda/lib/python3.6/site-packages/pandas/io/pickle.py\", line 149, in read_wrapper\n    return func(f)\n  File \"/Users/Rushi/anaconda/lib/python3.6/site-packages/pandas/io/pickle.py\", line 171, in <lambda>\n    lambda f: pc.load(f, encoding=encoding, compat=False))\n  File \"/Users/Rushi/anaconda/lib/python3.6/site-packages/pandas/compat/pickle_compat.py\", line 212, in load\n    return up.load()\n  File \"/Users/Rushi/anaconda/lib/python3.6/pickle.py\", line 1048, in load\n    raise EOFError\nEOFError\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/Users/Rushi/anaconda/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n    result = (True, func(*args, **kwds))\n  File \"/Users/Rushi/anaconda/lib/python3.6/multiprocessing/pool.py\", line 44, in mapstar\n    return list(map(*args))\n  File \"<ipython-input-8-42b3d5368104>\", line 252, in build_coredataV3\n    written = writedata(masterdata)\n  File \"<ipython-input-8-42b3d5368104>\", line 142, in writedata\n    masterdata = pd.concat([masterdata, pd.read_pickle('Masterdata_2015.pkl.gz')])\n  File \"/Users/Rushi/anaconda/lib/python3.6/site-packages/pandas/io/pickle.py\", line 180, in read_pickle\n    return try_read(path, encoding='latin1')\n  File \"/Users/Rushi/anaconda/lib/python3.6/site-packages/pandas/io/pickle.py\", line 175, in try_read\n    lambda f: pc.load(f, encoding=encoding, compat=True))\n  File \"/Users/Rushi/anaconda/lib/python3.6/site-packages/pandas/io/pickle.py\", line 149, in read_wrapper\n    return func(f)\n  File \"/Users/Rushi/anaconda/lib/python3.6/site-packages/pandas/io/pickle.py\", line 175, in <lambda>\n    lambda f: pc.load(f, encoding=encoding, compat=True))\n  File \"/Users/Rushi/anaconda/lib/python3.6/site-packages/pandas/compat/pickle_compat.py\", line 212, in load\n    return up.load()\n  File \"/Users/Rushi/anaconda/lib/python3.6/pickle.py\", line 1048, in load\n    raise EOFError\nEOFError\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mEOFError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-42b3d5368104>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mPool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocesses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0magents\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m     \u001b[0;31m#print(multiprocessing.current_process())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 280\u001b[0;31m     \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuild_coredataV3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    281\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[0;31m#df=pd.read_pickle(\"/Users/Rushi/Documents/GRAFall2018/Data/Masterdata_\"+str(year)+\".pkl.gz\", 'gzip')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[1;32m    258\u001b[0m         \u001b[0;32min\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mthat\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mreturned\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m         '''\n\u001b[0;32m--> 260\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapstar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    261\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstarmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    606\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 608\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mEOFError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#DEVELOPING DATASET FOR EACH YEAR\n",
    "#Goal: \n",
    "\n",
    "#To link [EIN, URL] from IRS to [EIN, NTEE1] from NCCS files, and get Mission or Purpose statements for each EIN.\n",
    "#To- do List: \n",
    "\n",
    "#[+] 1. Get [EIN, URL] from IRS. -> df_irs \n",
    "#[+] 2. Get [EIN, NTEE1] from NCCS. -> df_nccs \n",
    "#[+] 3. Intersect [df_irs, df_nccs] -> df_inter -> \"link'year'.csv\" \n",
    "#[+] 4. Visit each URL in df_inter and get data from relevant tabs. \n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import re, requests, string, os, gzip, pickle, sys\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from multiprocessing import Pool\n",
    "#use regex instead of beautifulsoup, if possible.\n",
    "\n",
    "year_list = [2015, 2014, 2013, 2012, 2011]\n",
    "year = year_list[0]\n",
    "\n",
    "#When we create a data frame with pandas â‰¤ 0.19.2 and pickle it (using pickle.dump), \n",
    "#it is not possible to unpickle it using pandas 0.20.1.\n",
    "#https://github.com/pandas-dev/pandas/issues/16474\n",
    "\n",
    "\n",
    "#Create the file if it does not exist, if the file exists: jump to step 4\n",
    "if(os.path.isfile(\"/Users/Rushi/Documents/GRAFall2018/Data/intermediary/link\"+str(year)+\".pkl.gz\")==False):\n",
    "    #Step 1. Get [EIN, URL] from IRS\n",
    "    irsfile = pd.read_json('https://s3.amazonaws.com/irs-form-990/index_'+str(year)+'.json')\n",
    "    ein_url=list(map(list, zip(*[[s['EIN'] for s in irsfile['Filings'+str(year)]], [s['URL'] for s in irsfile['Filings'+str(year)]]])))\n",
    "    df_irs = pd.DataFrame(ein_url, columns=['EIN', 'URL'])\n",
    "\n",
    "\n",
    "    #Step 2. Get [EIN, NTEE1] from NCCS\n",
    "    df_nccs1 = pd.read_csv('https://nccs-data.urban.org/data/core/'+str(year)+'/nccs.core'+str(year)+'pc.csv')\n",
    "    df_nccs = df_nccs1[['EIN', 'NTEE1']]\n",
    "\n",
    "\n",
    "    #Step 3. Get common URL from df_irs and df_nccs and make corresponding list of [EIN, URL, NTEE1]\n",
    "    df_nccs['EIN'] = df_nccs['EIN'].apply(str)\n",
    "    df_inter = pd.DataFrame(pd.merge(df_nccs, df_irs, how='outer', on=['EIN']), columns=['EIN','NTEE1','URL'])\n",
    "    df_inter.columns = ['EIN', 'NTEE', 'IRS_URL']\n",
    "\n",
    "    #file size limit on Github - 25 MB\n",
    "    df_inter.to_pickle('/Users/Rushi/Documents/GRAFall2018/Data/intermediary/link'+str(year)+'.pkl.gz', 'gzip')\n",
    "    \n",
    "    del irsfile, ein_url, df_irs, df_nccs1, df_nccs, df_inter\n",
    "\n",
    "\n",
    "#Step 4: Provide list of tags to search from:\n",
    "\n",
    "#Year\tMissionTags\tPurposeTags\n",
    "#2015, 2014, 2013\tActivityOrMissionDesc\tOtherExemptPurposeExpendGrp, TotalExemptPurposeExpendGrp\n",
    "#2012, 2011\tActivityOrMissionDescription\tOtherExemptPurposeExpenditures, TotalExemptPurposeExpenditures\n",
    "#FormType\tMission\tPurpose\n",
    "#990\tYes\tNo\n",
    "#990EZ\tNo\tYes\n",
    "#990PF\tNo\tNo\n",
    "\n",
    "\n",
    "#The original tag names are converted into small letters while parsing, for e.g. 'ActivityOrMissionDesc' is parsed as 'activityormissiondesc'.\n",
    "#So, we will provide original tags converted into small letters for comparision.\n",
    "#https://github.com/lecy/Open-Data-for-Nonprofit-Research/blob/master/Build_IRS990_E-Filer_Datasets/Data_Dictionary.md\n",
    "#year| tag| line#\n",
    "\n",
    "#alltags = ['ActivityOrMissionDesc', 'ActivityOrMissionDescription',\n",
    "#           'OtherExemptPurposeExpenGrp', 'OtherExemptPurposeExpenditures',\n",
    "#           'TotalExemptPurposeExpendGrp', 'TotalExemptPurposeExpenditures'] \n",
    "    \n",
    "    \n",
    "alltags = ['activityormissiondesc', 'activityormissiondescription',\n",
    "           'otherexemptpurposeexpengrp', 'otherexemptpurposeexpenditures',\n",
    "           'totalexemptpurposeexpendgrp', 'totalexemptpurposeexpenditures']\n",
    "\n",
    "\n",
    "df = pd.read_pickle(\"/Users/Rushi/Documents/GRAFall2018/Data/intermediary/link\"+str(year)+\".pkl.gz\", 'gzip')\n",
    "no_url = df[pd.isnull(df['IRS_URL'])]\n",
    "df_inter = df[~ pd.isnull(df['IRS_URL'])]\n",
    "\n",
    "masterdata=pd.DataFrame(no_url, columns=['EIN', 'NTEE', 'IRS_URL', 'TEXT', 'TEXTTYPE', 'YEAR'])\n",
    "#masterdata.to_pickle(\"/Users/Rushi/Documents/GRAFall2018/Data/Masterdata_\"+str(year)+\".pkl.gz\", 'gzip')\n",
    "\n",
    "#to overcome RecursionError: maximum recursion depth exceeded\n",
    "sys.setrecursionlimit(10000000)\n",
    "\n",
    "def build_coredata(r):\n",
    "    \n",
    "    print(\"inside: \",r)\n",
    "    process_id = os.getpid()\n",
    "    dest_file = '/Users/Rushi/Documents/GRAFall2018/Data/intermediary/'+str(year)+'/Masterdata_'+str(process_id)+'.pkl.gz'\n",
    "    \n",
    "    #assign local dataframe\n",
    "    masterdata=pd.DataFrame(columns=['EIN', 'NTEE', 'IRS_URL', 'TEXT', 'TEXTTYPE', 'YEAR'])\n",
    "    for i in range(r[0], r[1]):\n",
    "        row = df_inter.values[int(i)]\n",
    "        flag = 0\n",
    "\n",
    "        \n",
    "        page = requests.get(str(row[2]), timeout=5)\n",
    "        \n",
    "        bss = bs(page.text, 'html.parser')\n",
    "        \n",
    "        #Add record with tag name and string for each record tag matched in list \"alltags\"\n",
    "        for tag in bss.find_all():\n",
    "            if tag.name in alltags:\n",
    "                masterdata.loc[len(masterdata)] = [str(row[0]), row[1], row[2], tag.string, tag.name, str(year)]\n",
    "                flag = 1\n",
    "       \n",
    "        #Add record with EIN, NTEE and URL if no tags are found.\n",
    "        if(flag == 0):\n",
    "            masterdata.loc[len(masterdata)] = [str(row[0]), row[1], row[2],'','', str(year)]  \n",
    "        \n",
    "        #Data storage: to_pickle overwrites the existing file.\n",
    "        #Here \"Dataframe_2015.pkl.gz\" file will not be appended by to_pickle but only existing values will appear.\n",
    "        #First, we have to read the .pkl.gz, concat the dataframe, and write new dataframe as pkl.gz\n",
    "        #However, in multiprocessing, doing the same may lead to loss of data\n",
    "        #Hence, each thread will write in it's seperate .pkl.gz, and at the end all will be combined.\n",
    "        \n",
    "        #For every 200 records, move the data from dataframe to .csv file on disk.\n",
    "        if(i%200==0):\n",
    "            print(\"reached: \",i, \" on process: \", process_id)\n",
    "            #Since, pickle doesn't append file but rewrites it, we will first take the exisiting data and concat it with \n",
    "            #new data before storing into .pkl.gz\n",
    "            if(os.path.isfile(dest_file)==True):\n",
    "                masterdata = pd.concat([masterdata, pd.read_pickle(dest_file, 'gzip')])\n",
    "                \n",
    "            masterdata.to_pickle(dest_file, 'gzip')\n",
    "            del masterdata\n",
    "            masterdata=pd.DataFrame(columns=['EIN', 'NTEE', 'IRS_URL', 'TEXT', 'TEXTTYPE', 'YEAR'])\n",
    "            \n",
    "    #move remainig data\n",
    "    if(os.path.isfile(dest_file)==True):\n",
    "                masterdata = pd.concat([masterdata, pd.read_pickle(dest_file, 'gzip')])\n",
    "                \n",
    "    masterdata.to_pickle(dest_file, 'gzip')\n",
    "    del masterdata\n",
    "    \n",
    "    \n",
    "def writedata(masterdata):\n",
    "    masterdata = pd.concat([masterdata, pd.read_pickle('Masterdata_2015.pkl.gz')])           \n",
    "    masterdata.to_pickle('Masterdata_2015.pkl.gz')\n",
    "    return \"Added to pickle!\"\n",
    "    \n",
    "    \n",
    "def build_coredataV2(r):\n",
    "    \n",
    "    process_id = os.getpid()\n",
    "    print(\"inside: \",r, process_id)\n",
    "    #filedir = '/Users/Rushi/Documents/GRAFall2018/Data/intermediary/'+str(year)+'/Masterdata_'+str(process_id)+'.pkl.gz'\n",
    "    #dest_file = os.path.join(filedir, ('%s_annots.pkl' % 'imagesetfile'))\n",
    "    #dest_file = filedir\n",
    "    #print(filedir)\n",
    "    \n",
    "    #assign local dataframe\n",
    "    masterdata=pd.DataFrame(columns=['EIN', 'NTEE', 'IRS_URL', 'TEXT', 'TEXTTYPE', 'YEAR'])\n",
    "    #for i in tqdm(range(r[0], r[1])):\n",
    "    for i in range(r[0], r[1]):\n",
    "        row = df_inter.values[int(i)]\n",
    "        flag = 0\n",
    "\n",
    "        page = requests.get(str(row[2]), timeout=5)\n",
    "        bss = bs(page.text, 'html.parser')\n",
    "        \n",
    "        #Add record with tag name and string for each record tag matched in list \"alltags\"\n",
    "        for tag in bss.find_all():\n",
    "            if tag.name in alltags:\n",
    "                masterdata.loc[len(masterdata)] = [str(row[0]), row[1], row[2], tag.string, tag.name, str(year)]\n",
    "                flag = 1\n",
    "       \n",
    "        #Add record with EIN, NTEE and URL if no tags are found.\n",
    "        if(flag == 0):\n",
    "            masterdata.loc[len(masterdata)] = [str(row[0]), row[1], row[2],'','', str(year)]\n",
    "        \n",
    "        \n",
    "        #Data storage: to_pickle overwrites the existing file.\n",
    "        #Here \"Dataframe_2015.pkl.gz\" file will not be appended by to_pickle but only existing values will appear.\n",
    "        #First, we have to read the .pkl.gz, concat the dataframe, and write new dataframe as pkl.gz\n",
    "        #However, in multiprocessing, doing the same may lead to loss of data\n",
    "        #Hence, each thread will write in it's seperate .pkl.gz, and at the end all will be combined.\n",
    "        \n",
    "    #Since, pickle doesn't append file but rewrites it, we will first take the exisiting data and concat it with \n",
    "    #new data before storing into .pkl.gz\n",
    "    \n",
    "    written = writedata(masterdata, str(year)+'/Masterdata_'+str(process_id)+'.pkl.gz')\n",
    "    \n",
    "    #if(os.path.isfile(str(year)+'/Masterdata_'+str(process_id)+'.pkl.gz')):\n",
    "    #    masterdata = pd.concat([masterdata, pd.read_pickle(str(year)+'/Masterdata_'+str(process_id)+'.pkl.gz')])\n",
    "    #    print(\"length already: \",len(pd.read_pickle(str(year)+'/Masterdata_'+str(process_id)+'.pkl.gz')))\n",
    "        \n",
    "    #print(len(masterdata), os.getpid())\n",
    "    \n",
    "    #print(\"Here!!\")\n",
    "                \n",
    "    #masterdata.to_pickle(str(year)+'/Masterdata_'+str(process_id)+'.pkl.gz')\n",
    "    \n",
    "    masterdata=pd.DataFrame(columns=['EIN', 'NTEE', 'IRS_URL', 'TEXT', 'TEXTTYPE', 'YEAR'])\n",
    "    \n",
    "    print(r, written, process_id)\n",
    "    \n",
    "\n",
    "def writefile(masterdata, r):\n",
    "    \n",
    "    #totalmasterdata = \n",
    "    if(os.path.isfile(path)):\n",
    "        masterdata = pd.concat([masterdata, pd.read_pickle(str(year)+'/Masterdata_'+str(process_id)+'.pkl.gz')])\n",
    "                \n",
    "    masterdata.to_pickle(path)\n",
    "    return \"Added to Dataset!\"\n",
    "    \n",
    "    \n",
    "def build_coredataV3(r):\n",
    "    \n",
    "    process_id = os.getpid()\n",
    "    print(\"inside: \",r, process_id)\n",
    "    masterdata=pd.DataFrame(columns=['EIN', 'NTEE', 'IRS_URL', 'TEXT', 'TEXTTYPE', 'YEAR'])\n",
    "    #for i in tqdm(range(r[0], r[1])):\n",
    "    for i in range(r[0], r[1]):\n",
    "        row = df_inter.values[int(i)]\n",
    "        flag = 0\n",
    "\n",
    "        page = requests.get(str(row[2]), timeout=5)\n",
    "        bss = bs(page.text, 'html.parser')\n",
    "        \n",
    "        #Add record with tag name and string for each record tag matched in list \"alltags\"\n",
    "        for tag in bss.find_all():\n",
    "            if tag.name in alltags:\n",
    "                masterdata.loc[len(masterdata)] = [str(row[0]), row[1], row[2], tag.string, tag.name, str(year)]\n",
    "                flag = 1\n",
    "       \n",
    "        #Add record with EIN, NTEE and URL if no tags are found.\n",
    "        if(flag == 0):\n",
    "            masterdata.loc[len(masterdata)] = [str(row[0]), row[1], row[2],'','', str(year)]\n",
    "    \n",
    "    written = writedata(masterdata)\n",
    "    print(r, written, process_id)\n",
    "    \n",
    "\n",
    "#numbers of URLs accesed by each thread: must be greater than 1000 for this code\n",
    "#no_urls = 5\n",
    "\n",
    "#agents: # of cores, chunksize: total values from records to be accesed by a thread at one time\n",
    "agents = 4\n",
    "chunksize = 100\n",
    "\n",
    "#initialize list to be assigned to threads\n",
    "#records = [[i, i+no_urls] for i in range(0, len(df_inter)+1, no_urls)]\n",
    "records = [[i, i+chunksize] for i in range(18000, 18800, chunksize)]\n",
    "\n",
    "#implement parallel processing\n",
    "with Pool(processes=agents) as pool:\n",
    "    #print(multiprocessing.current_process())\n",
    "    pool.map(build_coredataV3, records)\n",
    "    \n",
    "#df=pd.read_pickle(\"/Users/Rushi/Documents/GRAFall2018/Data/Masterdata_\"+str(year)+\".pkl.gz\", 'gzip')\n",
    "#print(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
